<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Paul Graham Essays</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.8.0/gsap.min.js"></script>
</head>
<body>
        <div class="percent">
            <div class="fill"></div>
            <div class="counter"><h1></h1></div>
        </div>
        <div class="container">
            <div class="content">
                <p>
                <span id="title">Beyond Smart</span>
                <br>
                <span id="secondText">October 2021</span>
                <br><br>
                <p>
                If you asked people what was special about Einstein, most would say that he was really smart. Even the ones who tried to give you a more sophisticated-sounding answer would probably think this first. Till a few years ago I would have given the same answer myself. But that wasn't what was special about Einstein. What was special about him was that he had important new ideas. Being very smart was a necessary precondition for having those ideas, but the two are not identical.
            </p><br>
            <p>
                It may seem a hair-splitting distinction to point out that intelligence and its consequences are not identical, but
                it isn't. There's a big gap between them. Anyone who's spent time around universities and research labs knows how
                big. There are a lot of genuinely smart people who don't achieve very much.
            </p><br>
            <p>
                I grew up thinking that being smart was the thing most to be desired. Perhaps you did too. But I bet it's not what
                you really want. Imagine you had a choice between being really smart but discovering nothing new, and being less
                smart but discovering lots of new ideas. Surely you'd take the latter. I would. The choice makes me uncomfortable,
                but when you see the two options laid out explicitly like that, it's obvious which is better.
            </p><br>
            <p>
                The reason the choice makes me uncomfortable is that being smart still feels like the thing that matters, even
                though I know intellectually that it isn't. I spent so many years thinking it was. The circumstances of childhood
                are a perfect storm for fostering this illusion. Intelligence is much easier to measure than the value of new ideas,
                and you're constantly being judged by it. Whereas even the kids who will ultimately discover new things aren't
                usually discovering them yet. For kids that way inclined, intelligence is the only game in town.
            </p><br>
            <p>
                There are more subtle reasons too, which persist long into adulthood. Intelligence wins in conversation, and thus
                becomes the basis of the dominance hierarchy. [1] Plus having new ideas is such a new thing historically, and even
                now done by so few people, that society hasn't yet assimilated the fact that this is the actual destination, and
                intelligence merely a means to an end. [2]
            </p><br>
            <p>
                Why do so many smart people fail to discover anything new? Viewed from that direction, the question seems a rather
                depressing one. But there's another way to look at it that's not just more optimistic, but more interesting as well.
                Clearly intelligence is not the only ingredient in having new ideas. What are the other ingredients? Are they things
                we could cultivate?
            </p><br>
            <p>
                Because the trouble with intelligence, they say, is that it's mostly inborn. The evidence for this seems fairly
                convincing, especially considering that most of us don't want it to be true, and the evidence thus has to face a
                pretty stiff headwind. But I'm not going to get into that question here, because it's the other ingredients in new
                ideas that I care about, and it's clear that many of them can be cultivated.
            </p><br>
            <p>
                That means the truth is excitingly different from the story I got as a kid. If intelligence is what matters, and
                also mostly inborn, the natural consequence is a sort of Brave New World fatalism. The best you can do is figure out
                what sort of work you have an "aptitude" for, so that whatever intelligence you were born with will at least be put
                to the best use, and then work as hard as you can at it. Whereas if intelligence isn't what matters, but only one of
                several ingredients in what does, and many of those aren't inborn, things get more interesting. You have a lot more
                control, but the problem of how to arrange your life becomes that much more complicated.
            </p><br>
            <p>
                So what are the other ingredients in having new ideas? The fact that I can even ask this question proves the point I
                raised earlier — that society hasn't assimilated the fact that it's this and not intelligence that matters.
                Otherwise we'd all know the answers to such a fundamental question. [3]
            </p><br>
            <p>
                I'm not going to try to provide a complete catalogue of the other ingredients besides intelligence. This is the
                first time I've posed the question to myself this way, and I think it may take a while to answer. But I wrote
                recently about one of the most important: an obsessive interest in a particular topic. And this can definitely be
                cultivated.
            </p><br>
            <p>
                Another quality you need in order to discover new ideas is independent-mindedness. I wouldn't want to claim that
                this is distinct from intelligence — I'd be reluctant to call someone smart who wasn't independent-minded — but
                though largely inborn, this quality seems to be something that can be cultivated to some extent.
            </p><br>
            <p>
                There are general techniques for having new ideas — for example, for working on your own projects and for overcoming
                the obstacles you face with early work — and these can all be learned. Some of them can be learned by societies. And
                there are also collections of techniques for generating specific types of new ideas, like startup ideas and essay
                topics.
            </p><br>
            <p>
                And of course there are a lot of fairly mundane ingredients in discovering new ideas, like working hard, getting
                enough sleep, avoiding certain kinds of stress, having the right colleagues, and finding tricks for working on what
                you want even when it's not what you're supposed to be working on. Anything that prevents people from doing great
                work has an inverse that helps them to. And this class of ingredients is not as boring as it might seem at first.
                For example, having new ideas is generally associated with youth. But perhaps it's not youth per se that yields new
                ideas, but specific things that come with youth, like good health and lack of responsibilities. Investigating this
                might lead to strategies that will help people of any age to have better ideas.
            </p><br>
            <p>
                One of the most surprising ingredients in having new ideas is writing ability. There's a class of new ideas that are
                best discovered by writing essays and books. And that "by" is deliberate: you don't think of the ideas first, and
                then merely write them down. There is a kind of thinking that one does by writing, and if you're clumsy at writing,
                or don't enjoy doing it, that will get in your way if you try to do this kind of thinking. [4]
            </p><br>
            <p>
                I predict the gap between intelligence and new ideas will turn out to be an interesting place. If we think of this
                gap merely as a measure of unrealized potential, it becomes a sort of wasteland we try to hurry through with our
                eyes averted. But if we flip the question, and start inquiring into the other ingredients in new ideas that it
                implies must exist, we can mine this gap for discoveries about discovery.
            </p><br>
            <p>
                Notes
            </p><br>
            <p>
                [1] What wins in conversation depends on who with. It ranges from mere aggressiveness at the bottom, through
                quick-wittedness in the middle, to something closer to actual intelligence at the top, though probably always with
                some component of quick-wittedness.
            </p><br>
            <p>
                [2] Just as intelligence isn't the only ingredient in having new ideas, having new ideas isn't the only thing
                intelligence is useful for. It's also useful, for example, in diagnosing problems and figuring out how to fix them.
                Both overlap with having new ideas, but both have an end that doesn't.
            </p><br>
            <p>
                Those ways of using intelligence are much more common than having new ideas. And in such cases intelligence is even
                harder to distinguish from its consequences.
            </p><br>
            <p>
                [3] Some would attribute the difference between intelligence and having new ideas to "creativity," but this doesn't
                seem a very useful term. As well as being pretty vague, it's shifted half a frame sideways from what we care about:
                it's neither separable from intelligence, nor responsible for all the difference between intelligence and having new
                ideas.
            </p><br>
            <p>
                [4] Curiously enough, this essay is an example. It started out as an essay about writing ability. But when I came to
                the distinction between intelligence and having new ideas, that seemed so much more important that I turned the
                original essay inside out, making that the topic and my original topic one of the points in it. As in many other
                fields, that level of reworking is easier to contemplate once you've had a lot of practice.
            </p><br>
            <p>
                Thanks to Trevor Blackwell, Patrick Collison, Jessica Livingston, Robert Morris, Michael Nielsen, and Lisa Randall
                for reading drafts of this.
            </p><br>
            <br><br>
            <span id="title">Weird Languages</span>
                <br>
                <span id="secondText">August 2021</span>
                <br><br><br><br>
            <p>
                When people say that in their experience all programming languages are basically equivalent, they're making a
                statement not about languages but about the kind of programming they've done.
            </p><br>
            <p>
                99.5% of programming consists of gluing together calls to library functions. All popular languages are equally good
                at this. So one can easily spend one's whole career operating in the intersection of popular programming languages.
            </p><br>
            <p>
                But the other .5% of programming is disproportionately interesting. If you want to learn what it consists of, the
                weirdness of weird languages is a good clue to follow.
            </p><br>
            <p>
                Weird languages aren't weird by accident. Not the good ones, at least. The weirdness of the good ones usually
                implies the existence of some form of programming that's not just the usual gluing together of library calls.
            </p><br>
            <p>
                A concrete example: Lisp macros. Lisp macros seem weird even to many Lisp programmers. They're not only not in the
                intersection of popular languages, but by their nature would be hard to implement properly in a language without
                turning it into a dialect of Lisp. And macros are definitely evidence of techniques that go beyond glue programming.
                For example, solving problems by first writing a language for problems of that type, and then writing your specific
                application in it. Nor is this all you can do with macros; it's just one region in a space of program-manipulating
                techniques that even now is far from fully explored.
            </p><br>
            <p>
                So if you want to expand your concept of what programming can be, one way to do it is by learning weird languages.
                Pick a language that most programmers consider weird but whose median user is smart, and then focus on the
                differences between this language and the intersection of popular languages. What can you say in this language that
                would be impossibly inconvenient to say in others? In the process of learning how to say things you couldn't
                previously say, you'll probably be learning how to think things you couldn't previously think.
            </p><br>
            <p>
                Thanks to Trevor Blackwell, Patrick Collison, Daniel Gackle, Amjad Masad, and Robert Morris for reading drafts of
                this.
            </p><br>
            <br><br>
            <span id="title">How To Work Hard</span>
                <br>
                <span id="secondText">June 2021</span>
                <br><br><br><br>
        <p>
            It might not seem there's much to learn about how to work hard. Anyone who's been to school knows what it entails, even
            if they chose not to do it. There are 12 year olds who work amazingly hard. And yet when I ask if I know more about
            working hard now than when I was in school, the answer is definitely yes.
        </p><br>
        <p>
            One thing I know is that if you want to do great things, you'll have to work very hard. I wasn't sure of that as a kid.
            Schoolwork varied in difficulty; one didn't always have to work super hard to do well. And some of the things famous
            adults did, they seemed to do almost effortlessly. Was there, perhaps, some way to evade hard work through sheer
            brilliance? Now I know the answer to that question. There isn't.
        </p><br>
        <p>    
            The reason some subjects seemed easy was that my school had low standards. And the reason famous adults seemed to do
            things effortlessly was years of practice; they made it look easy.
        </p><br>
        <p>   
            Of course, those famous adults usually had a lot of natural ability too. There are three ingredients in great work:
            natural ability, practice, and effort. You can do pretty well with just two, but to do the best work you need all three:
            you need great natural ability and to have practiced a lot and to be trying very hard. [1]
        </p><br>
        <p>   
            Bill Gates, for example, was among the smartest people in business in his era, but he was also among the hardest
            working. "I never took a day off in my twenties," he said. "Not one." It was similar with Lionel Messi. He had great
            natural ability, but when his youth coaches talk about him, what they remember is not his talent but his dedication and
            his desire to win. P. G. Wodehouse would probably get my vote for best English writer of the 20th century, if I had to
            choose. Certainly no one ever made it look easier. But no one ever worked harder. At 74, he wrote
            with each new book of mine I have, as I say, the feeling that this time I have picked a lemon in the garden of
            literature. A good thing, really, I suppose. Keeps one up on one's toes and makes one rewrite every sentence ten times.
            Or in many cases twenty times.
            Sounds a bit extreme, you think. And yet Bill Gates sounds even more extreme. Not one day off in ten years? These two
            had about as much natural ability as anyone could have, and yet they also worked about as hard as anyone could work. You
            need both.
        </p><br>
        <p>   
            That seems so obvious, and yet in practice we find it slightly hard to grasp. There's a faint xor between talent and
            hard work. It comes partly from popular culture, where it seems to run very deep, and partly from the fact that the
            outliers are so rare. If great talent and great drive are both rare, then people with both are rare squared. Most people
            you meet who have a lot of one will have less of the other. But you'll need both if you want to be an outlier yourself.
            And since you can't really change how much natural talent you have, in practice doing great work, insofar as you can,
            reduces to working very hard.
        </p><br>
        <p>   
            It's straightforward to work hard if you have clearly defined, externally imposed goals, as you do in school. There is
            some technique to it: you have to learn not to lie to yourself, not to procrastinate (which is a form of lying to
            yourself), not to get distracted, and not to give up when things go wrong. But this level of discipline seems to be
            within the reach of quite young children, if they want it.
        </p><br>
        <p>   
            What I've learned since I was a kid is how to work toward goals that are neither clearly defined nor externally imposed.
            You'll probably have to learn both if you want to do really great things.
        </p><br>
        <p>   
            The most basic level of which is simply to feel you should be working without anyone telling you to. Now, when I'm not
            working hard, alarm bells go off. I can't be sure I'm getting anywhere when I'm working hard, but I can be sure I'm
            getting nowhere when I'm not, and it feels awful. [2]
        </p><br>
        <p>   
            There wasn't a single point when I learned this. Like most little kids, I enjoyed the feeling of achievement when I
            learned or did something new. As I grew older, this morphed into a feeling of disgust when I wasn't achieving anything.
            The one precisely dateable landmark I have is when I stopped watching TV, at age 13.
        </p><br>
        <p>   
            Several people I've talked to remember getting serious about work around this age. When I asked Patrick Collison when he
            started to find idleness distasteful, he said
            I think around age 13 or 14. I have a clear memory from around then of sitting in the sitting room, staring outside, and
            wondering why I was wasting my summer holiday.
            Perhaps something changes at adolescence. That would make sense.
        </p><br>
        <p>   
            Strangely enough, the biggest obstacle to getting serious about work was probably school, which made work (what they
            called work) seem boring and pointless. I had to learn what real work was before I could wholeheartedly desire to do it.
            That took a while, because even in college a lot of the work is pointless; there are entire departments that are
            pointless. But as I learned the shape of real work, I found that my desire to do it slotted into it as if they'd been
            made for each other.
        </p><br>
        <p>   
            I suspect most people have to learn what work is before they can love it. Hardy wrote eloquently about this in A
            Mathematician's Apology:
            I do not remember having felt, as a boy, any passion for mathematics, and such notions as I may have had of the career
            of a mathematician were far from noble. I thought of mathematics in terms of examinations and scholarships: I wanted to
            beat other boys, and this seemed to be the way in which I could do so most decisively.
            He didn't learn what math was really about till part way through college, when he read Jordan's Cours d'analyse.
            I shall never forget the astonishment with which I read that remarkable work, the first inspiration for so many
            mathematicians of my generation, and learnt for the first time as I read it what mathematics really meant.
            There are two separate kinds of fakeness you need to learn to discount in order to understand what real work is. One is
            the kind Hardy encountered in school. Subjects get distorted when they're adapted to be taught to kids — often so
            distorted that they're nothing like the work done by actual practitioners. [3] The other kind of fakeness is intrinsic
            to certain types of work. Some types of work are inherently bogus, or at best mere busywork.
        </p><br>
        <p>   
            There's a kind of solidity to real work. It's not all writing the Principia, but it all feels necessary. That's a vague
            criterion, but it's deliberately vague, because it has to cover a lot of different types. [4]
        </p><br>
        <p>   
            Once you know the shape of real work, you have to learn how many hours a day to spend on it. You can't solve this
            problem by simply working every waking hour, because in many kinds of work there's a point beyond which the quality of
            the result will start to decline.
        </p><br>
        <p>   
            That limit varies depending on the type of work and the person. I've done several different kinds of work, and the
            limits were different for each. My limit for the harder types of writing or programming is about five hours a day.
            Whereas when I was running a startup, I could work all the time. At least for the three years I did it; if I'd kept
            going much longer, I'd probably have needed to take occasional vacations. [5]
        </p><br>
        <p>   
            The only way to find the limit is by crossing it. Cultivate a sensitivity to the quality of the work you're doing, and
            then you'll notice if it decreases because you're working too hard. Honesty is critical here, in both directions: you
            have to notice when you're being lazy, but also when you're working too hard. And if you think there's something
            admirable about working too hard, get that idea out of your head. You're not merely getting worse results, but getting
            them because you're showing off — if not to other people, then to yourself. [6]
        </p><br>
        <p>   
            Finding the limit of working hard is a constant, ongoing process, not something you do just once. Both the difficulty of
            the work and your ability to do it can vary hour to hour, so you need to be constantly judging both how hard you're
            trying and how well you're doing.
        </p><br>
        <p>   
            Trying hard doesn't mean constantly pushing yourself to work, though. There may be some people who do, but I think my
            experience is fairly typical, and I only have to push myself occasionally when I'm starting a project or when I
            encounter some sort of check. That's when I'm in danger of procrastinating. But once I get rolling, I tend to keep
            going.
        </p><br>
        <p>   
            What keeps me going depends on the type of work. When I was working on Viaweb, I was driven by fear of failure. I barely
            procrastinated at all then, because there was always something that needed doing, and if I could put more distance
            between me and the pursuing beast by doing it, why wait? [7] Whereas what drives me now, writing essays, is the flaws in
            them. Between essays I fuss for a few days, like a dog circling while it decides exactly where to lie down. But once I
            get started on one, I don't have to push myself to work, because there's always some error or omission already pushing
            me.
        </p><br>
        <p>   
            I do make some amount of effort to focus on important topics. Many problems have a hard core at the center, surrounded
            by easier stuff at the edges. Working hard means aiming toward the center to the extent you can. Some days you may not
            be able to; some days you'll only be able to work on the easier, peripheral stuff. But you should always be aiming as
            close to the center as you can without stalling.
        </p><br>
        <p>   
            The bigger question of what to do with your life is one of these problems with a hard core. There are important problems
            at the center, which tend to be hard, and less important, easier ones at the edges. So as well as the small, daily
            adjustments involved in working on a specific problem, you'll occasionally have to make big, lifetime-scale adjustments
            about which type of work to do. And the rule is the same: working hard means aiming toward the center — toward the most
            ambitious problems.
        </p><br>
        <p>   
            By center, though, I mean the actual center, not merely the current consensus about the center. The consensus about
            which problems are most important is often mistaken, both in general and within specific fields. If you disagree with
            it, and you're right, that could represent a valuable opportunity to do something new.
        </p><br>
        <p>   
            The more ambitious types of work will usually be harder, but although you should not be in denial about this, neither
            should you treat difficulty as an infallible guide in deciding what to do. If you discover some ambitious type of work
            that's a bargain in the sense of being easier for you than other people, either because of the abilities you happen to
            have, or because of some new way you've found to approach it, or simply because you're more excited about it, by all
            means work on that. Some of the best work is done by people who find an easy way to do something hard.
        </p><br>
        <p>   
            As well as learning the shape of real work, you need to figure out which kind you're suited for. And that doesn't just
            mean figuring out which kind your natural abilities match the best; it doesn't mean that if you're 7 feet tall, you have
            to play basketball. What you're suited for depends not just on your talents but perhaps even more on your interests. A
            deep interest in a topic makes people work harder than any amount of discipline can.
        </p><br>
        <p>   
            It can be harder to discover your interests than your talents. There are fewer types of talent than interest, and they
            start to be judged early in childhood, whereas interest in a topic is a subtle thing that may not mature till your
            twenties, or even later. The topic may not even exist earlier. Plus there are some powerful sources of error you need to
            learn to discount. Are you really interested in x, or do you want to work on it because you'll make a lot of money, or
            because other people will be impressed with you, or because your parents want you to? [8]
        </p><br>
        <p>   
            The difficulty of figuring out what to work on varies enormously from one person to another. That's one of the most
            important things I've learned about work since I was a kid. As a kid, you get the impression that everyone has a
            calling, and all they have to do is figure out what it is. That's how it works in movies, and in the streamlined
            biographies fed to kids. Sometimes it works that way in real life. Some people figure out what to do as children and
            just do it, like Mozart. But others, like Newton, turn restlessly from one kind of work to another. Maybe in retrospect
            we can identify one as their calling — we can wish Newton spent more time on math and physics and less on alchemy and
            theology — but this is an illusion induced by hindsight bias. There was no voice calling to him that he could have
            heard.
        </p><br>
        <p>   
            So while some people's lives converge fast, there will be others whose lives never converge. And for these people,
            figuring out what to work on is not so much a prelude to working hard as an ongoing part of it, like one of a set of
            simultaneous equations. For these people, the process I described earlier has a third component: along with measuring
            both how hard you're working and how well you're doing, you have to think about whether you should keep working in this
            field or switch to another. If you're working hard but not getting good enough results, you should switch. It sounds
            simple expressed that way, but in practice it's very difficult. You shouldn't give up on the first day just because you
            work hard and don't get anywhere. You need to give yourself time to get going. But how much time? And what should you do
            if work that was going well stops going well? How much time do you give yourself then? [9]
        </p><br>
        <p>      
            What even counts as good results? That can be really hard to decide. If you're exploring an area few others have worked
            in, you may not even know what good results look like. History is full of examples of people who misjudged the
            importance of what they were working on.
        </p><br>
        <p>     
            The best test of whether it's worthwhile to work on something is whether you find it interesting. That may sound like a
            dangerously subjective measure, but it's probably the most accurate one you're going to get. You're the one working on
            the stuff. Who's in a better position than you to judge whether it's important, and what's a better predictor of its
            importance than whether it's interesting?
        </p><br>
        <p>     
            For this test to work, though, you have to be honest with yourself. Indeed, that's the most striking thing about the
            whole question of working hard: how at each point it depends on being honest with yourself.
        </p><br>
        <p>      
            Working hard is not just a dial you turn up to 11. It's a complicated, dynamic system that has to be tuned just right at
            each point. You have to understand the shape of real work, see clearly what kind you're best suited for, aim as close to
            the true core of it as you can, accurately judge at each moment both what you're capable of and how you're doing, and
            put in as many hours each day as you can without harming the quality of the result. This network is too complicated to
            trick. But if you're consistently honest and clear-sighted, it will automatically assume an optimal shape, and you'll be
            productive in a way few people are.
        </p><br>
        <p>      
            Notes
        </p><br>
        <p>     
            [1] In "The Bus Ticket Theory of Genius" I said the three ingredients in great work were natural ability, determination,
            and interest. That's the formula in the preceding stage; determination and interest yield practice and effort.
        </p><br>
        <p>     
            [2] I mean this at a resolution of days, not hours. You'll often get somewhere while not working in the sense that the
            solution to a problem comes to you while taking a shower, or even in your sleep, but only because you were working hard
            on it the day before.
        </p><br>
        <p>     
            It's good to go on vacation occasionally, but when I go on vacation, I like to learn new things. I wouldn't like just
            sitting on a beach.
        </p><br>
        <p>     
            [3] The thing kids do in school that's most like the real version is sports. Admittedly because many sports originated
            as games played in schools. But in this one area, at least, kids are doing exactly what adults do.
        </p><br>
        <p>     
            In the average American high school, you have a choice of pretending to do something serious, or seriously doing
            something pretend. Arguably the latter is no worse.
        </p><br>
        <p>     
            [4] Knowing what you want to work on doesn't mean you'll be able to. Most people have to spend a lot of their time
            working on things they don't want to, especially early on. But if you know what you want to do, you at least know what
            direction to nudge your life in.
        </p><br>
        <p>     
            [5] The lower time limits for intense work suggest a solution to the problem of having less time to work after you have
            kids: switch to harder problems. In effect I did that, though not deliberately.
        </p><br>
        <p>     
            [6] Some cultures have a tradition of performative hard work. I don't love this idea, because (a) it makes a parody of
            something important and (b) it causes people to wear themselves out doing things that don't matter. I don't know enough
            to say for sure whether it's net good or bad, but my guess is bad.
        </p><br>
        <p>    
            [7] One of the reasons people work so hard on startups is that startups can fail, and when they do, that failure tends
            to be both decisive and conspicuous.
        </p><br>
        <p>     
            [8] It's ok to work on something to make a lot of money. You need to solve the money problem somehow, and there's
            nothing wrong with doing that efficiently by trying to make a lot at once. I suppose it would even be ok to be
            interested in money for its own sake; whatever floats your boat. Just so long as you're conscious of your motivations.
            The thing to avoid is unconsciously letting the need for money warp your ideas about what kind of work you find most
            interesting.
        </p><br>
        <p>     
            [9] Many people face this question on a smaller scale with individual projects. But it's easier both to recognize and to
            accept a dead end in a single project than to abandon some type of work entirely. The more determined you are, the
            harder it gets. Like a Spanish Flu victim, you're fighting your own immune system: Instead of giving up, you tell
            yourself, I should just try harder. And who can say you're not right?
        </p><br>
        <p>        
            Thanks to Trevor Blackwell, John Carmack, John Collison, Patrick Collison, Robert Morris, Geoff Ralston, and Harj Taggar
            for reading drafts of this.
        </p><br>
        <br><br>
            <span id="title">A Project Of One's Own</span>
                <br>
                <span id="secondText">June 2021</span>
                <br><br><br><br>
        <p>
        A few days ago, on the way home from school, my nine year old son told me he couldn't wait to get home to write more of
        the story he was working on. This made me as happy as anything I've heard him say — not just because he was excited
        about his story, but because he'd discovered this way of working. Working on a project of your own is as different from
        ordinary work as skating is from walking. It's more fun, but also much more productive.
        </p><br>
        <p>
        What proportion of great work has been done by people who were skating in this sense? If not all of it, certainly a lot.
    </p><br>
    <p>
        There is something special about working on a project of your own. I wouldn't say exactly that you're happier. A better
        word would be excited, or engaged. You're happy when things are going well, but often they aren't. When I'm writing an
        essay, most of the time I'm worried and puzzled: worried that the essay will turn out badly, and puzzled because I'm
        groping for some idea that I can't see clearly enough. Will I be able to pin it down with words? In the end I usually
        can, if I take long enough, but I'm never sure; the first few attempts often fail.
    </p><br>
    <p>  
        You have moments of happiness when things work out, but they don't last long, because then you're on to the next
        problem. So why do it at all? Because to the kind of people who like working this way, nothing else feels as right. You
        feel as if you're an animal in its natural habitat, doing what you were meant to do — not always happy, maybe, but awake
        and alive.
    </p><br>
    <p>  
        Many kids experience the excitement of working on projects of their own. The hard part is making this converge with the
        work you do as an adult. And our customs make it harder. We treat "playing" and "hobbies" as qualitatively different
        from "work". It's not clear to a kid building a treehouse that there's a direct (though long) route from that to
        architecture or engineering. And instead of pointing out the route, we conceal it, by implicitly treating the stuff kids
        do as different from real work. [1]
    </p><br>
    <p>  
        Instead of telling kids that their treehouses could be on the path to the work they do as adults, we tell them the path
        goes through school. And unfortunately schoolwork tends be very different from working on projects of one's own. It's
        usually neither a project, nor one's own. So as school gets more serious, working on projects of one's own is something
        that survives, if at all, as a thin thread off to the side.
    </p><br>
    <p>  
        It's a bit sad to think of all the high school kids turning their backs on building treehouses and sitting in class
        dutifully learning about Darwin or Newton to pass some exam, when the work that made Darwin and Newton famous was
        actually closer in spirit to building treehouses than studying for exams.
    </p><br>
    <p>  
        If I had to choose between my kids getting good grades and working on ambitious projects of their own, I'd pick the
        projects. And not because I'm an indulgent parent, but because I've been on the other end and I know which has more
        predictive value. When I was picking startups for Y Combinator, I didn't care about applicants' grades. But if they'd
        worked on projects of their own, I wanted to hear all about those. [2]
    </p><br>
    <p>  It may be inevitable that school is the way it is. I'm not saying we have to redesign it (though I'm not saying we
        don't), just that we should understand what it does to our attitudes to work — that it steers us toward the dutiful
        plodding kind of work, often using competition as bait, and away from skating.
    </p><br>
    <p>   
        There are occasionally times when schoolwork becomes a project of one's own. Whenever I had to write a paper, that would
        become a project of my own — except in English classes, ironically, because the things one has to write in English
        classes are so bogus. And when I got to college and started taking CS classes, the programs I had to write became
        projects of my own. Whenever I was writing or programming, I was usually skating, and that has been true ever since.
    </p><br>
    <p>   
        So where exactly is the edge of projects of one's own? That's an interesting question, partly because the answer is so
        complicated, and partly because there's so much at stake. There turn out to be two senses in which work can be one's
        own: 1) that you're doing it voluntarily, rather than merely because someone told you to, and 2) that you're doing it by
        yourself.
    </p><br>
    <p>   
        The edge of the former is quite sharp. People who care a lot about their work are usually very sensitive to the
        difference between pulling, and being pushed, and work tends to fall into one category or the other. But the test isn't
        simply whether you're told to do something. You can choose to do something you're told to do. Indeed, you can own it far
        more thoroughly than the person who told you to do it.
    </p><br>
    <p>   
        For example, math homework is for most people something they're told to do. But for my father, who was a mathematician,
        it wasn't. Most of us think of the problems in a math book as a way to test or develop our knowledge of the material
        explained in each section. But to my father the problems were the part that mattered, and the text was merely a sort of
        annotation. Whenever he got a new math book it was to him like being given a puzzle: here was a new set of problems to
        solve, and he'd immediately set about solving all of them.
    </p><br>
    <p>  
        The other sense of a project being one's own — working on it by oneself — has a much softer edge. It shades gradually
        into collaboration. And interestingly, it shades into collaboration in two different ways. One way to collaborate is to
        share a single project. For example, when two mathematicians collaborate on a proof that takes shape in the course of a
        conversation between them. The other way is when multiple people work on separate projects of their own that fit
        together like a jigsaw puzzle. For example, when one person writes the text of a book and another does the graphic
        design. [3]
    </p><br>
    <p>  
        These two paths into collaboration can of course be combined. But under the right conditions, the excitement of working
        on a project of one's own can be preserved for quite a while before disintegrating into the turbulent flow of work in a
        large organization. Indeed, the history of successful organizations is partly the history of techniques for preserving
        that excitement. [4]
    </p><br>
    <p>   
        The team that made the original Macintosh were a great example of this phenomenon. People like Burrell Smith and Andy
        Hertzfeld and Bill Atkinson and Susan Kare were not just following orders. They were not tennis balls hit by Steve Jobs,
        but rockets let loose by Steve Jobs. There was a lot of collaboration between them, but they all seem to have
        individually felt the excitement of working on a project of one's own.
    </p><br>
    <p>   
        In Andy Hertzfeld's book on the Macintosh, he describes how they'd come back into the office after dinner and work late
        into the night. People who've never experienced the thrill of working on a project they're excited about can't
        distinguish this kind of working long hours from the kind that happens in sweatshops and boiler rooms, but they're at
        opposite ends of the spectrum. That's why it's a mistake to insist dogmatically on "work/life balance." Indeed, the mere
        expression "work/life" embodies a mistake: it assumes work and life are distinct. For those to whom the word "work"
        automatically implies the dutiful plodding kind, they are. But for the skaters, the relationship between work and life
        would be better represented by a dash than a slash. I wouldn't want to work on anything I didn't want to take over my
        life.
    </p><br>
    <p>   
        Of course, it's easier to achieve this level of motivation when you're making something like the Macintosh. It's easy
        for something new to feel like a project of your own. That's one of the reasons for the tendency programmers have to
        rewrite things that don't need rewriting, and to write their own versions of things that already exist. This sometimes
        alarms managers, and measured by total number of characters typed, it's rarely the optimal solution. But it's not always
        driven simply by arrogance or cluelessness. Writing code from scratch is also much more rewarding — so much more
        rewarding that a good programmer can end up net ahead, despite the shocking waste of characters. Indeed, it may be one
        of the advantages of capitalism that it encourages such rewriting. A company that needs software to do something can't
        use the software already written to do it at another company, and thus has to write their own, which often turns out
        better. [5]
    </p><br>
    <p>   
        The natural alignment between skating and solving new problems is one of the reasons the payoffs from startups are so
        high. Not only is the market price of unsolved problems higher, you also get a discount on productivity when you work on
        them. In fact, you get a double increase in productivity: when you're doing a clean-sheet design, it's easier to recruit
        skaters, and they get to spend all their time skating.
    </p><br>
    <p>   
        Steve Jobs knew a thing or two about skaters from having watched Steve Wozniak. If you can find the right people, you
        only have to tell them what to do at the highest level. They'll handle the details. Indeed, they insist on it. For a
        project to feel like your own, you must have sufficient autonomy. You can't be working to order, or slowed down by
        bureaucracy.
    </p><br>
    <p>    
        One way to ensure autonomy is not to have a boss at all. There are two ways to do that: to be the boss yourself, and to
        work on projects outside of work. Though they're at opposite ends of the scale financially, startups and open source
        projects have a lot in common, including the fact that they're often run by skaters. And indeed, there's a wormhole from
        one end of the scale to the other: one of the best ways to discover startup ideas is to work on a project just for fun.
    </p><br>
    <p>   
        If your projects are the kind that make money, it's easy to work on them. It's harder when they're not. And the hardest
        part, usually, is morale. That's where adults have it harder than kids. Kids just plunge in and build their treehouse
        without worrying about whether they're wasting their time, or how it compares to other treehouses. And frankly we could
        learn a lot from kids here. The high standards most grownups have for "real" work do not always serve us well.
    </p><br>
    <p>   
        The most important phase in a project of one's own is at the beginning: when you go from thinking it might be cool to do
        x to actually doing x. And at that point high standards are not merely useless but positively harmful. There are a few
        people who start too many new projects, but far more, I suspect, who are deterred by fear of failure from starting
        projects that would have succeeded if they had.
    </p><br>
    <p>    
        But if we couldn't benefit as kids from the knowledge that our treehouses were on the path to grownup projects, we can
        at least benefit as grownups from knowing that our projects are on a path that stretches back to treehouses. Remember
        that careless confidence you had as a kid when starting something new? That would be a powerful thing to recapture.
    </p><br>
    <p>   
        If it's harder as adults to retain that kind of confidence, we at least tend to be more aware of what we're doing. Kids
        bounce, or are herded, from one kind of work to the next, barely realizing what's happening to them. Whereas we know
        more about different types of work and have more control over which we do. Ideally we can have the best of both worlds:
        to be deliberate in choosing to work on projects of our own, and carelessly confident in starting new ones.
    </p><br>
    <p>     
        Notes
    </p><br>
    <p>  
        [1] "Hobby" is a curious word. Now it means work that isn't real work — work that one is not to be judged by — but
        originally it just meant an obsession in a fairly general sense (even a political opinion, for example) that one
        metaphorically rode as a child rides a hobby-horse. It's hard to say if its recent, narrower meaning is a change for the
        better or the worse. For sure there are lots of false positives — lots of projects that end up being important but are
        dismissed initially as mere hobbies. But on the other hand, the concept provides valuable cover for projects in the
        early, ugly duckling phase.
    </p><br>
    <p>   
        [2] Tiger parents, as parents so often do, are fighting the last war. Grades mattered more in the old days when the
        route to success was to acquire credentials while ascending some predefined ladder. But it's just as well that their
        tactics are focused on grades. How awful it would be if they invaded the territory of projects, and thereby gave their
        kids a distaste for this kind of work by forcing them to do it. Grades are already a grim, fake world, and aren't harmed
        much by parental interference, but working on one's own projects is a more delicate, private thing that could be damaged
        very easily.
    </p><br>
    <p>  
        [3] The complicated, gradual edge between working on one's own projects and collaborating with others is one reason
        there is so much disagreement about the idea of the "lone genius." In practice people collaborate (or not) in all kinds
        of different ways, but the idea of the lone genius is definitely not a myth. There's a core of truth to it that goes
        with a certain way of working.
    </p><br>
    <p>   
        [4] Collaboration is powerful too. The optimal organization would combine collaboration and ownership in such a way as
        to do the least damage to each. Interestingly, companies and university departments approach this ideal from opposite
        directions: companies insist on collaboration, and occasionally also manage both to recruit skaters and allow them to
        skate, and university departments insist on the ability to do independent research (which is by custom treated as
        skating, whether it is or not), and the people they hire collaborate as much as they choose.
    </p><br>
    <p>   
        [5] If a company could design its software in such a way that the best newly arrived programmers always got a clean
        sheet, it could have a kind of eternal youth. That might not be impossible. If you had a software backbone defining a
        game with sufficiently clear rules, individual programmers could write their own players.
    </p><br>
    <p>   
         Thanks to Trevor Blackwell, Paul Buchheit, Andy Hertzfeld, Jessica Livingston, and Peter Norvig for reading drafts of
        this.
    </p><br>
    <br><br>
            <span id="title">Fierce Nerds</span>
                <br>
                <span id="secondText">May 2021</span>
                <br><br><br><br>
    <p>
    Most people think of nerds as quiet, diffident people. In ordinary social situations they are — as quiet and diffident
    as the star quarterback would be if he found himself in the middle of a physics symposium. And for the same reason: they
    are fish out of water. But the apparent diffidence of nerds is an illusion due to the fact that when non-nerds observe
    them, it's usually in ordinary social situations. In fact some nerds are quite fierce.
</p><br>
<p> 
    The fierce nerds are a small but interesting group. They are as a rule extremely competitive — more competitive, I'd
    say, than highly competitive non-nerds. Competition is more personal for them. Partly perhaps because they're not
    emotionally mature enough to distance themselves from it, but also because there's less randomness in the kinds of
    competition they engage in, and they are thus more justified in taking the results personally.
</p><br>
<p>   
    Fierce nerds also tend to be somewhat overconfident, especially when young. It might seem like it would be a
    disadvantage to be mistaken about one's abilities, but empirically it isn't. Up to a point, confidence is a
    self-fullfilling prophecy.
</p><br>
<p>    
    Another quality you find in most fierce nerds is intelligence. Not all nerds are smart, but the fierce ones are always
    at least moderately so. If they weren't, they wouldn't have the confidence to be fierce. [1]
</p><br>
<p>   
    There's also a natural connection between nerdiness and independent-mindedness. It's hard to be independent-minded
    without being somewhat socially awkward, because conventional beliefs are so often mistaken, or at least arbitrary. No
    one who was both independent-minded and ambitious would want to waste the effort it takes to fit in. And the
    independent-mindedness of the fierce nerds will obviously be of the aggressive rather than the passive type: they'll be
    annoyed by rules, rather than dreamily unaware of them.
</p><br>
<p>    
    I'm less sure why fierce nerds are impatient, but most seem to be. You notice it first in conversation, where they tend
    to interrupt you. This is merely annoying, but in the more promising fierce nerds it's connected to a deeper impatience
    about solving problems. Perhaps the competitiveness and impatience of fierce nerds are not separate qualities, but two
    manifestations of a single underlying drivenness.
</p><br>
<p>    
    When you combine all these qualities in sufficient quantities, the result is quite formidable. The most vivid example of
    fierce nerds in action may be James Watson's The Double Helix. The first sentence of the book is "I have never seen
    Francis Crick in a modest mood," and the portrait he goes on to paint of Crick is the quintessential fierce nerd:
    brilliant, socially awkward, competitive, independent-minded, overconfident. But so is the implicit portrait he paints
    of himself. Indeed, his lack of social awareness makes both portraits that much more realistic, because he baldly states
    all sorts of opinions and motivations that a smoother person would conceal. And moreover it's clear from the story that
    Crick and Watson's fierce nerdiness was integral to their success. Their independent-mindedness caused them to consider
    approaches that most others ignored, their overconfidence allowed them to work on problems they only half understood
    (they were literally described as "clowns" by one eminent insider), and their impatience and competitiveness got them to
    the answer ahead of two other groups that would otherwise have found it within the next year, if not the next several
    months. [2]
</p><br>
<p>    
    The idea that there could be fierce nerds is an unfamiliar one not just to many normal people but even to some young
    nerds. Especially early on, nerds spend so much of their time in ordinary social situations and so little doing real
    work that they get a lot more evidence of their awkwardness than their power. So there will be some who read this
    description of the fierce nerd and realize "Hmm, that's me." And it is to you, young fierce nerd, that I now turn.
</p><br>
<p>    
    I have some good news, and some bad news. The good news is that your fierceness will be a great help in solving
    difficult problems. And not just the kind of scientific and technical problems that nerds have traditionally solved. As
    the world progresses, the number of things you can win at by getting the right answer increases. Recently getting rich
    became one of them: 7 of the 8 richest people in America are now fierce nerds.
</p><br>
<p>    
    Indeed, being a fierce nerd is probably even more helpful in business than in nerds' original territory of scholarship.
    Fierceness seems optional there. Darwin for example doesn't seem to have been especially fierce. Whereas it's impossible
    to be the CEO of a company over a certain size without being fierce, so now that nerds can win at business, fierce nerds
    will increasingly monopolize the really big successes.
</p><br>
<p>   
    The bad news is that if it's not exercised, your fierceness will turn to bitterness, and you will become an intellectual
    playground bully: the grumpy sysadmin, the forum troll, the hater, the shooter down of new ideas.
</p><br>
<p>    
    How do you avoid this fate? Work on ambitious projects. If you succeed, it will bring you a kind of satisfaction that
    neutralizes bitterness. But you don't need to have succeeded to feel this; merely working on hard projects gives most
    fierce nerds some feeling of satisfaction. And those it doesn't, it at least keeps busy. [3]
</p><br>
<p>    
    Another solution may be to somehow turn off your fierceness, by devoting yourself to meditation or psychotherapy or
    something like that. Maybe that's the right answer for some people. I have no idea. But it doesn't seem the optimal
    solution to me. If you're given a sharp knife, it seems to me better to use it than to blunt its edge to avoid cutting
    yourself.
</p><br>
<p>    
    If you do choose the ambitious route, you'll have a tailwind behind you. There has never been a better time to be a
    nerd. In the past century we've seen a continuous transfer of power from dealmakers to technicians — from the
    charismatic to the competent — and I don't see anything on the horizon that will end it. At least not till the nerds end
    it themselves by bringing about the singularity.
</p><br>
<p> Notes
    
    [1] To be a nerd is to be socially awkward, and there are two distinct ways to do that: to be playing the same game as
    everyone else, but badly, and to be playing a different game. The smart nerds are the latter type.
</p><br>
<p>    
    [2] The same qualities that make fierce nerds so effective can also make them very annoying. Fierce nerds would do well
    to remember this, and (a) try to keep a lid on it, and (b) seek out organizations and types of work where getting the
    right answer matters more than preserving social harmony. In practice that means small groups working on hard problems.
    Which fortunately is the most fun kind of environment anyway.
</p><br>
<p>    
    [3] If success neutralizes bitterness, why are there some people who are at least moderately successful and yet still
    quite bitter? Because people's potential bitterness varies depending on how naturally bitter their personality is, and
    how ambitious they are: someone who's naturally very bitter will still have a lot left after success neutralizes some of
    it, and someone who's very ambitious will need proportionally more success to satisfy that ambition.
</p><br>
<p>   
    So the worst-case scenario is someone who's both naturally bitter and extremely ambitious, and yet only moderately
    successful.
</p><br>
<p>     
    Thanks to Trevor Blackwell, Steve Blank, Patrick Collison, Jessica Livingston, Amjad Masad, and Robert Morris for
    reading drafts of this.
</p><br>
<br><br>
            <span id="title">Crazy New Ideas</span>
                <br>
                <span id="secondText">May 2021</span>
                <br><br><br><br>
    <p>
    There's one kind of opinion I'd be very afraid to express publicly. If someone I knew to be both a domain expert and a
    reasonable person proposed an idea that sounded preposterous, I'd be very reluctant to say "That will never work."
</p><br>
<p> 
    Anyone who has studied the history of ideas, and especially the history of science, knows that's how big things start.
    Someone proposes an idea that sounds crazy, most people dismiss it, then it gradually takes over the world.
</p><br>
<p> 
    Most implausible-sounding ideas are in fact bad and could be safely dismissed. But not when they're proposed by
    reasonable domain experts. If the person proposing the idea is reasonable, then they know how implausible it sounds. And
    yet they're proposing it anyway. That suggests they know something you don't. And if they have deep domain expertise,
    that's probably the source of it. [1]
</p><br>
<p>   
    Such ideas are not merely unsafe to dismiss, but disproportionately likely to be interesting. When the average person
    proposes an implausible-sounding idea, its implausibility is evidence of their incompetence. But when a reasonable
    domain expert does it, the situation is reversed. There's something like an efficient market here: on average the ideas
    that seem craziest will, if correct, have the biggest effect. So if you can eliminate the theory that the person
    proposing an implausible-sounding idea is incompetent, its implausibility switches from evidence that it's boring to
    evidence that it's exciting. [2]
</p><br>
<p>    
    Such ideas are not guaranteed to work. But they don't have to be. They just have to be sufficiently good bets — to have
    sufficiently high expected value. And I think on average they do. I think if you bet on the entire set of
    implausible-sounding ideas proposed by reasonable domain experts, you'd end up net ahead.
</p><br>
<p>    
    The reason is that everyone is too conservative. The word "paradigm" is overused, but this is a case where it's
    warranted. Everyone is too much in the grip of the current paradigm. Even the people who have the new ideas undervalue
    them initially. Which means that before they reach the stage of proposing them publicly, they've already subjected them
    to an excessively strict filter. [3]
</p><br>
<p>    
    The wise response to such an idea is not to make statements, but to ask questions, because there's a real mystery here.
    Why has this smart and reasonable person proposed an idea that seems so wrong? Are they mistaken, or are you? One of you
    has to be. If you're the one who's mistaken, that would be good to know, because it means there's a hole in your model
    of the world. But even if they're mistaken, it should be interesting to learn why. A trap that an expert falls into is
    one you have to worry about too.
</p><br>
<p>    
    This all seems pretty obvious. And yet there are clearly a lot of people who don't share my fear of dismissing new
    ideas. Why do they do it? Why risk looking like a jerk now and a fool later, instead of just reserving judgement?
</p><br>
<p>    
    One reason they do it is envy. If you propose a radical new idea and it succeeds, your reputation (and perhaps also your
    wealth) will increase proportionally. Some people would be envious if that happened, and this potential envy propagates
    back into a conviction that you must be wrong.
</p><br>
<p>    
    Another reason people dismiss new ideas is that it's an easy way to seem sophisticated. When a new idea first emerges,
    it usually seems pretty feeble. It's a mere hatchling. Received wisdom is a full-grown eagle by comparison. So it's easy
    to launch a devastating attack on a new idea, and anyone who does will seem clever to those who don't understand this
    asymmetry.
</p><br>
<p>    
    This phenomenon is exacerbated by the difference between how those working on new ideas and those attacking them are
    rewarded. The rewards for working on new ideas are weighted by the value of the outcome. So it's worth working on
    something that only has a 10% chance of succeeding if it would make things more than 10x better. Whereas the rewards for
    attacking new ideas are roughly constant; such attacks seem roughly equally clever regardless of the target.
</p><br>
<p>    
    People will also attack new ideas when they have a vested interest in the old ones. It's not surprising, for example,
    that some of Darwin's harshest critics were churchmen. People build whole careers on some ideas. When someone claims
    they're false or obsolete, they feel threatened.
</p><br>
<p>    
    The lowest form of dismissal is mere factionalism: to automatically dismiss any idea associated with the opposing
    faction. The lowest form of all is to dismiss an idea because of who proposed it.
</p><br>
<p>    
    But the main thing that leads reasonable people to dismiss new ideas is the same thing that holds people back from
    proposing them: the sheer pervasiveness of the current paradigm. It doesn't just affect the way we think; it is the Lego
    blocks we build thoughts out of. Popping out of the current paradigm is something only a few people can do. And even
    they usually have to suppress their intuitions at first, like a pilot flying through cloud who has to trust his
    instruments over his sense of balance. [4]
</p><br>
<p>     
    Paradigms don't just define our present thinking. They also vacuum up the trail of crumbs that led to them, making our
    standards for new ideas impossibly high. The current paradigm seems so perfect to us, its offspring, that we imagine it
    must have been accepted completely as soon as it was discovered. Whatever the church thought of the heliocentric model,
    astronomers must have been convinced as soon as Copernicus proposed it. Far, in fact, from it. Copernicus published the
    heliocentric model in 1532, but it wasn't till the mid seventeenth century that the balance of scientific opinion
    shifted in its favor. [5]
</p><br>
<p>    
    Few understand how feeble new ideas look when they first appear. So if you want to have new ideas yourself, one of the
    most valuable things you can do is to learn what they look like when they're born. Read about how new ideas happened,
    and try to get yourself into the heads of people at the time. How did things look to them, when the new idea was only
    half-finished, and even the person who had it was only half-convinced it was right?
</p><br>
<p>    
    But you don't have to stop at history. You can observe big new ideas being born all around you right now. Just look for
    a reasonable domain expert proposing something that sounds wrong.
</p><br>
<p>     
    If you're nice, as well as wise, you won't merely resist attacking such people, but encourage them. Having new ideas is
    a lonely business. Only those who've tried it know how lonely. These people need your help. And if you help them, you'll
    probably learn something in the process.
</p><br>
<p>      
    Notes
</p><br>
<p>    
    [1] This domain expertise could be in another field. Indeed, such crossovers tend to be particularly promising.
</p><br>
<p>    
    [2] I'm not claiming this principle extends much beyond math, engineering, and the hard sciences. In politics, for
    example, crazy-sounding ideas generally are as bad as they sound. Though arguably this is not an exception, because the
    people who propose them are not in fact domain experts; politicians are domain experts in political tactics, like how to
    get elected and how to get legislation passed, but not in the world that policy acts upon. Perhaps no one could be.
</p><br>
<p>    
    [3] This sense of "paradigm" was defined by Thomas Kuhn in his Structure of Scientific Revolutions, but I also recommend
    his Copernican Revolution, where you can see him at work developing the idea.
</p><br>
<p>    
    [4] This is one reason people with a touch of Asperger's may have an advantage in discovering new ideas. They're always
    flying on instruments.
</p><br>
<p>    
    [5] Hall, Rupert. From Galileo to Newton. Collins, 1963. This book is particularly good at getting into contemporaries'
    heads.
</p><br>
<p>    
    Thanks to Trevor Blackwell, Patrick Collison, Suhail Doshi, Daniel Gackle, Jessica Livingston, and Robert Morris for
    reading drafts of this.
</p><br>
<br><br>
<span id="title">An NFT That Saves Lives</span>
                <br>
                <span id="secondText">May 2021</span>
                <br><br><br><br>   
<p>    
Noora Health, a nonprofit I've supported for years, just launched a new NFT. It has a dramatic name, Save Thousands of
Lives, because that's what the proceeds will do.
</p><br>
<p> 
Noora has been saving lives for 7 years. They run programs in hospitals in South Asia to teach new mothers how to take
care of their babies once they get home. They're in 165 hospitals now. And because they know the numbers before and
after they start at a new hospital, they can measure the impact they have. It is massive. For every 1000 live births,
they save 9 babies.
</p><br>
<p> 
This number comes from a study of 133,733 families at 28 different hospitals that Noora conducted in collaboration with
the Better Birth team at Ariadne Labs, a joint center for health systems innovation at Brigham and Women’s Hospital and
Harvard T.H. Chan School of Public Health.
</p><br>
<p> 
Noora is so effective that even if you measure their costs in the most conservative way, by dividing their entire budget
by the number of lives saved, the cost of saving a life is the lowest I've seen. $1,235.
</p><br>
<p> 
For this NFT, they're going to issue a public report tracking how this specific tranche of money is spent, and
estimating the number of lives saved as a result.
</p><br>
<p> 
NFTs are a new territory, and this way of using them is especially new, but I'm excited about its potential. And I'm
excited to see what happens with this particular auction, because unlike an NFT representing something that has already
happened, this NFT gets better as the price gets higher.
</p><br>
<p> 
The reserve price was about $2.5 million, because that's what it takes for the name to be accurate: that's what it costs
to save 2000 lives. But the higher the price of this NFT goes, the more lives will be saved. What a sentence to be able
to write.
</p><br>
<br><br>
<span id="title">The Real Reason To End The Death Penalty</span>
                <br>
                <span id="secondText">April 2021</span>
                <br><br><br><br>   
<p>  
When intellectuals talk about the death penalty, they talk about things like whether it's permissible for the state to
take someone's life, whether the death penalty acts as a deterrent, and whether more death sentences are given to some
groups than others. But in practice the debate about the death penalty is not about whether it's ok to kill murderers.
It's about whether it's ok to kill innocent people, because at least 4% of people on death row are innocent.
</p><br>
<p> 
When I was a kid I imagined that it was unusual for people to be convicted of crimes they hadn't committed, and that in
murder cases especially this must be very rare. Far from it. Now, thanks to organizations like the Innocence Project, we
see a constant stream of stories about murder convictions being overturned after new evidence emerges. Sometimes the
police and prosecutors were just very sloppy. Sometimes they were crooked, and knew full well they were convicting an
innocent person.
</p><br>
<p> 
Kenneth Adams and three other men spent 18 years in prison on a murder conviction. They were exonerated after DNA
testing implicated three different men, two of whom later confessed. The police had been told about the other men early
in the investigation, but never followed up the lead.
</p><br>
<p> 
Keith Harward spent 33 years in prison on a murder conviction. He was convicted because "experts" said his teeth matched
photos of bite marks on one victim. He was exonerated after DNA testing showed the murder had been committed by another
man, Jerry Crotty.
</p><br>
<p> 
Ricky Jackson and two other men spent 39 years in prison after being convicted of murder on the testimony of a 12 year
old boy, who later recanted and said he'd been coerced by police. Multiple people have confirmed the boy was elsewhere
at the time. The three men were exonerated after the county prosecutor dropped the charges, saying "The state is
conceding the obvious."
</p><br>
<p> 
Alfred Brown spent 12 years in prison on a murder conviction, including 10 years on death row. He was exonerated after
it was discovered that the assistant district attorney had concealed phone records proving he could not have committed
the crimes.
</p><br>
<p> 
Glenn Ford spent 29 years on death row after having been convicted of murder. He was exonerated after new evidence
proved he was not even at the scene when the murder occurred. The attorneys assigned to represent him had never tried a
jury case before.
</p><br>
<p> 
Cameron Willingham was actually executed in 2004 by lethal injection. The "expert" who testified that he deliberately
set fire to his house has since been discredited. A re-examination of the case ordered by the state of Texas in 2009
concluded that "a finding of arson could not be sustained."
</p><br>
<p> 
Rich Glossip has spent 20 years on death row after being convicted of murder on the testimony of the actual killer, who
escaped with a life sentence in return for implicating him. In 2015 he came within minutes of execution before it
emerged that Oklahoma had been planning to kill him with an illegal combination of drugs. They still plan to go ahead
with the execution, perhaps as soon as this summer, despite new evidence exonerating him.
</p><br>
<p> 
I could go on. There are hundreds of similar cases. In Florida alone, 29 death row prisoners have been exonerated so
far.
</p><br>
<p> 
Far from being rare, wrongful murder convictions are very common. Police are under pressure to solve a crime that has
gotten a lot of attention. When they find a suspect, they want to believe he's guilty, and ignore or even destroy
evidence suggesting otherwise. District attorneys want to be seen as effective and tough on crime, and in order to win
convictions are willing to manipulate witnesses and withhold evidence. Court-appointed defense attorneys are overworked
and often incompetent. There's a ready supply of criminals willing to give false testimony in return for a lighter
sentence, suggestible witnesses who can be made to say whatever police want, and bogus "experts" eager to claim that
science proves the defendant is guilty. And juries want to believe them, since otherwise some terrible crime remains
unsolved.
</p><br>
<p>
    This circus of incompetence and dishonesty is the real issue with the death penalty. We don't even reach the point
    where
    theoretical questions about the moral justification or effectiveness of capital punishment start to matter, because
    so
    many of the people sentenced to death are actually innocent. Whatever it means in theory, in practice capital
    punishment
    means killing innocent people.
</p><br>
<p>
    Thanks to Trevor Blackwell, Jessica Livingston, and Don Knight for reading drafts of this.
</p><br>
<br><br>
<span id="title">How People Get Rich Now</span>
                <br>
                <span id="secondText">April 2021</span>
                <br><br><br><br>   
<p>  
Every year since 1982, Forbes magazine has published a list of the richest Americans. If we compare the 100 richest
people in 1982 to the 100 richest in 2020, we notice some big differences.
</p><br>
<p>
In 1982 the most common source of wealth was inheritance. Of the 100 richest people, 60 inherited from an ancestor.
There were 10 du Pont heirs alone. By 2020 the number of heirs had been cut in half, accounting for only 27 of the
biggest 100 fortunes.
</p><br>
<p>
Why would the percentage of heirs decrease? Not because inheritance taxes increased. In fact, they decreased
significantly during this period. The reason the percentage of heirs has decreased is not that fewer people are
inheriting great fortunes, but that more people are making them.
</p><br>
<p>
How are people making these new fortunes? Roughly 3/4 by starting companies and 1/4 by investing. Of the 73 new fortunes
in 2020, 56 derive from founders' or early employees' equity (52 founders, 2 early employees, and 2 wives of founders),
and 17 from managing investment funds.
</p><br>
<p>
There were no fund managers among the 100 richest Americans in 1982. Hedge funds and private equity firms existed in
1982, but none of their founders were rich enough yet to make it into the top 100. Two things changed: fund managers
discovered new ways to generate high returns, and more investors were willing to trust them with their money. [1]
</p><br>
<p>
But the main source of new fortunes now is starting companies, and when you look at the data, you see big changes there
too. People get richer from starting companies now than they did in 1982, because the companies do different things.
</p><br>
<p>
In 1982, there were two dominant sources of new wealth: oil and real estate. Of the 40 new fortunes in 1982, at least 24
were due primarily to oil or real estate. Now only a small number are: of the 73 new fortunes in 2020, 4 were due to
real estate and only 2 to oil.
</p><br>
<p>
By 2020 the biggest source of new wealth was what are sometimes called "tech" companies. Of the 73 new fortunes, about
30 derive from such companies. These are particularly common among the richest of the rich: 8 of the top 10 fortunes in
2020 were new fortunes of this type.
</p><br>
<p>
Arguably it's slightly misleading to treat tech as a category. Isn't Amazon really a retailer, and Tesla a car maker?
Yes and no. Maybe in 50 years, when what we call tech is taken for granted, it won't seem right to put these two
businesses in the same category. But at the moment at least, there is definitely something they share in common that
distinguishes them. What retailer starts AWS? What car maker is run by someone who also has a rocket company?
</p><br>
<p>
The tech companies behind the top 100 fortunes also form a well-differentiated group in the sense that they're all
companies that venture capitalists would readily invest in, and the others mostly not. And there's a reason why: these
are mostly companies that win by having better technology, rather than just a CEO who's really driven and good at making
deals.
</p><br>
<p>
To that extent, the rise of the tech companies represents a qualitative change. The oil and real estate magnates of the
1982 Forbes 400 didn't win by making better technology. They won by being really driven and good at making deals. [2]
And indeed, that way of getting rich is so old that it predates the Industrial Revolution. The courtiers who got rich in
the (nominal) service of European royal houses in the 16th and 17th centuries were also, as a rule, really driven and
good at making deals.
</p><br>
<p>
People who don't look any deeper than the Gini coefficient look back on the world of 1982 as the good old days, because
those who got rich then didn't get as rich. But if you dig into how they got rich, the old days don't look so good. In
1982, 84% of the richest 100 people got rich by inheritance, extracting natural resources, or doing real estate deals.
Is that really better than a world in which the richest people get rich by starting tech companies?
</p><br>
<p>
Why are people starting so many more new companies than they used to, and why are they getting so rich from it? The
answer to the first question, curiously enough, is that it's misphrased. We shouldn't be asking why people are starting
companies, but why they're starting companies again. [3]
</p><br>
<p>
In 1892, the New York Herald Tribune compiled a list of all the millionaires in America. They found 4047 of them. How
many had inherited their wealth then? Only about 20% — less than the proportion of heirs today. And when you investigate
the sources of the new fortunes, 1892 looks even more like today. Hugh Rockoff found that "many of the richest ...
gained their initial edge from the new technology of mass production." [4]
</p><br>
<p>
So it's not 2020 that's the anomaly here, but 1982. The real question is why so few people had gotten rich from starting
companies in 1982. And the answer is that even as the Herald Tribune's list was being compiled, a wave of consolidation
was sweeping through the American economy. In the late 19th and early 20th centuries, financiers like J. P. Morgan
combined thousands of smaller companies into a few hundred giant ones with commanding economies of scale. By the end of
World War II, as Michael Lind writes, "the major sectors of the economy were either organized as government-backed
cartels or dominated by a few oligopolistic corporations." [5]
</p><br>
<p>
In 1960, most of the people who start startups today would have gone to work for one of them. You could get rich from
starting your own company in 1890 and in 2020, but in 1960 it was not really a viable option. You couldn't break through
the oligopolies to get at the markets. So the prestigious route in 1960 was not to start your own company, but to work
your way up the corporate ladder at an existing one. [6]
</p><br>
<p>
Making everyone a corporate employee decreased economic inequality (and every other kind of variation), but if your
model of normal is the mid 20th century, you have a very misleading model in that respect. J. P. Morgan's economy turned
out to be just a phase, and starting in the 1970s, it began to break up.
</p><br>
<p>
Why did it break up? Partly senescence. The big companies that seemed models of scale and efficiency in 1930 had by 1970
become slack and bloated. By 1970 the rigid structure of the economy was full of cosy nests that various groups had
built to insulate themselves from market forces. During the Carter administration the federal government realized
something was amiss and began, in a process they called "deregulation," to roll back the policies that propped up the
oligopolies.
</p><br>
<p>
But it wasn't just decay from within that broke up J. P. Morgan's economy. There was also pressure from without, in the
form of new technology, and particularly microelectronics. The best way to envision what happened is to imagine a pond
with a crust of ice on top. Initially the only way from the bottom to the surface is around the edges. But as the ice
crust weakens, you start to be able to punch right through the middle.
</p><br>
<p>
The edges of the pond were pure tech: companies that actually described themselves as being in the electronics or
software business. When you used the word "startup" in 1990, that was what you meant. But now startups are punching
right through the middle of the ice crust and displacing incumbents like retailers and TV networks and car companies.
[7]
</p><br>
<p>
But though the breakup of J. P. Morgan's economy created a new world in the technological sense, it was a reversion to
the norm in the social sense. If you only look back as far as the mid 20th century, it seems like people getting rich by
starting their own companies is a recent phenomenon. But if you look back further, you realize it's actually the
default. So what we should expect in the future is more of the same. Indeed, we should expect both the number and wealth
of founders to grow, because every decade it gets easier to start a startup.
</p><br>
<p>
Part of the reason it's getting easier to start a startup is social. Society is (re)assimilating the concept. If you
start one now, your parents won't freak out the way they would have a generation ago, and knowledge about how to do it
is much more widespread. But the main reason it's easier to start a startup now is that it's cheaper. Technology has
driven down the cost of both building products and acquiring customers.
</p><br>
<p>
The decreasing cost of starting a startup has in turn changed the balance of power between founders and investors. Back
when starting a startup meant building a factory, you needed investors' permission to do it at all. But now investors
need founders more than founders need investors, and that, combined with the increasing amount of venture capital
available, has driven up valuations. [8]
</p><br>
<p>
So the decreasing cost of starting a startup increases the number of rich people in two ways: it means that more people
start them, and that those who do can raise money on better terms.
</p><br>
<p>
But there's also a third factor at work: the companies themselves are more valuable, because newly founded companies
grow faster than they used to. Technology hasn't just made it cheaper to build and distribute things, but faster too.
</p><br>
<p>
This trend has been running for a long time. IBM, founded in 1896, took 45 years to reach a billion 2020 dollars in
revenue. Hewlett-Packard, founded in 1939, took 25 years. Microsoft, founded in 1975, took 13 years. Now the norm for
fast-growing companies is 7 or 8 years. [9]
</p><br>
<p>
Fast growth has a double effect on the value of founders' stock. The value of a company is a function of its revenue and
its growth rate. So if a company grows faster, you not only get to a billion dollars in revenue sooner, but the company
is more valuable when it reaches that point than it would be if it were growing slower.
</p><br>
<p>
That's why founders sometimes get so rich so young now. The low initial cost of starting a startup means founders can
start young, and the fast growth of companies today means that if they succeed they could be surprisingly rich just a
few years later.
</p><br>
<p>
It's easier now to start and grow a company than it has ever been. That means more people start them, that those who do
get better terms from investors, and that the resulting companies become more valuable. Once you understand how these
mechanisms work, and that startups were suppressed for most of the 20th century, you don't have to resort to some vague
right turn the country took under Reagan to explain why America's Gini coefficient is increasing. Of course the Gini
coefficient is increasing. With more people starting more valuable companies, how could it not be?
</p><br>
<p>
Notes
</p><br>
<p>
[1] Investment firms grew rapidly after a regulatory change by the Labor Department in 1978 allowed pension funds to
invest in them, but the effects of this growth were not yet visible in the top 100 fortunes in 1982.
</p><br>
<p>
[2] George Mitchell deserves mention as an exception. Though really driven and good at making deals, he was also the
first to figure out how to use fracking to get natural gas out of shale.
</p><br>
<p>
[3] When I say people are starting more companies, I mean the type of company meant to grow very big. There has actually
been a decrease in the last couple decades in the overall number of new companies. But the vast majority of companies
are small retail and service businesses. So what the statistics about the decreasing number of new businesses mean is
that people are starting fewer shoe stores and barber shops.
</p><br>
<p>
People sometimes get confused when they see a graph labelled "startups" that's going down, because there are two senses
of the word "startup": (1) the founding of a company, and (2) a particular type of company designed to grow big fast.
The statistics mean startup in sense (1), not sense (2).
</p><br>
<p>
[4] Rockoff, Hugh. "Great Fortunes of the Gilded Age." NBER Working Paper 14555, 2008.
</p><br>
<p>
[5] Lind, Michael. Land of Promise. HarperCollins, 2012.
</p><br>
<p>
It's also likely that the high tax rates in the mid 20th century deterred people from starting their own companies.
Starting one's own company is risky, and when risk isn't rewarded, people opt for safety instead.
</p><br>
<p>
But it wasn't simply cause and effect. The oligopolies and high tax rates of the mid 20th century were all of a piece.
Lower taxes are not just a cause of entrepreneurship, but an effect as well: the people getting rich in the mid 20th
century from real estate and oil exploration lobbied for and got huge tax loopholes that made their effective tax rate
much lower, and presumably if it had been more common to grow big companies by building new technology, the people doing
that would have lobbied for their own loopholes as well.
</p><br>
<p>
[6] That's why the people who did get rich in the mid 20th century so often got rich from oil exploration or real
estate. Those were the two big areas of the economy that weren't susceptible to consolidation.
</p><br>
<p>
[7] The pure tech companies used to be called "high technology" startups. But now that startups can punch through the
middle of the ice crust, we don't need a separate name for the edges, and the term "high-tech" has a decidedly retro
sound.
</p><br>
<p>
[8] Higher valuations mean you either sell less stock to get a given amount of money, or get more money for a given
amount of stock. The typical startup does some of each. Obviously you end up richer if you keep more stock, but you
should also end up richer if you raise more money, because (a) it should make the company more successful, and (b) you
should be able to last longer before the next round, or not even need one. Notice all those shoulds though. In practice
a lot of money slips through them.
</p><br>
<p>
It might seem that the huge rounds raised by startups nowadays contradict the claim that it has become cheaper to start
one. But there's no contradiction here; the startups that raise the most are the ones doing it by choice, in order to
grow faster, not the ones doing it because they need the money to survive. There's nothing like not needing money to
make people offer it to you.
</p><br>
<p>
You would think, after having been on the side of labor in its fight with capital for almost two centuries, that the far
left would be happy that labor has finally prevailed. But none of them seem to be. You can almost hear them saying "No,
no, not that way."
</p><br>
<p>
[9] IBM was created in 1911 by merging three companies, the most important of which was Herman Hollerith's Tabulating
Machine Company, founded in 1896. In 1941 its revenues were $60 million.
</p><br>
<p>
Hewlett-Packard's revenues in 1964 were $125 million.
</p><br>
<p>
Microsoft's revenues in 1988 were $590 million.
</p><br>
<p>
    Thanks to Trevor Blackwell, Jessica Livingston, Bob Lesko, Robert Morris, Russ Roberts, and Alex Tabarrok for reading
drafts of this, and to Jon Erlichman for growth data.
</p><br>
<br><br>
<span id="title">Write Simply</span>
                <br>
                <span id="secondText">March 2021</span>
                <br><br><br><br>   
<p>
I try to write using ordinary words and simple sentences.
</p><br>
<p>
That kind of writing is easier to read, and the easier something is to read, the more deeply readers will engage with
it. The less energy they expend on your prose, the more they'll have left for your ideas.
</p><br>
<p>
And the further they'll read. Most readers' energy tends to flag part way through an article or essay. If the friction
of reading is low enough, more keep going till the end.
</p><br>
<p>
There's an Italian dish called saltimbocca, which means "leap into the mouth." My goal when writing might be called
saltintesta: the ideas leap into your head and you barely notice the words that got them there.
</p><br>
<p>
It's too much to hope that writing could ever be pure ideas. You might not even want it to be. But for most writers,
most of the time, that's the goal to aim for. The gap between most writing and pure ideas is not filled with poetry.
</p><br>
<p>
Plus it's more considerate to write simply. When you write in a fancy way to impress people, you're making them do extra
work just so you can seem cool. It's like trailing a long train behind you that readers have to carry.
</p><br>
<p>
And remember, if you're writing in English, that a lot of your readers won't be native English speakers. Their
understanding of ideas may be way ahead of their understanding of English. So you can't assume that writing about a
difficult topic means you can use difficult words.
</p><br>
<p>
Of course, fancy writing doesn't just conceal ideas. It can also conceal the lack of them. That's why some people write
that way, to conceal the fact that they have nothing to say. Whereas writing simply keeps you honest. If you say nothing
simply, it will be obvious to everyone, including you.
</p><br>
<p>
Simple writing also lasts better. People reading your stuff in the future will be in much the same position as people
from other countries reading it today. The culture and the language will have changed. It's not vain to care about that,
any more than it's vain for a woodworker to build a chair to last.
</p><br>
<p>
Indeed, lasting is not merely an accidental quality of chairs, or writing. It's a sign you did a good job.
</p><br>
<p>
But although these are all real advantages of writing simply, none of them are why I do it. The main reason I write
simply is that it offends me not to. When I write a sentence that seems too complicated, or that uses unnecessarily
intellectual words, it doesn't seem fancy to me. It seems clumsy.
</p><br>
<p>
There are of course times when you want to use a complicated sentence or fancy word for effect. But you should never do
it by accident.
</p><br>
<p>
The other reason my writing ends up being simple is the way I do it. I write the first draft fast, then spend days
editing it, trying to get everything just right. Much of this editing is cutting, and that makes simple writing even
simpler.
</p><br>
<br><br>
<span id="title">Donate Unrestricted</span>
                <br>
                <span id="secondText">March 2021</span>
                <br><br><br><br>   
<p>
The secret curse of the nonprofit world is restricted donations. If you haven't been involved with nonprofits, you may
never have heard this phrase before. But if you have been, it probably made you wince.
</p><br>
<p>
Restricted donations mean donations where the donor limits what can be done with the money. This is common with big
donations, perhaps the default. And yet it's usually a bad idea. Usually the way the donor wants the money spent is not
the way the nonprofit would have chosen. Otherwise there would have been no need to restrict the donation. But who has a
better understanding of where money needs to be spent, the nonprofit or the donor?
</p><br>
<p>
If a nonprofit doesn't understand better than its donors where money needs to be spent, then it's incompetent and you
shouldn't be donating to it at all.
</p><br>
<p>
Which means a restricted donation is inherently suboptimal. It's either a donation to a bad nonprofit, or a donation for
the wrong things.
</p><br>
<p>
There are a couple exceptions to this principle. One is when the nonprofit is an umbrella organization. It's reasonable
to make a restricted donation to a university, for example, because a university is only nominally a single nonprofit.
Another exception is when the donor actually does know as much as the nonprofit about where money needs to be spent. The
Gates Foundation, for example, has specific goals and often makes restricted donations to individual nonprofits to
accomplish them. But unless you're a domain expert yourself or donating to an umbrella organization, your donation would
do more good if it were unrestricted.
</p><br>
<p>
If restricted donations do less good than unrestricted ones, why do donors so often make them? Partly because doing good
isn't donors' only motive. They often have other motives as well — to make a mark, or to generate good publicity [1], or
to comply with regulations or corporate policies. Many donors may simply never have considered the distinction between
restricted and unrestricted donations. They may believe that donating money for some specific purpose is just how
donation works. And to be fair, nonprofits don't try very hard to discourage such illusions. They can't afford to.
People running nonprofits are almost always anxious about money. They can't afford to talk back to big donors.
</p><br>
<p>
You can't expect candor in a relationship so asymmetric. So I'll tell you what nonprofits wish they could tell you. If
you want to donate to a nonprofit, donate unrestricted. If you trust them to spend your money, trust them to decide how.
</p><br>
<p>
Note
</p><br>
<p>
[1] Unfortunately restricted donations tend to generate more publicity than unrestricted ones. "X donates money to build
a school in Africa" is not only more interesting than "X donates money to Y nonprofit to spend as Y chooses," but also
focuses more attention on X.
</p><br>
<p>
Thanks to Chase Adam, Ingrid Bassett, Trevor Blackwell, and Edith Elliot for reading drafts of this.
</p><br>
<br><br>
<span id="title">What I Worked On</span>
                <br>
                <span id="secondText">February 2021</span>
                <br><br><br><br>   
<p>
Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays.
I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were
awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.
</p><br>
<p>
The first programs I tried writing were on the IBM 1401 that our school district used for what was then called "data
processing." This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our
junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair
down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised
floor under bright fluorescent lights.
</p><br>
<p>
The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the
card reader and press a button to load the program into memory and run it. The result would ordinarily be to print
something on the spectacularly loud printer.
</p><br>
<p>
I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have
done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on
punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of
pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any
programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for
programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a
technical error, as the data center manager's expression made clear.
</p><br>
<p>
With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that
could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then
stopping. [1]
</p><br>
<p>
The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly
how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.
</p><br>
<p>
Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a
TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really
started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word
processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so
he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.
</p><br>
<p>
Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which
sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to
which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was
that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate
truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.
</p><br>
<p>
I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and
they kept being boring. So I decided to switch to AI.
</p><br>
<p>
AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by
Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary
that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how
well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd
have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to
do was teach SHRDLU more words.
</p><br>
<p>
There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which
meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming
languages then were pretty primitive, and programmers' ideas correspondingly so. The default language at Cornell was a
Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a
program so fast that it was years before I started to have a sense of where the new limits were. This was more like it;
this was what I had expected college to do. It wasn't happening in a class, like it was supposed to, but that was ok.
For the next couple years I was on a roll. I knew what I was going to do.
</p><br>
<p>
For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing
bit of code, but what made it even more exciting was my belief — hard to imagine now, but not unique in 1985 — that it
was already climbing the lower slopes of intelligence.
</p><br>
<p>
I had gotten into a program at Cornell that didn't make you choose a major. You could take whatever classes you liked,
and choose whatever you liked to put on your degree. I of course chose "Artificial Intelligence." When I got the actual
physical diploma, I was dismayed to find that the quotes had been included, which made them read as scare-quotes. At the
time this bothered me, but now it seems amusingly accurate, for reasons I was about to discover.
</p><br>
<p>
I applied to 3 grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which I'd visited
because Rich Draves went there, and was also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU
clone. Only Harvard accepted me, so that was where I went.
</p><br>
<p>
I don't remember the moment it happened, or if there even was a specific moment, but during the first year of grad
school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program
that's told "the dog is sitting on the chair" translates this into some formal representation and adds it to the list of
things it knows.
</p><br>
<p>
What these programs really showed was that there's a subset of natural language that's a formal language. But a very
proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding
natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with
explicit data structures representing concepts, was not going to work. Its brokenness did, as so often happens, generate
a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to
get us Mike.
</p><br>
<p>
So I looked around to see what I could salvage from the wreckage of my plans, and there was Lisp. I knew from experience
that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main
reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp
hacking. It's scary to think how little I knew about Lisp hacking when I started writing that book. But there's nothing
like writing a book about something to help you learn it. The book, On Lisp, wasn't published till 1993, but I wrote
much of it in grad school.
</p><br>
<p>
Computer Science is an uneasy alliance between two halves, theory and systems. The theory people prove things, and the
systems people build things. I wanted to build things. I had plenty of respect for theory — indeed, a sneaking suspicion
that it was the more admirable of the two halves — but building things seemed so much more exciting.
</p><br>
<p>
The problem with systems work, though, was that it didn't last. Any program you wrote today, no matter how good, would
be obsolete in a couple decades at best. People might mention your software in footnotes, but no one would actually use
it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize
that, in its time, it had been good.
</p><br>
<p>
There were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play
around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No
one else wanted one either, so off they went. That was what happened to systems work.
</p><br>
<p>
I wanted not just to build things, but to build things that would last.
</p><br>
<p>
In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to
visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized
something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make
that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.
</p><br>
<p>
And moreover this was something you could make a living doing. Not as easily as you could by writing software, of
course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to
survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research
funding.
</p><br>
<p>
I had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I
knew intellectually that people made art — that it didn't just appear spontaneously — but it was as if the people who
made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in
profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost
miraculous.
</p><br>
<p>
That fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor,
Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.
</p><br>
<p>
So now I was in a PhD program in computer science, yet planning to be an artist, yet also genuinely in love with Lisp
hacking and working away at On Lisp. In other words, like many a grad student, I was working energetically on multiple
projects that were not my thesis.
</p><br>
<p>
I didn't see a way out of this situation. I didn't want to drop out of grad school, but how else was I going to get out?
I remember when my friend Robert Morris got kicked out of Cornell for writing the internet worm of 1988, I was envious
that he'd found such a spectacular way to get out of grad school.
</p><br>
<p>
Then one day in April 1990 a crack appeared in the wall. I ran into professor Cheatham and he asked if I was far enough
along to graduate that June. I didn't have a word of my dissertation written, but in what must have been the quickest
bit of thinking in my life, I decided to take a shot at writing one in the 5 weeks or so that remained before the
deadline, reusing parts of On Lisp where I could, and I was able to respond, with no perceptible delay "Yes, I think so.
I'll give you something to read in a few days."
</p><br>
<p>
I picked applications of continuations as the topic. In retrospect I should have written about macros and embedded
languages. There's a whole world there that's barely been explored. But all I wanted was to get out of grad school, and
my rapidly written dissertation sufficed, just barely.
</p><br>
<p>
Meanwhile I was applying to art schools. I applied to two: RISD in the US, and the Accademia di Belli Arti in Florence,
which, because it was the oldest art school, I imagined would be good. RISD accepted me, and I never heard back from the
Accademia, so off to Providence I went.
</p><br>
<p>
I'd applied for the BFA program at RISD, which meant in effect that I had to go to college again. This was not as
strange as it sounds, because I was only 25, and art schools are full of people of different ages. RISD counted me as a
transfer sophomore and said I had to do the foundation that summer. The foundation means the classes that everyone has
to take in fundamental subjects like drawing, color, and design.
</p><br>
<p>
Toward the end of the summer I got a big surprise: a letter from the Accademia, which had been delayed because they'd
sent it to Cambridge England instead of Cambridge Massachusetts, inviting me to take the entrance exam in Florence that
fall. This was now only weeks away. My nice landlady let me leave my stuff in her attic. I had some money saved from
consulting work I'd done in grad school; there was probably enough to last a year if I lived cheaply. Now all I had to
do was learn Italian.
</p><br>
<p>
Only stranieri (foreigners) had to take this entrance exam. In retrospect it may well have been a way of excluding them,
because there were so many stranieri attracted by the idea of studying art in Florence that the Italian students would
otherwise have been outnumbered. I was in decent shape at painting and drawing from the RISD foundation that summer, but
I still don't know how I managed to pass the written exam. I remember that I answered the essay question by writing
about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited
vocabulary. [2]
</p><br>
<p>
I'm only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august
institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed. The
students and faculty in the painting department at the Accademia were the nicest people you could imagine, but they had
long since arrived at an arrangement whereby the students wouldn't require the faculty to teach anything, and in return
the faculty wouldn't require the students to learn anything. And at the same time all involved would adhere outwardly to
the conventions of a 19th century atelier. We actually had one of those little stoves, fed with kindling, that you see
in 19th century studio paintings, and a nude model sitting as close to it as possible without getting burned. Except
hardly anyone else painted her besides me. The rest of the students spent their time chatting or occasionally trying to
imitate things they'd seen in American art magazines.
</p><br>
<p>
Our model turned out to live just down the street from me. She made a living from a combination of modelling and making
fakes for a local antique dealer. She'd copy an obscure old painting out of a book, and then he'd take the copy and
maltreat it to make it look old. [3]
</p><br>
<p>
While I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny,
because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time.
Painting still lives is different from painting people, because the subject, as its name suggests, can't move. People
can't sit for more than about 15 minutes at a time, and when they do they don't sit very still. So the traditional m.o.
for painting people is to know how to paint a generic person, which you then modify to match the specific person you're
painting. Whereas a still life you can, if you want, copy pixel by pixel from what you're seeing. You don't want to stop
there, of course, or you get merely photographic accuracy, and what makes a still life interesting is that it's been
through a head. You want to emphasize the visual cues that tell you, for example, that the reason the color changes
suddenly at a certain point is that it's the edge of an object. By subtly emphasizing such things you can make paintings
that are more realistic than photographs not just in some metaphorical sense, but in the strict information-theoretic
sense. [4]
</p><br>
<p>
I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren't consciously
aware of much we're seeing. Most visual perception is handled by low-level processes that merely tell your brain "that's
a water droplet" without telling you details like where the lightest and darkest points are, or "that's a bush" without
telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be
distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and
when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people
usually take for granted, just as you can after days of trying to write an essay about something people usually take for
granted.
</p><br>
<p>
This is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to
be worth trying.
</p><br>
<p>
Our teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down
in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was
running out, so at the end of the first year I went back to the US.
</p><br>
<p>
I wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and
then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents.
You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But
Interleaf still had a few years to live yet. [5]
</p><br>
<p>
Interleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the
scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing
I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was
the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of
the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during
certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way
of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time
surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.
</p><br>
<p>
The good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying
my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that
every hour, even when I was just sitting in a meeting. By living cheaply I not only managed to save enough to go back to
RISD, but also paid off my college loans.
</p><br>
<p>
I learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it's better for
technology companies to be run by product people than sales people (though sales is a real skill and people who are good
at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is
no bargain if it's depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic
customers are a dangerous source of money, and that there's not much overlap between conventional office hours and the
optimal time for hacking, or conventional offices and the optimal place for it.
</p><br>
<p>
But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the
high end: that it's good to be the "entry level" option, even though that will be less prestigious, because if you're
not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign.
</p><br>
<p>
When I left to go back to RISD the next fall, I arranged to do freelance work for the group that did projects for
customers, and this was how I survived for the next several years. When I came back to visit for a project later on,
someone told me about a new thing called HTML, which was, as he described it, a derivative of SGML. Markup language
enthusiasts were an occupational hazard at Interleaf and I ignored him, but this HTML thing later became a big part of
my life.
</p><br>
<p>
In the fall of 1992 I moved back to Providence to continue at RISD. The foundation had merely been intro stuff, and the
Accademia had been a (very civilized) joke. Now I was going to see what real art school was like. But alas it was more
like the Accademia than not. Better organized, certainly, and a lot more expensive, but it was now becoming clear that
art school did not bear the same relationship to art that medical school bore to medicine. At least not the painting
department. The textile department, which my next door neighbor belonged to, seemed to be pretty rigorous. No doubt
illustration and architecture were too. But painting was post-rigorous. Painting students were supposed to express
themselves, which to the more worldly ones meant to try to cook up some sort of distinctive signature style.
</p><br>
<p>
A signature style is the visual equivalent of what in show business is known as a "schtick": something that immediately
identifies the work as yours and no one else's. For example, when you see a painting that looks like a certain kind of
cartoon, you know it's by Roy Lichtenstein. So if you see a big painting of this type hanging in the apartment of a
hedge fund manager, you know he paid millions of dollars for it. That's not always why artists have a signature style,
but it's usually why buyers pay a lot for such work. [6]
</p><br>
<p>
There were plenty of earnest students too: kids who "could draw" in high school, and now had come to what was supposed
to be the best art school in the country, to learn to draw even better. They tended to be confused and demoralized by
what they found at RISD, but they kept going, because painting was what they did. I was not one of the kids who could
draw in high school, but at RISD I was definitely closer to their tribe than the tribe of signature style seekers.
</p><br>
<p>
I learned a lot in the color class I took at RISD, but otherwise I was basically teaching myself to paint, and I could
do that for free. So in 1993 I dropped out. I hung around Providence for a bit, and then my college friend Nancy Parmet
did me a big favor. A rent-controlled apartment in a building her mother owned in New York was becoming vacant. Did I
want it? It wasn't much more than my current place, and New York was supposed to be where the artists were. So yes, I
wanted it! [7]
</p><br>
<p>
Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You
can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's
not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist — in
the strictly technical sense of making paintings and living in New York.
</p><br>
<p>
I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was
very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was
lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a
popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and
spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around
this time.)
</p><br>
<p>
The best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the
early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her
students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her
de facto studio assistant.
</p><br>
<p>
She liked to paint on big, square canvases, 4 to 5 feet on a side. One day in late 1994 as I was stretching one of these
monsters there was something on the radio about a famous fund manager. He wasn't that much older than me, and was super
rich. The thought suddenly occurred to me: why don't I become rich? Then I'll be able to work on whatever I want.
</p><br>
<p>
Meanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me
when I visited him in Cambridge, where he was now in grad school at Harvard. It seemed to me that the web would be a big
deal. I'd seen what graphical user interfaces had done for the popularity of microcomputers. It seemed like the web
would do the same for the internet.
</p><br>
<p>
If I wanted to get rich, here was the next train leaving the station. I was right about that part. What I got wrong was
the idea. I decided we should start a company to put art galleries online. I can't honestly say, after reading so many Y
Combinator applications, that this was the worst startup idea ever, but it was up there. Art galleries didn't want to be
online, and still don't, not the fancy ones. That's not how they sell. I wrote some software to generate web sites for
galleries, and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up
galleries. To call this a difficult sale would be an understatement. It was difficult to give away. A few galleries let
us make sites for them for free, but none paid us.
</p><br>
<p>
Then some online stores started to appear, and I realized that except for the order buttons they were identical to the
sites we'd been generating for galleries. This impressive-sounding thing called an "internet storefront" was something
we already knew how to build.
</p><br>
<p>
So in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started
trying to write software to build online stores. At first this was going to be normal desktop software, which in those
days meant Windows software. That was an alarming prospect, because neither of us knew how to write Windows software or
wanted to learn. We lived in the Unix world. But we decided we'd at least try writing a prototype store builder on Unix.
Robert wrote a shopping cart, and I wrote a new site generator for stores — in Lisp, of course.
</p><br>
<p>
We were working out of Robert's apartment in Cambridge. His roommate was away for big chunks of time, during which I got
to sleep in his room. For some reason there was no bed frame or sheets, just a mattress on the floor. One morning as I
was lying on this mattress I had an idea that made me sit up like a capital L. What if we ran the software on the
server, and let users control it by clicking on links? Then we'd never have to write anything to run on users'
computers. We could generate the sites on the same server we'd serve them from. Users wouldn't need anything more than a
browser.
</p><br>
<p>
This kind of software, known as a web app, is common now, but at the time it wasn't clear that it was even possible. To
find out, we decided to try making a version of our store builder that you could control through the browser. A couple
days later, on August 12, we had one that worked. The UI was horrible, but it proved you could build a whole store
through the browser, without any client software or typing anything into the command line on the server.
</p><br>
<p>
Now we felt like we were really onto something. I had visions of a whole new generation of software working this way.
You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group called Release
Engineering that seemed to be at least as big as the group that actually wrote the software. Now you could just update
the software right on the server.
</p><br>
<p>
We started a new company we called Viaweb, after the fact that our software worked via the web, and we got $10,000 in
seed funding from Idelle's husband Julian. In return for that and doing the initial legal work and giving us business
advice, we gave him 10% of the company. Ten years later this deal became the model for Y Combinator's. We knew founders
needed something like this, because we'd needed it ourselves.
</p><br>
<p>
At this stage I had a negative net worth, because the thousand dollars or so I had in the bank was more than
counterbalanced by what I owed the government in taxes. (Had I diligently set aside the proper proportion of the money
I'd made consulting for Interleaf? No, I had not.) So although Robert had his graduate student stipend, I needed that
seed funding to live on.
</p><br>
<p>
We originally hoped to launch in September, but we got more ambitious about the software as we worked on it. Eventually
we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exactly like the
static ones that would be generated later, except that instead of leading to static pages, the links all referred to
closures stored in a hash table on the server.
</p><br>
<p>
It helped to have studied art, because the main goal of an online store builder is to make users look legit, and the key
to looking legit is high production values. If you get page layouts and fonts and colors right, you can make a guy
running a store out of his bedroom look more legit than a big company.
</p><br>
<p>
(If you're curious why my site looks so old-fashioned, it's because it's still made with this software. It may look
clunky today, but in 1996 it was the last word in slick.)
</p><br>
<p>
In September, Robert rebelled. "We've been working on this for a month," he said, "and it's still not done." This is
funny in retrospect, because he would still be working on it almost 3 years later. But I decided it might be prudent to
recruit more programmers, and I asked Robert who else in grad school with him was really good. He recommended Trevor
Blackwell, which surprised me at first, because at that point I knew Trevor mainly for his plan to reduce everything in
his life to a stack of notecards, which he carried around with him. But Rtm was right, as usual. Trevor turned out to be
a frighteningly effective hacker.
</p><br>
<p>
It was a lot of fun working with Robert and Trevor. They're the two most independent-minded people I know, and in
completely different ways. If you could see inside Rtm's brain it would look like a colonial New England church, and if
you could see inside Trevor's it would look like the worst excesses of Austrian Rococo.
</p><br>
<p>
We opened for business, with 6 stores, in January 1996. It was just as well we waited a few months, because although we
worried we were late, we were actually almost fatally early. There was a lot of talk in the press then about ecommerce,
but not many people actually wanted online stores. [8]
</p><br>
<p>
There were three main parts to the software: the editor, which people used to build sites and which I wrote, the
shopping cart, which Robert wrote, and the manager, which kept track of orders and statistics, and which Trevor wrote.
In its time, the editor was one of the best general-purpose site builders. I kept the code tight and didn't have to
integrate with any other software except Robert's and Trevor's, so it was quite fun to work on. If all I'd had to do was
work on this software, the next 3 years would have been the easiest of my life. Unfortunately I had to do a lot more,
all of it stuff I was worse at than programming, and the next 3 years were instead the most stressful.
</p><br>
<p>
There were a lot of startups making ecommerce software in the second half of the 90s. We were determined to be the
Microsoft Word, not the Interleaf. Which meant being easy to use and inexpensive. It was lucky for us that we were poor,
because that caused us to make Viaweb even more inexpensive than we realized. We charged $100 a month for a small store
and $300 a month for a big one. This low price was a big attraction, and a constant thorn in the sides of competitors,
but it wasn't because of some clever insight that we set the price low. We had no idea what businesses paid for things.
$300 a month seemed like a lot of money to us.
</p><br>
<p>
We did a lot of things right by accident like that. For example, we did what's now called "doing things that don't
scale," although at the time we would have described it as "being so lame that we're driven to the most desperate
measures to get users." The most common of which was building stores for them. This seemed particularly humiliating,
since the whole raison d'etre of our software was that people could use it to make their own stores. But anything to get
users.
</p><br>
<p>
We learned a lot more about retail than we wanted to know. For example, that if you could only have a small image of a
man's shirt (and all images were small then by present standards), it was better to have a closeup of the collar than a
picture of the whole shirt. The reason I remember learning this was that it meant I had to rescan about 30 images of
men's shirts. My first set of scans were so beautiful too.
</p><br>
<p>
Though this felt wrong, it was exactly the right thing to be doing. Building stores for users taught us about retail,
and about how it felt to use our software. I was initially both mystified and repelled by "business" and thought we
needed a "business person" to be in charge of it, but once we started to get users, I was converted, in much the same
way I was converted to fatherhood once I had kids. Whatever users wanted, I was all theirs. Maybe one day we'd have so
many users that I couldn't scan their images for them, but in the meantime there was nothing more important to do.
</p><br>
<p>
Another thing I didn't get at the time is that growth rate is the ultimate test of a startup. Our growth rate was fine.
We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered
was the absolute number of users. And that is the thing that matters in the sense that that's how much money you're
making, and if you're not making enough, you might go out of business. But in the long term the growth rate takes care
of the absolute number. If we'd been a startup I was advising at Y Combinator, I would have said: Stop being so stressed
out, because you're doing fine. You're growing 7x a year. Just don't hire too many more people and you'll soon be
profitable, and then you'll control your own destiny.
</p><br>
<p>
Alas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did
during the Internet Bubble. A company with just a handful of employees would have seemed amateurish. So we didn't reach
breakeven until about when Yahoo bought us in the summer of 1998. Which in turn meant we were at the mercy of investors
for the entire life of the company. And since both we and our investors were noobs at startups, the result was a mess
even by startup standards.
</p><br>
<p>
It was a huge relief when Yahoo bought us. In principle our Viaweb stock was valuable. It was a share in a business that
was profitable and growing rapidly. But it didn't feel very valuable to me; I had no idea how to value a business, but I
was all too keenly aware of the near-death experiences we seemed to have every few months. Nor had I changed my grad
student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since
we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were
by far the most luxurious thing I owned.
</p><br>
<p>
The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't
realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to
California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's
prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt
disconcertingly like working at Interleaf.
</p><br>
<p>
Yahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never
be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of
options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half
forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done
this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.
</p><br>
<p>
When I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the
kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was
because he thought I was lying. My options at that point were worth about $2 million a month. If I was leaving that kind
of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me.
This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire.
Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.
</p><br>
<p>
But I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years
getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same:
take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the
idea never occurred to me.
</p><br>
<p>
So I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know
many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a
beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New
York, where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up
like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even
though I didn't know any of them.
</p><br>
<p>
When I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my
old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was
raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little
restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented
with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on
canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully
hadn't rotted yet).
</p><br>
<p>
Meanwhile I looked for an apartment to buy. Now I could actually choose what neighborhood to live in. Where, I asked
myself and various real estate agents, is the Cambridge of New York? Aided by occasional visits to actual Cambridge, I
gradually realized there wasn't one. Huh.
</p><br>
<p>
Around this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were
the future. Why not build a web app for making web apps? Why not let people edit code on our server through the browser,
and then host the resulting applications for them? [9] You could run all sorts of services on the servers that these
applications could use just by making an API call: making and receiving phone calls, manipulating images, taking credit
card payments, etc.
</p><br>
<p>
I got so excited about this idea that I couldn't think about anything else. It seemed obvious that this was the future.
I didn't particularly want to start another company, but it was clear that this idea would have to be embodied as one,
so I decided to move to Cambridge and start it. I hoped to lure Robert into working on it with me, but there I ran into
a hitch. Robert was now a postdoc at MIT, and though he'd made a lot of money the last time I'd lured him into working
on one of my schemes, it had also been a huge time sink. So while he agreed that it sounded like a plausible idea, he
firmly refused to work on it.
</p><br>
<p>
Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted
summer jobs, and we got to work trying to build what it's now clear is about twenty companies and several open source
projects worth of software. The language for defining applications would of course be a dialect of Lisp. But I wasn't so
naive as to assume I could spring an overt Lisp on a general audience; we'd hide the parentheses, like Dylan did.
</p><br>
<p>
By then there was a name for the kind of company Viaweb was, an "application service provider," or ASP. This name didn't
last long before it was replaced by "software as a service," but it was current for long enough that I named this new
company after it: it was going to be called Aspra.
</p><br>
<p>
I started working on the application builder, Dan worked on network infrastructure, and the two undergrads worked on the
first two services (images and phone calls). But about halfway through the summer I realized I really didn't want to run
a company — especially not a big one, which it was looking like this would have to be. I'd only started Viaweb because I
needed the money. Now that I didn't need money anymore, why was I doing this? If this vision had to be realized as a
company, then screw the vision. I'd build a subset that could be done as an open source project.
</p><br>
<p>
Much to my surprise, the time I spent working on this stuff was not wasted after all. After we started Y Combinator, I
would often encounter startups working on parts of this new architecture, and it was very useful to have spent so much
time thinking about it and even trying to write some of it.
</p><br>
<p>
The subset I would build as an open source project was the new Lisp, whose parentheses I now wouldn't even have to hide.
A lot of Lisp hackers dream of building a new Lisp, partly because one of the distinctive features of the language is
that it has dialects, and partly, I think, because we have in our minds a Platonic form of Lisp that all existing
dialects fall short of. I certainly did. So at the end of the summer Dan and I switched to working on this new dialect
of Lisp, which I called Arc, in a house I bought in Cambridge.
</p><br>
<p>
The following spring, lightning struck. I was invited to give a talk at a Lisp conference, so I gave one about how we'd
used Lisp at Viaweb. Afterward I put a postscript file of this talk online, on paulgraham.com, which I'd created years
before using Viaweb but had never used for anything. In one day it got 30,000 page views. What on earth had happened?
The referring urls showed that someone had posted it on Slashdot. [10]
</p><br>
<p>
Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. That may seem
obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce
monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or
in a newspaper or magazine. Now anyone could publish anything.
</p><br>
<p>
This had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved
with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to
realize it. Even then it took me several years to understand the implications. It meant there would be a whole new
generation of essays. [11]
</p><br>
<p>
In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed
thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing
about their specialties. There were so many essays that had never been written, because there had been no way to publish
them. Now they could be, and I was going to write them. [12]
</p><br>
<p>
I've worked on several different things, but to the extent there was a turning point where I figured out what to work
on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write
essays too.
</p><br>
<p>
I knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on
their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this
point I knew enough to find that encouraging instead of discouraging.
</p><br>
<p>
One of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on
things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y
Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm
writing, and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious
intellectually in something like the way Latin is, also seems about as hip.
</p><br>
<p>
It's not that unprestigious types of work are good per se. But when you find yourself drawn to some kind of work despite
its current lack of prestige, it's a sign both that there's something real to be discovered there, and that you have the
right kind of motives. Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it
will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the
right track, it at least guarantees you're not on the most common type of wrong one.
</p><br>
<p>
Over the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection
of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some
more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for
groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use
as an office.
</p><br>
<p>
One night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was
one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two
thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I
didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.
</p><br>
<p>
Jessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the
next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how
colorful their stories were. So she decided to compile a book of interviews with startup founders.
</p><br>
<p>
When the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early
2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this
time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger
number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical
founders instead of MBAs, they should let the founders remain as CEO, and so on.
</p><br>
<p>
One of my tricks for writing essays had always been to give talks. The prospect of having to stand up in front of a
group of people and tell them something that won't waste their time is a great spur to the imagination. When the Harvard
Computer Society, the undergrad computer club, asked me to give a talk, I decided I would tell them how to start a
startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.
</p><br>
<p>
So I gave this talk, in the course of which I told them that the best sources of seed funding were successful startup
founders, because then they'd be sources of advice too. Whereupon it seemed they were all looking expectantly at me.
Horrified at the prospect of having my inbox flooded by business plans (if I'd only known), I blurted out "But not me!"
and went on with the talk. But afterward it occurred to me that I should really stop procrastinating about angel
investing. I'd been meaning to since Yahoo bought us, and now it was 7 years later and I still hadn't done one angel
investment.
</p><br>
<p>
Meanwhile I had been scheming with Robert and Trevor about projects we could work on together. I missed working with
them, and it seemed like there had to be something we could collaborate on.
</p><br>
<p>
As Jessica and I were walking home from dinner on March 11, at the corner of Garden and Walker streets, these three
threads converged. Screw the VCs who were taking so long to make up their minds. We'd start our own investment firm and
actually implement the ideas we'd been talking about. I'd fund it, and Jessica could quit her job and work for it, and
we'd get Robert and Trevor as partners too. [13]
</p><br>
<p>
Once again, ignorance worked in our favor. We had no idea how to be angel investors, and in Boston in 2005 there were no
Ron Conways to learn from. So we just made what seemed like the obvious choices, and some of the things we did turned
out to be novel.
</p><br>
<p>
There are multiple components to Y Combinator, and we didn't figure them all out at once. The part we got first was to
be an angel firm. In those days, those two words didn't go together. There were VC firms, which were organized companies
with people whose job it was to make investments, but they only did big, million dollar investments. And there were
angels, who did smaller investments, but these were individuals who were usually focused on other things and made
investments on the side. And neither of them helped founders enough in the beginning. We knew how helpless founders were
in some respects, because we remembered how helpless we'd been. For example, one thing Julian had done for us that
seemed to us like magic was to get us set up as a company. We were fine writing fairly difficult software, but actually
getting incorporated, with bylaws and stock and all that stuff, how on earth did you do that? Our plan was not only to
make seed investments, but to do for startups everything Julian had done for us.
</p><br>
<p>
YC was not organized as a fund. It was cheap enough to run that we funded it with our own money. That went right by 99%
of readers, but professional investors are thinking "Wow, that means they got all the returns." But once again, this was
not due to any particular insight on our part. We didn't know how VC firms were organized. It never occurred to us to
try to raise a fund, and if it had, we wouldn't have known where to start. [14]
</p><br>
<p>
The most distinctive thing about YC is the batch model: to fund a bunch of startups all at once, twice a year, and then
to spend three months focusing intensively on trying to help them. That part we discovered by accident, not merely
implicitly but explicitly due to our ignorance about investing. We needed to get experience as investors. What better
way, we thought, than to fund a whole bunch of startups at once? We knew undergrads got temporary jobs at tech companies
during the summer. Why not organize a summer program where they'd start startups instead? We wouldn't feel guilty for
being in a sense fake investors, because they would in a similar sense be fake founders. So while we probably wouldn't
make much money out of it, we'd at least get to practice being investors on them, and they for their part would probably
have a more interesting summer than they would working at Microsoft.
</p><br>
<p>
We'd use the building I owned in Cambridge as our headquarters. We'd all have dinner there once a week — on tuesdays,
since I was already cooking for the thursday diners on thursdays — and after dinner we'd bring in experts on startups to
give talks.
</p><br>
<p>
We knew undergrads were deciding then about summer jobs, so in a matter of days we cooked up something we called the
Summer Founders Program, and I posted an announcement on my site, inviting undergrads to apply. I had never imagined
that writing essays would be a way to get "deal flow," as investors call it, but it turned out to be the perfect source.
[15] We got 225 applications for the Summer Founders Program, and we were surprised to find that a lot of them were from
people who'd already graduated, or were about to that spring. Already this SFP thing was starting to feel more serious
than we'd intended.
</p><br>
<p>
We invited about 20 of the 225 groups to interview in person, and from those we picked 8 to fund. They were an
impressive group. That first batch included reddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron
Swartz, who had already helped write the RSS spec and would a few years later become a martyr for open access, and Sam
Altman, who would later become the second president of YC. I don't think it was entirely luck that the first batch was
so good. You had to be pretty bold to sign up for a weird thing like the Summer Founders Program instead of a summer job
at a legit place like Microsoft or Goldman Sachs.
</p><br>
<p>
The deal for startups was based on a combination of the deal we did with Julian ($10k for 10%) and what Robert said MIT
grad students got for the summer ($6k). We invested $6k per founder, which in the typical two-founder case was $12k, in
return for 6%. That had to be fair, because it was twice as good as the deal we ourselves had taken. Plus that first
summer, which was really hot, Jessica brought the founders free air conditioners. [16]
</p><br>
<p>
Fairly quickly I realized that we had stumbled upon the way to scale startup funding. Funding startups in batches was
more convenient for us, because it meant we could do things for a lot of startups at once, but being part of a batch was
better for the startups too. It solved one of the biggest problems faced by founders: the isolation. Now you not only
had colleagues, but colleagues who understood the problems you were facing and could tell you how they were solving
them.
</p><br>
<p>
As YC grew, we started to notice other advantages of scale. The alumni became a tight community, dedicated to helping
one another, and especially the current batch, whose shoes they remembered being in. We also noticed that the startups
were becoming one another's customers. We used to refer jokingly to the "YC GDP," but as YC grows this becomes less and
less of a joke. Now lots of startups get their initial set of customers almost entirely from among their batchmates.
</p><br>
<p>
I had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on
YC. As YC grew, and I grew more excited about it, it started to take up a lot more than a third of my attention. But for
the first few years I was still able to work on other things.
</p><br>
<p>
In the summer of 2006, Robert and I started working on a new version of Arc. This one was reasonably fast, because it
was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news
aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing
but startups. Plus it wasn't startup founders we wanted to reach. It was future startup founders. So I changed the name
to Hacker News and the topic to whatever engaged one's intellectual curiosity.
</p><br>
<p>
HN was no doubt good for YC, but it was also by far the biggest source of stress for me. If all I'd had to do was select
and help founders, life would have been so easy. And that implies that HN was a mistake. Surely the biggest source of
stress in one's work should at least be something close to the core of the work. Whereas I was like someone who was in
pain while running a marathon not from the exertion of running, but because I had a blister from an ill-fitting shoe.
When I was dealing with some urgent problem during YC, there was about a 60% chance it had to do with HN, and a 40%
chance it had do with everything else combined. [17]
</p><br>
<p>
As well as HN, I wrote all of YC's internal software in Arc. But while I continued to work a good deal in Arc, I
gradually stopped working on Arc, partly because I didn't have time to, and partly because it was a lot less attractive
to mess around with the language now that we had all this infrastructure depending on it. So now my three projects were
reduced to two: writing essays and working on YC.
</p><br>
<p>
YC was different from other kinds of work I've done. Instead of deciding for myself what to work on, the problems came
to me. Every 6 months there was a new batch of startups, and their problems, whatever they were, became our problems. It
was very engaging work, because their problems were quite varied, and the good founders were very effective. If you were
trying to learn the most you could about startups in the shortest possible time, you couldn't have picked a better way
to do it.
</p><br>
<p>
There were parts of the job I didn't like. Disputes between cofounders, figuring out when people were lying to us,
fighting with people who maltreated the startups, and so on. But I worked hard even at the parts I didn't like. I was
haunted by something Kevin Hale once said about companies: "No one works harder than the boss." He meant it both
descriptively and prescriptively, and it was the second part that scared me. I wanted YC to be good, so if how hard I
worked set the upper bound on how hard everyone else worked, I'd better work very hard.
</p><br>
<p>
One day in 2010, when he was visiting California for interviews, Robert Morris did something astonishing: he offered me
unsolicited advice. I can only remember him doing that once before. One day at Viaweb, when I was bent over double from
a kidney stone, he suggested that it would be a good idea for him to take me to the hospital. That was what it took for
Rtm to offer unsolicited advice. So I remember his exact words very clearly. "You know," he said, "you should make sure
Y Combinator isn't the last cool thing you do."
</p><br>
<p>
At the time I didn't understand what he meant, but gradually it dawned on me that he was saying I should quit. This
seemed strange advice, because YC was doing great. But if there was one thing rarer than Rtm offering advice, it was Rtm
being wrong. So this set me thinking. It was true that on my current trajectory, YC would be the last thing I did,
because it was only taking up more of my attention. It had already eaten Arc, and was in the process of eating essays
too. Either YC was my life's work or I'd have to leave eventually. And it wasn't, so I would.
</p><br>
<p>
In the summer of 2012 my mother had a stroke, and the cause turned out to be a blood clot caused by colon cancer. The
stroke destroyed her balance, and she was put in a nursing home, but she really wanted to get out of it and back to her
house, and my sister and I were determined to help her do it. I used to fly up to Oregon to visit her regularly, and I
had a lot of time to think on those flights. On one of them I realized I was ready to hand YC over to someone else.
</p><br>
<p>
I asked Jessica if she wanted to be president, but she didn't, so we decided we'd try to recruit Sam Altman. We talked
to Robert and Trevor and we agreed to make it a complete changing of the guard. Up till that point YC had been
controlled by the original LLC we four had started. But we wanted YC to last for a long time, and to do that it couldn't
be controlled by the founders. So if Sam said yes, we'd let him reorganize YC. Robert and I would retire, and Jessica
and Trevor would become ordinary partners.
</p><br>
<p>
When we asked Sam if he wanted to be president of YC, initially he said no. He wanted to start a startup to make nuclear
reactors. But I kept at it, and in October 2013 he finally agreed. We decided he'd take over starting with the winter
2014 batch. For the rest of 2013 I left running YC more and more to Sam, partly so he could learn the job, and partly
because I was focused on my mother, whose cancer had returned.
</p><br>
<p>
She died on January 15, 2014. We knew this was coming, but it was still hard when it did.
</p><br>
<p>
I kept working on YC till March, to help get that batch of startups through Demo Day, then I checked out pretty
completely. (I still talk to alumni and to new startups working on things I'm interested in, but that only takes a few
hours a week.)
</p><br>
<p>
What should I do next? Rtm's advice hadn't included anything about that. I wanted to do something completely different,
so I decided I'd paint. I wanted to see how good I could get if I really focused on it. So the day after I stopped
working on YC, I started painting. I was rusty and it took a while to get back into shape, but it was at least
completely engaging. [18]
</p><br>
<p>
I spent most of the rest of 2014 painting. I'd never been able to work so uninterruptedly before, and I got to be better
than I had been. Not good enough, but better. Then in November, right in the middle of a painting, I ran out of steam.
Up till that point I'd always been curious to see how the painting I was working on would turn out, but suddenly
finishing this one seemed like a chore. So I stopped working on it and cleaned my brushes and haven't painted since. So
far anyway.
</p><br>
<p>
I realize that sounds rather wimpy. But attention is a zero sum game. If you can choose what to work on, and you choose
a project that's not the best one (or at least a good one) for you, then it's getting in the way of another project that
is. And at 50 there was some opportunity cost to screwing around.
</p><br>
<p>
I started writing essays again, and wrote bunch of new ones over the next few months. I even wrote a couple that weren't
about startups. Then in March 2015 I started working on Lisp again.
</p><br>
<p>
The distinctive thing about Lisp is that its core is a language defined by writing an interpreter in itself. It wasn't
originally intended as a programming language in the ordinary sense. It was meant to be a formal model of computation,
an alternative to the Turing machine. If you want to write an interpreter for a language in itself, what's the minimum
set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer
to that question. [19]
</p><br>
<p>
McCarthy didn't realize this Lisp could even be used to program computers till his grad student Steve Russell suggested
it. Russell translated McCarthy's interpreter into IBM 704 machine language, and from that point Lisp started also to be
a programming language in the ordinary sense. But its origins as a model of computation gave it a power and elegance
that other languages couldn't match. It was this that attracted me in college, though I didn't understand why at the
time.
</p><br>
<p>
McCarthy's 1960 Lisp did nothing more than interpret Lisp expressions. It was missing a lot of things you'd want in a
programming language. So these had to be added, and when they were, they weren't defined using McCarthy's original
axiomatic approach. That wouldn't have been feasible at the time. McCarthy tested his interpreter by hand-simulating the
execution of programs. But it was already getting close to the limit of interpreters you could test that way — indeed,
there was a bug in it that McCarthy had overlooked. To test a more complicated interpreter, you'd have had to run it,
and computers then weren't powerful enough.
</p><br>
<p>
Now they are, though. Now you could continue using McCarthy's axiomatic approach till you'd defined a complete
programming language. And as long as every change you made to McCarthy's Lisp was a discoveredness-preserving
transformation, you could, in principle, end up with a complete language that had this quality. Harder to do than to
talk about, of course, but if it was possible in principle, why not try? So I decided to take a shot at it. It took 4
years, from March 26, 2015 to October 12, 2019. It was fortunate that I had a precisely defined goal, or it would have
been hard to keep at it for so long.
</p><br>
<p>
I wrote this new Lisp, called Bel, in itself in Arc. That may sound like a contradiction, but it's an indication of the
sort of trickery I had to engage in to make this work. By means of an egregious collection of hacks I managed to make
something close enough to an interpreter written in itself that could actually run. Not fast, but fast enough to test.
</p><br>
<p>
I had to ban myself from writing essays during most of this time, or I would never have finished. In late 2015 I spent 3
months writing essays, and when I went back to working on Bel I could barely understand the code. Not so much because it
was badly written as because the problem is so convoluted. When you're working on an interpreter written in itself, it's
hard to keep track of what's happening at what level, and errors can be practically encrypted by the time you get them.
</p><br>
<p>
So I said no more essays till Bel was done. But I told few people about Bel while I was working on it. So for years it
must have seemed that I was doing nothing, when in fact I was working harder than I'd ever worked on anything.
Occasionally after wrestling for hours with some gruesome bug I'd check Twitter or HN and see someone asking "Does Paul
Graham still code?"
</p><br>
<p>
Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the
code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring
out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I
was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had
more moments like this over the next few years.
</p><br>
<p>
In the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and
since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked
it so much that we still live there. So most of Bel was written in England.
</p><br>
<p>
In the fall of 2019, Bel was finally finished. Like McCarthy's original Lisp, it's a spec rather than an implementation,
although like McCarthy's Lisp it's a spec expressed as code.
</p><br>
<p>
Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writing essays through
2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I
chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long
and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be
interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for
others to read, and this is the last sentence of it.
</p><br>
<p>
Notes
</p><br>
<p>
[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went
straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.
</p><br>
<p>
[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for
occasional traps like polluzione). It's the everyday words that differ. So if you string together a lot of abstract
concepts with a few simple verbs, you can make a little Italian go a long way.
</p><br>
<p>
[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight down the spine of old Florence: past the
Pitti, across the bridge, past Orsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli to Piazza
San Marco. I saw Florence at street level in every possible condition, from empty dark winter evenings to sweltering
summer days when the streets were packed with tourists.
</p><br>
<p>
[4] You can of course paint people like still lives if you want to, and they're willing. That sort of portrait is
arguably the apex of still life painting, though the long sitting does tend to produce pained expressions in the
sitters.
</p><br>
<p>
[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by
Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end,
special-purpose hardware and software companies like a bulldozer.
</p><br>
<p>
[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly
coupled. Anything expensive comes to be seen as cool, and anything seen as cool will soon become equally expensive.
</p><br>
<p>
[7] Technically the apartment wasn't rent-controlled but rent-stabilized, but this is a refinement only New Yorkers
would know or care about. The point is that it was really cheap, less than half market price.
</p><br>
<p>
[8] Most software you can launch as soon as it's done. But when the software is an online store builder and you're
hosting the stores, if you don't have any users yet, that fact will be painfully obvious. So before we could launch
publicly we had to launch privately, in the sense of recruiting an initial set of users and making sure they had
decent-looking stores.
</p><br>
<p>
[9] We'd had a code editor in Viaweb for users to define their own page styles. They didn't know it, but they were
editing Lisp expressions underneath. But this wasn't an app editor, because the code ran when the merchants' sites were
generated, not when shoppers visited them.
</p><br>
<p>
[10] This was the first instance of what is now a familiar experience, and so was what happened next, when I read the
comments and found they were full of angry people. How could I claim that Lisp was better than other languages? Weren't
they all Turing complete? People who see the responses to essays I write sometimes tell me how sorry they feel for me,
but I'm not exaggerating when I reply that it has always been like this, since the very beginning. It comes with the
territory. An essay must tell readers things they don't already know, and some people dislike being told such things.
</p><br>
<p>
[11] People put plenty of stuff on the internet in the 90s of course, but putting something online is not the same as
publishing it online. Publishing online means you treat the online version as the (or at least a) primary version.
</p><br>
<p>
[12] There is a general lesson here that our experience with Y Combinator also teaches: Customs continue to constrain
you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs
about publishing essays, been based on real constraints. Startups had once been much more expensive to start, and
proportionally rare. Now they could be cheap and common, but the VCs' customs still reflected the old world, just as
customs about writing essays still reflected the constraints of the print era.
</p><br>
<p>
Which in turn implies that people who are independent-minded (i.e. less influenced by custom) will have an advantage in
fields affected by rapid change (where customs are more likely to be obsolete).
</p><br>
<p>
Here's an interesting point, though: you can't always predict which fields will be affected by rapid change. Obviously
software and venture capital will be, but who would have predicted that essay writing would be?
</p><br>
<p>
[13] Y Combinator was not the original name. At first we were called Cambridge Seed. But we didn't want a regional name,
in case someone copied us in Silicon Valley, so we renamed ourselves after one of the coolest tricks in the lambda
calculus, the Y combinator.
</p><br>
<p>
I picked orange as our color partly because it's the warmest, and partly because no VC used it. In 2005 all the VCs used
staid colors like maroon, navy blue, and forest green, because they were trying to appeal to LPs, not founders. The YC
logo itself is an inside joke: the Viaweb logo had been a white V on a red circle, so I made the YC logo a white Y on an
orange square.
</p><br>
<p>
[14] YC did become a fund for a couple years starting in 2009, because it was getting so big I could no longer afford to
fund it personally. But after Heroku got bought we had enough money to go back to being self-funded.
</p><br>
<p>
[15] I've never liked the term "deal flow," because it implies that the number of new startups at any given time is
fixed. This is not only false, but it's the purpose of YC to falsify it, by causing startups to be founded that would
not otherwise have existed.
</p><br>
<p>
[16] She reports that they were all different shapes and sizes, because there was a run on air conditioners and she had
to get whatever she could, but that they were all heavier than she could carry now.
</p><br>
<p>
[17] Another problem with HN was a bizarre edge case that occurs when you both write essays and run a forum. When you
run a forum, you're assumed to see if not every conversation, at least every conversation involving you. And when you
write essays, people post highly imaginative misinterpretations of them on forums. Individually these two phenomena are
tedious but bearable, but the combination is disastrous. You actually have to respond to the misinterpretations, because
the assumption that you're present in the conversation means that not responding to any sufficiently upvoted
misinterpretation reads as a tacit admission that it's correct. But that in turn encourages more; anyone who wants to
pick a fight with you senses that now is their chance.
</p><br>
<p>
[18] The worst thing about leaving YC was not working with Jessica anymore. We'd been working on YC almost the whole
time we'd known each other, and we'd neither tried nor wanted to separate it from our personal lives, so leaving was
like pulling up a deeply rooted tree.
</p><br>
<p>
[19] One way to get more precise about the concept of invented vs discovered is to talk about space aliens. Any
sufficiently advanced alien civilization would certainly know about the Pythagorean theorem, for example. I believe,
though with less certainty, that they would also know about the Lisp in McCarthy's 1960 paper.
</p><br>
<p>
But if so there's no reason to suppose that this is the limit of the language that might be known to them. Presumably
aliens need numbers and errors and I/O too. So it seems likely there exists at least one path out of McCarthy's Lisp
along which discoveredness is preserved.
</p><br>
<p>
Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Jessica Livingston, Robert
Morris, and Harj Taggar for reading drafts of this.
</p><br>
<br><br>
<span id="title">Earnestness</span>
                <br>
                <span id="secondText">December 2020</span>
                <br><br><br><br>   
<p>
    Jessica and I have certain words that have special significance when we're talking about startups. The highest
    compliment we can pay to founders is to describe them as "earnest." This is not by itself a guarantee of success. You
    could be earnest but incapable. But when founders are both formidable (another of our words) and earnest, they're as
    close to unstoppable as you get.
</p><br>
<p>
    Earnestness sounds like a boring, even Victorian virtue. It seems a bit of an anachronism that people in Silicon Valley
    would care about it. Why does this matter so much?
</p><br>
<p>   
    When you call someone earnest, you're making a statement about their motives. It means both that they're doing something
    for the right reasons, and that they're trying as hard as they can. If we imagine motives as vectors, it means both the
    direction and the magnitude are right. Though these are of course related: when people are doing something for the right
    reasons, they try harder. [1]
</p><br>
<p>   
    The reason motives matter so much in Silicon Valley is that so many people there have the wrong ones. Starting a
    successful startup makes you rich and famous. So a lot of the people trying to start them are doing it for those
    reasons. Instead of what? Instead of interest in the problem for its own sake. That is the root of earnestness. [2]
</p><br>
<p>   
    It's also the hallmark of a nerd. Indeed, when people describe themselves as "x nerds," what they mean is that they're
    interested in x for its own sake, and not because it's cool to be interested in x, or because of what they can get from
    it. They're saying they care so much about x that they're willing to sacrifice seeming cool for its sake.
</p><br>
<p>   
    A genuine interest in something is a very powerful motivator — for some people, the most powerful motivator of all. [3]
    Which is why it's what Jessica and I look for in founders. But as well as being a source of strength, it's also a source
    of vulnerability. Caring constrains you. The earnest can't easily reply in kind to mocking banter, or put on a cool
    facade of nihil admirari. They care too much. They are doomed to be the straight man. That's a real disadvantage in your
    teenage years, when mocking banter and nihil admirari often have the upper hand. But it becomes an advantage later.
</p><br>
<p>   
    It's a commonplace now that the kids who were nerds in high school become the cool kids' bosses later on. But people
    misunderstand why this happens. It's not just because the nerds are smarter, but also because they're more earnest. When
    the problems get harder than the fake ones you're given in high school, caring about them starts to matter.
</p><br>
<p>   
    Does it always matter? Do the earnest always win? Not always. It probably doesn't matter much in politics, or in crime,
    or in certain types of business that are similar to crime, like gambling, personal injury law, patent trolling, and so
    on. Nor does it matter in academic fields at the more bogus end of the spectrum. And though I don't know enough to say
    for sure, it may not matter in some kinds of humor: it may be possible to be completely cynical and still be very funny.
    [4]
</p><br>
<p>   
    Looking at the list of fields I mentioned, there's an obvious pattern. Except possibly for humor, these are all types of
    work I'd avoid like the plague. So that could be a useful heuristic for deciding which fields to work in: how much does
    earnestness matter? Which can in turn presumably be inferred from the prevalence of nerds at the top.
</p><br>
<p>   
    Along with "nerd," another word that tends to be associated with earnestness is "naive." The earnest often seem naive.
    It's not just that they don't have the motives other people have. They often don't fully grasp that such motives exist.
    Or they may know intellectually that they do, but because they don't feel them, they forget about them. [5]
</p><br>
<p>   
    It works to be slightly naive not just about motives but also, believe it or not, about the problems you're working on.
    Naive optimism can compensate for the bit rot that rapid change causes in established beliefs. You plunge into some
    problem saying "How hard can it be?", and then after solving it you learn that it was till recently insoluble.
</p><br>
<p>   
    Naivete is an obstacle for anyone who wants to seem sophisticated, and this is one reason would-be intellectuals find it
    so difficult to understand Silicon Valley. It hasn't been safe for such people to use the word "earnest" outside scare
    quotes since Oscar Wilde wrote "The Importance of Being Earnest" in 1895. And yet when you zoom in on Silicon Valley,
    right into Jessica Livingston's brain, that's what her x-ray vision is seeking out in founders. Earnestness! Who'd have
    guessed? Reporters literally can't believe it when founders making piles of money say that they started their companies
    to make the world better. The situation seems made for mockery. How can these founders be so naive as not to realize how
    implausible they sound?
</p><br>
<p>   
    Though those asking this question don't realize it, that's not a rhetorical question.
</p><br>
<p>   
    A lot of founders are faking it, of course, particularly the smaller fry, and the soon to be smaller fry. But not all of
    them. There are a significant number of founders who really are interested in the problem they're solving mainly for its
    own sake.
</p><br>
<p>   
    Why shouldn't there be? We have no difficulty believing that people would be interested in history or math or even old
    bus tickets for their own sake. Why can't there be people interested in self-driving cars or social networks for their
    own sake? When you look at the question from this side, it seems obvious there would be. And isn't it likely that having
    a deep interest in something would be a source of great energy and resilience? It is in every other field.
</p><br>
<p>   
    The question really is why we have a blind spot about business. And the answer to that is obvious if you know enough
    history. For most of history, making large amounts of money has not been very intellectually interesting. In
    preindustrial times it was never far from robbery, and some areas of business still retain that character, except using
    lawyers instead of soldiers.
</p><br>
<p>   
    But there are other areas of business where the work is genuinely interesting. Henry Ford got to spend much of his time
    working on interesting technical problems, and for the last several decades the trend in that direction has been
    accelerating. It's much easier now to make a lot of money by working on something you're interested in than it was 50
    years ago. And that, rather than how fast they grow, may be the most important change that startups represent. Though
    indeed, the fact that the work is genuinely interesting is a big part of why it gets done so fast. [6]
</p><br>
<p>   
    Can you imagine a more important change than one in the relationship between intellectual curiosity and money? These are
    two of the most powerful forces in the world, and in my lifetime they've become significantly more aligned. How could
    you not be fascinated to watch something like this happening in real time?
</p><br>
<p>   
    I meant this essay to be about earnestness generally, and now I've gone and talked about startups again. But I suppose
    at least it serves as an example of an x nerd in the wild.
</p><br>
<p> Notes
</p><br>
<p>  
    [1] It's interesting how many different ways there are not to be earnest: to be cleverly cynical, to be superficially
    brilliant, to be conspicuously virtuous, to be cool, to be sophisticated, to be orthodox, to be a snob, to bully, to
    pander, to be on the make. This pattern suggests that earnestness is not one end of a continuum, but a target one can
    fall short of in multiple dimensions.
</p><br>
<p>  
    Another thing I notice about this list is that it sounds like a list of the ways people behave on Twitter. Whatever else
    social media is, it's a vivid catalogue of ways not to be earnest.
</p><br>
<p>   
    [2] People's motives are as mixed in Silicon Valley as anywhere else. Even the founders motivated mostly by money tend
    to be at least somewhat interested in the problem they're solving, and even the founders most interested in the problem
    they're solving also like the idea of getting rich. But there's great variation in the relative proportions of different
    founders' motivations.
</p><br>
<p>   
    And when I talk about "wrong" motives, I don't mean morally wrong. There's nothing morally wrong with starting a startup
    to make money. I just mean that those startups don't do as well.
</p><br>
<p>    
    [3] The most powerful motivator for most people is probably family. But there are some for whom intellectual curiosity
    comes first. In his (wonderful) autobiography, Paul Halmos says explicitly that for a mathematician, math must come
    before anything else, including family. Which at least implies that it did for him.
</p><br>
<p>   
    [4] Interestingly, just as the word "nerd" implies earnestness even when used as a metaphor, the word "politics" implies
    the opposite. It's not only in actual politics that earnestness seems to be a handicap, but also in office politics and
    academic politics.
</p><br>
<p>   
    [5] It's a bigger social error to seem naive in most European countries than it is in America, and this may be one of
    subtler reasons startups are less common there. Founder culture is completely at odds with sophisticated cynicism.
</p><br>
<p>  
    The most earnest part of Europe is Scandinavia, and not surprisingly this is also the region with the highest number of
    successful startups per capita.
</p><br>
<p>   
    [6] Much of business is schleps, and probably always will be. But even being a professor is largely schleps. It would be
    interesting to collect statistics about the schlep ratios of different jobs, but I suspect they'd rarely be less than
    30%.
</p><br>
<p>      
    Thanks to Trevor Blackwell, Patrick Collison, Suhail Doshi, Jessica Livingston, Mattias Ljungman, Harj Taggar, and Kyle
    Vogt for reading drafts of this.
</p><br>
<br><br>
<span id="title">Billionaires Build</span>
                <br>
                <span id="secondText">December 2020</span>
                <br><br><br><br>   
<p>As I was deciding what to write about next, I was surprised to find that two separate essays I'd been planning to write
were actually the same.
</p><br>
<p>  
The first is about how to ace your Y Combinator interview. There has been so much nonsense written about this topic that
I've been meaning for years to write something telling founders the truth.
</p><br>
<p>  
The second is about something politicians sometimes say — that the only way to become a billionaire is by exploiting
people — and why this is mistaken.
</p><br>
<p>  
Keep reading, and you'll learn both simultaneously.
</p><br>
<p>  
I know the politicians are mistaken because it was my job to predict which people will become billionaires. I think I
can truthfully say that I know as much about how to do this as anyone. If the key to becoming a billionaire — the
defining feature of billionaires — was to exploit people, then I, as a professional billionaire scout, would surely
realize this and look for people who would be good at it, just as an NFL scout looks for speed in wide receivers.
</p><br>
<p>  
But aptitude for exploiting people is not what Y Combinator looks for at all. In fact, it's the opposite of what they
look for. I'll tell you what they do look for, by explaining how to convince Y Combinator to fund you, and you can see
for yourself.
</p><br>
<p>  
What YC looks for, above all, is founders who understand some group of users and can make what they want. This is so
important that it's YC's motto: "Make something people want."
</p><br>
<p>  
A big company can to some extent force unsuitable products on unwilling customers, but a startup doesn't have the power
to do that. A startup must sing for its supper, by making things that genuinely delight its customers. Otherwise it will
never get off the ground.
</p><br>
<p>  
Here's where things get difficult, both for you as a founder and for the YC partners trying to decide whether to fund
you. In a market economy, it's hard to make something people want that they don't already have. That's the great thing
about market economies. If other people both knew about this need and were able to satisfy it, they already would be,
and there would be no room for your startup.
</p><br>
<p>  
Which means the conversation during your YC interview will have to be about something new: either a new need, or a new
way to satisfy one. And not just new, but uncertain. If it were certain that the need existed and that you could satisfy
it, that certainty would be reflected in large and rapidly growing revenues, and you wouldn't be seeking seed funding.
</p><br>
<p>  
So the YC partners have to guess both whether you've discovered a real need, and whether you'll be able to satisfy it.
That's what they are, at least in this part of their job: professional guessers. They have 1001 heuristics for doing
this, and I'm not going to tell you them all, but I'm happy to tell you the most important ones, because these can't be
faked; the only way to "hack" them would be to do what you should be doing anyway as a founder.
</p><br>
<p>  
The first thing the partners will try to figure out, usually, is whether what you're making will ever be something a lot
of people want. It doesn't have to be something a lot of people want now. The product and the market will both evolve,
and will influence each other's evolution. But in the end there has to be something with a huge market. That's what the
partners will be trying to figure out: is there a path to a huge market? [1]
</p><br>
<p>  
Sometimes it's obvious there will be a huge market. If Boom manages to ship an airliner at all, international airlines
will have to buy it. But usually it's not obvious. Usually the path to a huge market is by growing a small market. This
idea is important enough that it's worth coining a phrase for, so let's call one of these small but growable markets a
"larval market."
</p><br>
<p>  
The perfect example of a larval market might be Apple's market when they were founded in 1976. In 1976, not many people
wanted their own computer. But more and more started to want one, till now every 10 year old on the planet wants a
computer (but calls it a "phone").
</p><br>
<p>  
The ideal combination is the group of founders who are "living in the future" in the sense of being at the leading edge
of some kind of change, and who are building something they themselves want. Most super-successful startups are of this
type. Steve Wozniak wanted a computer. Mark Zuckerberg wanted to engage online with his college friends. Larry and
Sergey wanted to find things on the web. All these founders were building things they and their peers wanted, and the
fact that they were at the leading edge of change meant that more people would want these things in the future.
</p><br>
<p>  
But although the ideal larval market is oneself and one's peers, that's not the only kind. A larval market might also be
regional, for example. You build something to serve one location, and then expand to others.
</p><br>
<p>  
The crucial feature of the initial market is that it exist. That may seem like an obvious point, but the lack of it is
the biggest flaw in most startup ideas. There have to be some people who want what you're building right now, and want
it so urgently that they're willing to use it, bugs and all, even though you're a small company they've never heard of.
There don't have to be many, but there have to be some. As long as you have some users, there are straightforward ways
to get more: build new features they want, seek out more people like them, get them to refer you to their friends, and
so on. But these techniques all require some initial seed group of users.
</p><br>
<p>  
So this is one thing the YC partners will almost certainly dig into during your interview. Who are your first users
going to be, and how do you know they want this? If I had to decide whether to fund startups based on a single question,
it would be "How do you know people want this?"
</p><br>
<p>  
The most convincing answer is "Because we and our friends want it." It's even better when this is followed by the news
that you've already built a prototype, and even though it's very crude, your friends are using it, and it's spreading by
word of mouth. If you can say that and you're not lying, the partners will switch from default no to default yes.
Meaning you're in unless there's some other disqualifying flaw.
</p><br>
<p>  
That is a hard standard to meet, though. Airbnb didn't meet it. They had the first part. They had made something they
themselves wanted. But it wasn't spreading. So don't feel bad if you don't hit this gold standard of convincingness. If
Airbnb didn't hit it, it must be too high.
</p><br>
<p>  
In practice, the YC partners will be satisfied if they feel that you have a deep understanding of your users' needs. And
the Airbnbs did have that. They were able to tell us all about what motivated hosts and guests. They knew from
first-hand experience, because they'd been the first hosts. We couldn't ask them a question they didn't know the answer
to. We ourselves were not very excited about the idea as users, but we knew this didn't prove anything, because there
were lots of successful startups we hadn't been excited about as users. We were able to say to ourselves "They seem to
know what they're talking about. Maybe they're onto something. It's not growing yet, but maybe they can figure out how
to make it grow during YC." Which they did, about three weeks into the batch.
</p><br>
<p>  
The best thing you can do in a YC interview is to teach the partners about your users. So if you want to prepare for
your interview, one of the best ways to do it is to go talk to your users and find out exactly what they're thinking.
Which is what you should be doing anyway.
</p><br>
<p>  
This may sound strangely credulous, but the YC partners want to rely on the founders to tell them about the market.
Think about how VCs typically judge the potential market for an idea. They're not ordinarily domain experts themselves,
so they forward the idea to someone who is, and ask for their opinion. YC doesn't have time to do this, but if the YC
partners can convince themselves that the founders both (a) know what they're talking about and (b) aren't lying, they
don't need outside domain experts. They can use the founders themselves as domain experts when evaluating their own
idea.
</p><br>
<p>  
This is why YC interviews aren't pitches. To give as many founders as possible a chance to get funded, we made
interviews as short as we could: 10 minutes. That is not enough time for the partners to figure out, through the
indirect evidence in a pitch, whether you know what you're talking about and aren't lying. They need to dig in and ask
you questions. There's not enough time for sequential access. They need random access. [2]
</p><br>
<p>  
The worst advice I ever heard about how to succeed in a YC interview is that you should take control of the interview
and make sure to deliver the message you want to. In other words, turn the interview into a pitch. ⟨elaborate
expletive⟩. It is so annoying when people try to do that. You ask them a question, and instead of answering it, they
deliver some obviously prefabricated blob of pitch. It eats up 10 minutes really fast.
</p><br>
<p>  
There is no one who can give you accurate advice about what to do in a YC interview except a current or former YC
partner. People who've merely been interviewed, even successfully, have no idea of this, but interviews take all sorts
of different forms depending on what the partners want to know about most. Sometimes they're all about the founders,
other times they're all about the idea. Sometimes some very narrow aspect of the idea. Founders sometimes walk away from
interviews complaining that they didn't get to explain their idea completely. True, but they explained enough.
</p><br>
<p>  
Since a YC interview consists of questions, the way to do it well is to answer them well. Part of that is answering them
candidly. The partners don't expect you to know everything. But if you don't know the answer to a question, don't try to
bullshit your way out of it. The partners, like most experienced investors, are professional bullshit detectors, and you
are (hopefully) an amateur bullshitter. And if you try to bullshit them and fail, they may not even tell you that you
failed. So it's better to be honest than to try to sell them. If you don't know the answer to a question, say you don't,
and tell them how you'd go about finding it, or tell them the answer to some related question.
</p><br>
<p>  
If you're asked, for example, what could go wrong, the worst possible answer is "nothing." Instead of convincing them
that your idea is bullet-proof, this will convince them that you're a fool or a liar. Far better to go into gruesome
detail. That's what experts do when you ask what could go wrong. The partners know that your idea is risky. That's what
a good bet looks like at this stage: a tiny probability of a huge outcome.
</p><br>
<p>  
Ditto if they ask about competitors. Competitors are rarely what kills startups. Poor execution does. But you should
know who your competitors are, and tell the YC partners candidly what your relative strengths and weaknesses are.
Because the YC partners know that competitors don't kill startups, they won't hold competitors against you too much.
They will, however, hold it against you if you seem either to be unaware of competitors, or to be minimizing the threat
they pose. They may not be sure whether you're clueless or lying, but they don't need to be.
</p><br>
<p>  
The partners don't expect your idea to be perfect. This is seed investing. At this stage, all they can expect are
promising hypotheses. But they do expect you to be thoughtful and honest. So if trying to make your idea seem perfect
causes you to come off as glib or clueless, you've sacrificed something you needed for something you didn't.
</p><br>
<p>  
If the partners are sufficiently convinced that there's a path to a big market, the next question is whether you'll be
able to find it. That in turn depends on three things: the general qualities of the founders, their specific expertise
in this domain, and the relationship between them. How determined are the founders? Are they good at building things?
Are they resilient enough to keep going when things go wrong? How strong is their friendship?
</p><br>
<p>  
Though the Airbnbs only did ok in the idea department, they did spectacularly well in this department. The story of how
they'd funded themselves by making Obama- and McCain-themed breakfast cereal was the single most important factor in our
decision to fund them. They didn't realize it at the time, but what seemed to them an irrelevant story was in fact
fabulously good evidence of their qualities as founders. It showed they were resourceful and determined, and could work
together.
</p><br>
<p>  
It wasn't just the cereal story that showed that, though. The whole interview showed that they cared. They weren't doing
this just for the money, or because startups were cool. The reason they were working so hard on this company was because
it was their project. They had discovered an interesting new idea, and they just couldn't let it go.
</p><br>
<p>  
Mundane as it sounds, that's the most powerful motivator of all, not just in startups, but in most ambitious
undertakings: to be genuinely interested in what you're building. This is what really drives billionaires, or at least
the ones who become billionaires from starting companies. The company is their project.
</p><br>
<p>  
One thing few people realize about billionaires is that all of them could have stopped sooner. They could have gotten
acquired, or found someone else to run the company. Many founders do. The ones who become really rich are the ones who
keep working. And what makes them keep working is not just money. What keeps them working is the same thing that keeps
anyone else working when they could stop if they wanted to: that there's nothing else they'd rather do.
</p><br>
<p>  
That, not exploiting people, is the defining quality of people who become billionaires from starting companies. So
that's what YC looks for in founders: authenticity. People's motives for starting startups are usually mixed. They're
usually doing it from some combination of the desire to make money, the desire to seem cool, genuine interest in the
problem, and unwillingness to work for someone else. The last two are more powerful motivators than the first two. It's
ok for founders to want to make money or to seem cool. Most do. But if the founders seem like they're doing it just to
make money or just to seem cool, they're not likely to succeed on a big scale. The founders who are doing it for the
money will take the first sufficiently large acquisition offer, and the ones who are doing it to seem cool will rapidly
discover that there are much less painful ways of seeming cool. [3]
</p><br>
<p>  
Y Combinator certainly sees founders whose m.o. is to exploit people. YC is a magnet for them, because they want the YC
brand. But when the YC partners detect someone like that, they reject them. If bad people made good founders, the YC
partners would face a moral dilemma. Fortunately they don't, because bad people make bad founders. This exploitative
type of founder is not going to succeed on a large scale, and in fact probably won't even succeed on a small one,
because they're always going to be taking shortcuts. They see YC itself as a shortcut.
</p><br>
<p>  
Their exploitation usually begins with their own cofounders, which is disastrous, since the cofounders' relationship is
the foundation of the company. Then it moves on to the users, which is also disastrous, because the sort of early
adopters a successful startup wants as its initial users are the hardest to fool. The best this kind of founder can hope
for is to keep the edifice of deception tottering along until some acquirer can be tricked into buying it. But that kind
of acquisition is never very big. [4]
</p><br>
<p>  
If professional billionaire scouts know that exploiting people is not the skill to look for, why do some politicians
think this is the defining quality of billionaires?
</p><br>
<p>  
I think they start from the feeling that it's wrong that one person could have so much more money than another. It's
understandable where that feeling comes from. It's in our DNA, and even in the DNA of other species.
</p><br>
<p>  
If they limited themselves to saying that it made them feel bad when one person had so much more money than other
people, who would disagree? It makes me feel bad too, and I think people who make a lot of money have a moral obligation
to use it for the common good. The mistake they make is to jump from feeling bad that some people are much richer than
others to the conclusion that there's no legitimate way to make a very large amount of money. Now we're getting into
statements that are not only falsifiable, but false.
</p><br>
<p>  
There are certainly some people who become rich by doing bad things. But there are also plenty of people who behave
badly and don't make that much from it. There is no correlation — in fact, probably an inverse correlation — between how
badly you behave and how much money you make.
</p><br>
<p>  
The greatest danger of this nonsense may not even be that it sends policy astray, but that it misleads ambitious people.
Can you imagine a better way to destroy social mobility than by telling poor kids that the way to get rich is by
exploiting people, while the rich kids know, from having watched the preceding generation do it, how it's really done?
</p><br>
<p>  
I'll tell you how it's really done, so you can at least tell your own kids the truth. It's all about users. The most
reliable way to become a billionaire is to start a company that grows fast, and the way to grow fast is to make what
users want. Newly started startups have no choice but to delight users, or they'll never even get rolling. But this
never stops being the lodestar, and bigger companies take their eye off it at their peril. Stop delighting users, and
eventually someone else will.
</p><br>
<p>  
Users are what the partners want to know about in YC interviews, and what I want to know about when I talk to founders
that we funded ten years ago and who are billionaires now. What do users want? What new things could you build for them?
Founders who've become billionaires are always eager to talk about that topic. That's how they became billionaires.
</p><br>
<p>  
Notes
</p><br>
<p>  
[1] The YC partners have so much practice doing this that they sometimes see paths that the founders themselves haven't
seen yet. The partners don't try to seem skeptical, as buyers in transactions often do to increase their leverage.
Although the founders feel their job is to convince the partners of the potential of their idea, these roles are not
infrequently reversed, and the founders leave the interview feeling their idea has more potential than they realized.
</p><br>
<p>  
[2] In practice, 7 minutes would be enough. You rarely change your mind at minute 8. But 10 minutes is socially
convenient.
</p><br>
<p>  
[3] I myself took the first sufficiently large acquisition offer in my first startup, so I don't blame founders for
doing this. There's nothing wrong with starting a startup to make money. You need to make money somehow, and for some
people startups are the most efficient way to do it. I'm just saying that these are not the startups that get really
big.
</p><br>
<p>  
[4] Not these days, anyway. There were some big ones during the Internet Bubble, and indeed some big IPOs.
</p><br>
<p>  
Thanks to Trevor Blackwell, Jessica Livingston, Robert Morris, Geoff Ralston, and Harj Taggar for reading drafts of
this.
</p><br>
<br><br>
<span id="title">The Airbnbs</span>
                <br>
                <span id="secondText">December 2020</span>
                <br><br><br><br>   
<p>
    To celebrate Airbnb's IPO and to help future founders, I thought it might be useful to explain what was special about
    Airbnb.
  </p><br>
<p>   
    What was special about the Airbnbs was how earnest they were. They did nothing half-way, and we could sense this even in
    the interview. Sometimes after we interviewed a startup we'd be uncertain what to do, and have to talk it over. Other
    times we'd just look at one another and smile. The Airbnbs' interview was that kind. We didn't even like the idea that
    much. Nor did users, at that stage; they had no growth. But the founders seemed so full of energy that it was impossible
    not to like them.
</p><br>
<p>  
    That first impression was not misleading. During the batch our nickname for Brian Chesky was The Tasmanian Devil,
    because like the cartoon character he seemed a tornado of energy. All three of them were like that. No one ever worked
    harder during YC than the Airbnbs did. When you talked to the Airbnbs, they took notes. If you suggested an idea to them
    in office hours, the next time you talked to them they'd not only have implemented it, but also implemented two new
    ideas they had in the process. "They probably have the best attitude of any startup we've funded" I wrote to Mike
    Arrington during the batch.
</p><br>
<p>  
    They're still like that. Jessica and I had dinner with Brian in the summer of 2018, just the three of us. By this point
    the company is ten years old. He took a page of notes about ideas for new things Airbnb could do.
</p><br>
<p>  
    What we didn't realize when we first met Brian and Joe and Nate was that Airbnb was on its last legs. After working on
    the company for a year and getting no growth, they'd agreed to give it one last shot. They'd try this Y Combinator
    thing, and if the company still didn't take off, they'd give up.
</p><br>
<p>  
    Any normal person would have given up already. They'd been funding the company with credit cards. They had a binder full
    of credit cards they'd maxed out. Investors didn't think much of the idea. One investor they met in a cafe walked out in
    the middle of meeting with them. They thought he was going to the bathroom, but he never came back. "He didn't even
    finish his smoothie," Brian said. And now, in late 2008, it was the worst recession in decades. The stock market was in
    free fall and wouldn't hit bottom for another four months.
</p><br>
<p>   
    Why hadn't they given up? This is a useful question to ask. People, like matter, reveal their nature under extreme
    conditions. One thing that's clear is that they weren't doing this just for the money. As a money-making scheme, this
    was pretty lousy: a year's work and all they had to show for it was a binder full of maxed-out credit cards. So why were
    they still working on this startup? Because of the experience they'd had as the first hosts.
</p><br>
<p>  
    When they first tried renting out airbeds on their floor during a design convention, all they were hoping for was to
    make enough money to pay their rent that month. But something surprising happened: they enjoyed having those first three
    guests staying with them. And the guests enjoyed it too. Both they and the guests had done it because they were in a
    sense forced to, and yet they'd all had a great experience. Clearly there was something new here: for hosts, a new way
    to make money that had literally been right under their noses, and for guests, a new way to travel that was in many ways
    better than hotels.
</p><br>
<p>  
    That experience was why the Airbnbs didn't give up. They knew they'd discovered something. They'd seen a glimpse of the
    future, and they couldn't let it go.
</p><br>
<p>   
    They knew that once people tried staying in what is now called "an airbnb," they would also realize that this was the
    future. But only if they tried it, and they weren't. That was the problem during Y Combinator: to get growth started.
</p><br>
<p>   
    Airbnb's goal during YC was to reach what we call ramen profitability, which means making enough money that the company
    can pay the founders' living expenses, if they live on ramen noodles. Ramen profitability is not, obviously, the end
    goal of any startup, but it's the most important threshold on the way, because this is the point where you're airborne.
    This is the point where you no longer need investors' permission to continue existing. For the Airbnbs, ramen
    profitability was $4000 a month: $3500 for rent, and $500 for food. They taped this goal to the mirror in the bathroom
    of their apartment.
</p><br>
<p>   
    The way to get growth started in something like Airbnb is to focus on the hottest subset of the market. If you can get
    growth started there, it will spread to the rest. When I asked the Airbnbs where there was most demand, they knew from
    searches: New York City. So they focused on New York. They went there in person to visit their hosts and help them make
    their listings more attractive. A big part of that was better pictures. So Joe and Brian rented a professional camera
    and took pictures of the hosts' places themselves.
</p><br>
<p>   
    This didn't just make the listings better. It also taught them about their hosts. When they came back from their first
    trip to New York, I asked what they'd noticed about hosts that surprised them, and they said the biggest surprise was
    how many of the hosts were in the same position they'd been in: they needed this money to pay their rent. This was,
    remember, the worst recession in decades, and it had hit New York first. It definitely added to the Airbnbs' sense of
    mission to feel that people needed them.
</p><br>
<p>   
    In late January 2009, about three weeks into Y Combinator, their efforts started to show results, and their numbers
    crept upward. But it was hard to say for sure whether it was growth or just random fluctuation. By February it was clear
    that it was real growth. They made $460 in fees in the first week of February, $897 in the second, and $1428 in the
    third. That was it: they were airborne. Brian sent me an email on February 22 announcing that they were ramen profitable
    and giving the last three weeks' numbers.
</p><br>
<p>   
    "I assume you know what you've now set yourself up for next week," I responded.
</p><br>
<p>   
    Brian's reply was seven words: "We are not going to slow down."
</p><br>
<br><br>
<span id="title">How To Think For Yourself</span>
                <br>
                <span id="secondText">November 2020</span>
                <br><br><br><br>   
<p> There are some kinds of work that you can't do well without thinking differently from your peers. To be a successful
    scientist, for example, it's not enough just to be correct. Your ideas have to be both correct and novel. You can't
    publish papers saying things other people already know. You need to say things no one else has realized yet.
</p><br>
<p>     
    The same is true for investors. It's not enough for a public market investor to predict correctly how a company will do.
    If a lot of other people make the same prediction, the stock price will already reflect it, and there's no room to make
    money. The only valuable insights are the ones most other investors don't share.
</p><br>
<p>     
    You see this pattern with startup founders too. You don't want to start a startup to do something that everyone agrees
    is a good idea, or there will already be other companies doing it. You have to do something that sounds to most other
    people like a bad idea, but that you know isn't — like writing software for a tiny computer used by a few thousand
    hobbyists, or starting a site to let people rent airbeds on strangers' floors.
</p><br>
<p>     
    Ditto for essayists. An essay that told people things they already knew would be boring. You have to tell them something
    new.
</p><br>
<p>     
    But this pattern isn't universal. In fact, it doesn't hold for most kinds of work. In most kinds of work — to be an
    administrator, for example — all you need is the first half. All you need is to be right. It's not essential that
    everyone else be wrong.
</p><br>
<p>     
    There's room for a little novelty in most kinds of work, but in practice there's a fairly sharp distinction between the
    kinds of work where it's essential to be independent-minded, and the kinds where it's not.
</p><br>
<p>     
    I wish someone had told me about this distinction when I was a kid, because it's one of the most important things to
    think about when you're deciding what kind of work you want to do. Do you want to do the kind of work where you can only
    win by thinking differently from everyone else? I suspect most people's unconscious mind will answer that question
    before their conscious mind has a chance to. I know mine does.
</p><br>
<p>     
    Independent-mindedness seems to be more a matter of nature than nurture. Which means if you pick the wrong type of work,
    you're going to be unhappy. If you're naturally independent-minded, you're going to find it frustrating to be a middle
    manager. And if you're naturally conventional-minded, you're going to be sailing into a headwind if you try to do
    original research.
</p><br>
<p>     
    One difficulty here, though, is that people are often mistaken about where they fall on the spectrum from conventional-
    to independent-minded. Conventional-minded people don't like to think of themselves as conventional-minded. And in any
    case, it genuinely feels to them as if they make up their own minds about everything. It's just a coincidence that their
    beliefs are identical to their peers'. And the independent-minded, meanwhile, are often unaware how different their
    ideas are from conventional ones, at least till they state them publicly. [1]
</p><br>
<p>     
    By the time they reach adulthood, most people know roughly how smart they are (in the narrow sense of ability to solve
    pre-set problems), because they're constantly being tested and ranked according to it. But schools generally ignore
    independent-mindedness, except to the extent they try to suppress it. So we don't get anything like the same kind of
    feedback about how independent-minded we are.
</p><br>
<p>     
    There may even be a phenomenon like Dunning-Kruger at work, where the most conventional-minded people are confident that
    they're independent-minded, while the genuinely independent-minded worry they might not be independent-minded enough.
</p><br>
<p>    
    ___________
</p><br>
<p>    
    
    Can you make yourself more independent-minded? I think so. This quality may be largely inborn, but there seem to be ways
    to magnify it, or at least not to suppress it.
</p><br>
<p>    
    One of the most effective techniques is one practiced unintentionally by most nerds: simply to be less aware what
    conventional beliefs are. It's hard to be a conformist if you don't know what you're supposed to conform to. Though
    again, it may be that such people already are independent-minded. A conventional-minded person would probably feel
    anxious not knowing what other people thought, and make more effort to find out.
</p><br>
<p>    
    It matters a lot who you surround yourself with. If you're surrounded by conventional-minded people, it will constrain
    which ideas you can express, and that in turn will constrain which ideas you have. But if you surround yourself with
    independent-minded people, you'll have the opposite experience: hearing other people say surprising things will
    encourage you to, and to think of more.
</p><br>
<p>    
    Because the independent-minded find it uncomfortable to be surrounded by conventional-minded people, they tend to
    self-segregate once they have a chance to. The problem with high school is that they haven't yet had a chance to. Plus
    high school tends to be an inward-looking little world whose inhabitants lack confidence, both of which magnify the
    forces of conformism. And so high school is often a bad time for the independent-minded. But there is some advantage
    even here: it teaches you what to avoid. If you later find yourself in a situation that makes you think "this is like
    high school," you know you should get out. [2]
</p><br>
<p>    
    Another place where the independent- and conventional-minded are thrown together is in successful startups. The founders
    and early employees are almost always independent-minded; otherwise the startup wouldn't be successful. But
    conventional-minded people greatly outnumber independent-minded ones, so as the company grows, the original spirit of
    independent-mindedness is inevitably diluted. This causes all kinds of problems besides the obvious one that the company
    starts to suck. One of the strangest is that the founders find themselves able to speak more freely with founders of
    other companies than with their own employees. [3]
</p><br>
<p>    
    Fortunately you don't have to spend all your time with independent-minded people. It's enough to have one or two you can
    talk to regularly. And once you find them, they're usually as eager to talk as you are; they need you too. Although
    universities no longer have the kind of monopoly they used to have on education, good universities are still an
    excellent way to meet independent-minded people. Most students will still be conventional-minded, but you'll at least
    find clumps of independent-minded ones, rather than the near zero you may have found in high school.
</p><br>
<p>    
    It also works to go in the other direction: as well as cultivating a small collection of independent-minded friends, to
    try to meet as many different types of people as you can. It will decrease the influence of your immediate peers if you
    have several other groups of peers. Plus if you're part of several different worlds, you can often import ideas from one
    to another.
</p><br>
<p>    
    But by different types of people, I don't mean demographically different. For this technique to work, they have to think
    differently. So while it's an excellent idea to go and visit other countries, you can probably find people who think
    differently right around the corner. When I meet someone who knows a lot about something unusual (which includes
    practically everyone, if you dig deep enough), I try to learn what they know that other people don't. There are almost
    always surprises here. It's a good way to make conversation when you meet strangers, but I don't do it to make
    conversation. I really want to know.
</p><br>
<p>    
    You can expand the source of influences in time as well as space, by reading history. When I read history I do it not
    just to learn what happened, but to try to get inside the heads of people who lived in the past. How did things look to
    them? This is hard to do, but worth the effort for the same reason it's worth travelling far to triangulate a point.
</p><br>
<p>    
    You can also take more explicit measures to prevent yourself from automatically adopting conventional opinions. The most
    general is to cultivate an attitude of skepticism. When you hear someone say something, stop and ask yourself "Is that
    true?" Don't say it out loud. I'm not suggesting that you impose on everyone who talks to you the burden of proving what
    they say, but rather that you take upon yourself the burden of evaluating what they say.
</p><br>
<p>    
    Treat it as a puzzle. You know that some accepted ideas will later turn out to be wrong. See if you can guess which. The
    end goal is not to find flaws in the things you're told, but to find the new ideas that had been concealed by the broken
    ones. So this game should be an exciting quest for novelty, not a boring protocol for intellectual hygiene. And you'll
    be surprised, when you start asking "Is this true?", how often the answer is not an immediate yes. If you have any
    imagination, you're more likely to have too many leads to follow than too few.
</p><br>
<p>    
    More generally your goal should be not to let anything into your head unexamined, and things don't always enter your
    head in the form of statements. Some of the most powerful influences are implicit. How do you even notice these? By
    standing back and watching how other people get their ideas.
</p><br>
<p>    
    When you stand back at a sufficient distance, you can see ideas spreading through groups of people like waves. The most
    obvious are in fashion: you notice a few people wearing a certain kind of shirt, and then more and more, until half the
    people around you are wearing the same shirt. You may not care much what you wear, but there are intellectual fashions
    too, and you definitely don't want to participate in those. Not just because you want sovereignty over your own
    thoughts, but because unfashionable ideas are disproportionately likely to lead somewhere interesting. The best place to
    find undiscovered ideas is where no one else is looking. [4]
</p><br>
<p>    
    ___________
    
</p><br>
<p>    
    To go beyond this general advice, we need to look at the internal structure of independent-mindedness — at the
    individual muscles we need to exercise, as it were. It seems to me that it has three components: fastidiousness about
    truth, resistance to being told what to think, and curiosity.
</p><br>
<p>    
    Fastidiousness about truth means more than just not believing things that are false. It means being careful about degree
    of belief. For most people, degree of belief rushes unexamined toward the extremes: the unlikely becomes impossible, and
    the probable becomes certain. [5] To the independent-minded, this seems unpardonably sloppy. They're willing to have
    anything in their heads, from highly speculative hypotheses to (apparent) tautologies, but on subjects they care about,
    everything has to be labelled with a carefully considered degree of belief. [6]
</p><br>
<p>    
    The independent-minded thus have a horror of ideologies, which require one to accept a whole collection of beliefs at
    once, and to treat them as articles of faith. To an independent-minded person that would seem revolting, just as it
    would seem to someone fastidious about food to take a bite of a submarine sandwich filled with a large variety of
    ingredients of indeterminate age and provenance.
</p><br>
<p>    
    Without this fastidiousness about truth, you can't be truly independent-minded. It's not enough just to have resistance
    to being told what to think. Those kind of people reject conventional ideas only to replace them with the most random
    conspiracy theories. And since these conspiracy theories have often been manufactured to capture them, they end up being
    less independent-minded than ordinary people, because they're subject to a much more exacting master than mere
    convention. [7]
</p><br>
<p>    
    Can you increase your fastidiousness about truth? I would think so. In my experience, merely thinking about something
    you're fastidious about causes that fastidiousness to grow. If so, this is one of those rare virtues we can have more of
    merely by wanting it. And if it's like other forms of fastidiousness, it should also be possible to encourage in
    children. I certainly got a strong dose of it from my father. [8]
</p><br>
<p>    
    The second component of independent-mindedness, resistance to being told what to think, is the most visible of the
    three. But even this is often misunderstood. The big mistake people make about it is to think of it as a merely negative
    quality. The language we use reinforces that idea. You're unconventional. You don't care what other people think. But
    it's not just a kind of immunity. In the most independent-minded people, the desire not to be told what to think is a
    positive force. It's not mere skepticism, but an active delight in ideas that subvert the conventional wisdom, the more
    counterintuitive the better.
</p><br>
<p>    
    Some of the most novel ideas seemed at the time almost like practical jokes. Think how often your reaction to a novel
    idea is to laugh. I don't think it's because novel ideas are funny per se, but because novelty and humor share a certain
    kind of surprisingness. But while not identical, the two are close enough that there is a definite correlation between
    having a sense of humor and being independent-minded — just as there is between being humorless and being
    conventional-minded. [9]
</p><br>
<p>     
    I don't think we can significantly increase our resistance to being told what to think. It seems the most innate of the
    three components of independent-mindedness; people who have this quality as adults usually showed all too visible signs
    of it as children. But if we can't increase our resistance to being told what to think, we can at least shore it up, by
    surrounding ourselves with other independent-minded people.
</p><br>
<p>    
    The third component of independent-mindedness, curiosity, may be the most interesting. To the extent that we can give a
    brief answer to the question of where novel ideas come from, it's curiosity. That's what people are usually feeling
    before having them.
</p><br>
<p>    
    In my experience, independent-mindedness and curiosity predict one another perfectly. Everyone I know who's
    independent-minded is deeply curious, and everyone I know who's conventional-minded isn't. Except, curiously, children.
    All small children are curious. Perhaps the reason is that even the conventional-minded have to be curious in the
    beginning, in order to learn what the conventions are. Whereas the independent-minded are the gluttons of curiosity, who
    keep eating even after they're full. [10]
</p><br>
<p>    
    The three components of independent-mindedness work in concert: fastidiousness about truth and resistance to being told
    what to think leave space in your brain, and curiosity finds new ideas to fill it.
</p><br>
<p>     
    Interestingly, the three components can substitute for one another in much the same way muscles can. If you're
    sufficiently fastidious about truth, you don't need to be as resistant to being told what to think, because
    fastidiousness alone will create sufficient gaps in your knowledge. And either one can compensate for curiosity, because
    if you create enough space in your brain, your discomfort at the resulting vacuum will add force to your curiosity. Or
    curiosity can compensate for them: if you're sufficiently curious, you don't need to clear space in your brain, because
    the new ideas you discover will push out the conventional ones you acquired by default.
</p><br>
<p>    
    Because the components of independent-mindedness are so interchangeable, you can have them to varying degrees and still
    get the same result. So there is not just a single model of independent-mindedness. Some independent-minded people are
    openly subversive, and others are quietly curious. They all know the secret handshake though.
</p><br>
<p>    
    Is there a way to cultivate curiosity? To start with, you want to avoid situations that suppress it. How much does the
    work you're currently doing engage your curiosity? If the answer is "not much," maybe you should change something.
</p><br>
<p>    
    The most important active step you can take to cultivate your curiosity is probably to seek out the topics that engage
    it. Few adults are equally curious about everything, and it doesn't seem as if you can choose which topics interest you.
    So it's up to you to find them. Or invent them, if necessary.
</p><br>
<p>     
    Another way to increase your curiosity is to indulge it, by investigating things you're interested in. Curiosity is
    unlike most other appetites in this respect: indulging it tends to increase rather than to sate it. Questions lead to
    more questions.
</p><br>
<p>    
    Curiosity seems to be more individual than fastidiousness about truth or resistance to being told what to think. To the
    degree people have the latter two, they're usually pretty general, whereas different people can be curious about very
    different things. So perhaps curiosity is the compass here. Perhaps, if your goal is to discover novel ideas, your motto
    should not be "do what you love" so much as "do what you're curious about."
</p><br>
<p>    
     Notes
    </p><br>
    <p>
     [1] One convenient consequence of the fact that no one identifies as conventional-minded is that you can say what you
    like about conventional-minded people without getting in too much trouble. When I wrote "The Four Quadrants of
    Conformism" I expected a firestorm of rage from the aggressively conventional-minded, but in fact it was quite muted.
    They sensed that there was something about the essay that they disliked intensely, but they had a hard time finding a
    specific passage to pin it on.
</p><br>
<p>   
    [2] When I ask myself what in my life is like high school, the answer is Twitter. It's not just full of
    conventional-minded people, as anything its size will inevitably be, but subject to violent storms of
    conventional-mindedness that remind me of descriptions of Jupiter. But while it probably is a net loss to spend time
    there, it has at least made me think more about the distinction between independent- and conventional-mindedness, which
    I probably wouldn't have done otherwise.
</p><br>
<p>   
    [3] The decrease in independent-mindedness in growing startups is still an open problem, but there may be solutions.
</p><br>
<p>   
    Founders can delay the problem by making a conscious effort only to hire independent-minded people. Which of course also
    has the ancillary benefit that they have better ideas.
</p><br>
<p>   
    Another possible solution is to create policies that somehow disrupt the force of conformism, much as control rods slow
    chain reactions, so that the conventional-minded aren't as dangerous. The physical separation of Lockheed's Skunk Works
    may have had this as a side benefit. Recent examples suggest employee forums like Slack may not be an unmitigated good.
</p><br>
<p>   
    The most radical solution would be to grow revenues without growing the company. You think hiring that junior PR person
    will be cheap, compared to a programmer, but what will be the effect on the average level of independent-mindedness in
    your company? (The growth in staff relative to faculty seems to have had a similar effect on universities.) Perhaps the
    rule about outsourcing work that's not your "core competency" should be augmented by one about outsourcing work done by
    people who'd ruin your culture as employees.
</p><br>
<p>   
    Some investment firms already seem to be able to grow revenues without growing the number of employees. Automation plus
    the ever increasing articulation of the "tech stack" suggest this may one day be possible for product companies.
</p><br>
<p>   
    [4] There are intellectual fashions in every field, but their influence varies. One of the reasons politics, for
    example, tends to be boring is that it's so extremely subject to them. The threshold for having opinions about politics
    is much lower than the one for having opinions about set theory. So while there are some ideas in politics, in practice
    they tend to be swamped by waves of intellectual fashion.
</p><br>
<p>   
    [5] The conventional-minded are often fooled by the strength of their opinions into believing that they're
    independent-minded. But strong convictions are not a sign of independent-mindedness. Rather the opposite.
</p><br>
<p>   
    [6] Fastidiousness about truth doesn't imply that an independent-minded person won't be dishonest, but that he won't be
    deluded. It's sort of like the definition of a gentleman as someone who is never unintentionally rude.
</p><br>
<p>   
    [7] You see this especially among political extremists. They think themselves nonconformists, but actually they're niche
    conformists. Their opinions may be different from the average person's, but they are often more influenced by their
    peers' opinions than the average person's are.
</p><br>
<p>   
    [8] If we broaden the concept of fastidiousness about truth so that it excludes pandering, bogusness, and pomposity as
    well as falsehood in the strict sense, our model of independent-mindedness can expand further into the arts.
</p><br>
<p>   
    [9] This correlation is far from perfect, though. Gödel and Dirac don't seem to have been very strong in the humor
    department. But someone who is both "neurotypical" and humorless is very likely to be conventional-minded.
</p><br>
<p>   
    [10] Exception: gossip. Almost everyone is curious about gossip.   
</p><br>
<p> Thanks to Trevor Blackwell, Paul Buchheit, Patrick Collison, Jessica Livingston, Robert Morris, Harj Taggar, and Peter
    Thiel for reading drafts of this.
</p><br>
<br><br>
<span id="title">Early Work</span>
                <br>
                <span id="secondText">October 2020</span>
                <br><br><br><br>   
<p>
    One of the biggest things holding people back from doing great work is the fear of making something lame. And this fear
    is not an irrational one. Many great projects go through a stage early on where they don't seem very impressive, even to
    their creators. You have to push through this stage to reach the great work that lies beyond. But many people don't.
    Most people don't even reach the stage of making something they're embarrassed by, let alone continue past it. They're
    too frightened even to start.
</p><br>
<p>    
    Imagine if we could turn off the fear of making something lame. Imagine how much more we'd do.
</p><br>
<p>    
    Is there any hope of turning it off? I think so. I think the habits at work here are not very deeply rooted.
</p><br>
<p>   
    Making new things is itself a new thing for us as a species. It has always happened, but till the last few centuries it
    happened so slowly as to be invisible to individual humans. And since we didn't need customs for dealing with new ideas,
    we didn't develop any.
</p><br>
<p>   
    We just don't have enough experience with early versions of ambitious projects to know how to respond to them. We judge
    them as we would judge more finished work, or less ambitious projects. We don't realize they're a special case.
</p><br>
<p>   
    Or at least, most of us don't. One reason I'm confident we can do better is that it's already starting to happen. There
    are already a few places that are living in the future in this respect. Silicon Valley is one of them: an unknown person
    working on a strange-sounding idea won't automatically be dismissed the way they would back home. In Silicon Valley,
    people have learned how dangerous that is.
</p><br>
<p>   
    The right way to deal with new ideas is to treat them as a challenge to your imagination — not just to have lower
    standards, but to switch polarity entirely, from listing the reasons an idea won't work to trying to think of ways it
    could. That's what I do when I meet people with new ideas. I've become quite good at it, but I've had a lot of practice.
    Being a partner at Y Combinator means being practically immersed in strange-sounding ideas proposed by unknown people.
    Every six months you get thousands of new ones thrown at you and have to sort through them, knowing that in a world with
    a power-law distribution of outcomes, it will be painfully obvious if you miss the needle in this haystack. Optimism
    becomes urgent.
</p><br>
<p>   
    But I'm hopeful that, with time, this kind of optimism can become widespread enough that it becomes a social custom, not
    just a trick used by a few specialists. It is after all an extremely lucrative trick, and those tend to spread quickly.
</p><br>
<p>   
    Of course, inexperience is not the only reason people are too harsh on early versions of ambitious projects. They also
    do it to seem clever. And in a field where the new ideas are risky, like startups, those who dismiss them are in fact
    more likely to be right. Just not when their predictions are weighted by outcome.
</p><br>
<p>    
    But there is another more sinister reason people dismiss new ideas. If you try something ambitious, many of those around
    you will hope, consciously or unconsciously, that you'll fail. They worry that if you try something ambitious and
    succeed, it will put you above them. In some countries this is not just an individual failing but part of the national
    culture.
</p><br>
<p>   
    I wouldn't claim that people in Silicon Valley overcome these impulses because they're morally better. [1] The reason
    many hope you'll succeed is that they hope to rise with you. For investors this incentive is particularly explicit. They
    want you to succeed because they hope you'll make them rich in the process. But many other people you meet can hope to
    benefit in some way from your success. At the very least they'll be able to say, when you're famous, that they've known
    you since way back.
</p><br>
<p>   
    But even if Silicon Valley's encouraging attitude is rooted in self-interest, it has over time actually grown into a
    sort of benevolence. Encouraging startups has been practiced for so long that it has become a custom. Now it just seems
    that that's what one does with startups.
</p><br>
<p>   
    Maybe Silicon Valley is too optimistic. Maybe it's too easily fooled by impostors. Many less optimistic journalists want
    to believe that. But the lists of impostors they cite are suspiciously short, and plagued with asterisks. [2] If you use
    revenue as the test, Silicon Valley's optimism seems better tuned than the rest of the world's. And because it works, it
    will spread.
</p><br>
<p>   
    There's a lot more to new ideas than new startup ideas, of course. The fear of making something lame holds people back
    in every field. But Silicon Valley shows how quickly customs can evolve to support new ideas. And that in turn proves
    that dismissing new ideas is not so deeply rooted in human nature that it can't be unlearnt.
</p><br>
<p>   
    ___________
    
</p><br>
<p>    
    Unfortunately, if you want to do new things, you'll face a force more powerful than other people's skepticism: your own
    skepticism. You too will judge your early work too harshly. How do you avoid that?
</p><br>
<p>    
    This is a difficult problem, because you don't want to completely eliminate your horror of making something lame. That's
    what steers you toward doing good work. You just want to turn it off temporarily, the way a painkiller temporarily turns
    off pain.
</p><br>
<p>    
    People have already discovered several techniques that work. Hardy mentions two in A Mathematician's Apology:
    Good work is not done by "humble" men. It is one of the first duties of a professor, for example, in any subject, to
    exaggerate a little both the importance of his subject and his importance in it.
    If you overestimate the importance of what you're working on, that will compensate for your mistakenly harsh judgment of
    your initial results. If you look at something that's 20% of the way to a goal worth 100 and conclude that it's 10% of
    the way to a goal worth 200, your estimate of its expected value is correct even though both components are wrong.
</p><br>
<p>    
    It also helps, as Hardy suggests, to be slightly overconfident. I've noticed in many fields that the most successful
    people are slightly overconfident. On the face of it this seems implausible. Surely it would be optimal to have exactly
    the right estimate of one's abilities. How could it be an advantage to be mistaken? Because this error compensates for
    other sources of error in the opposite direction: being slightly overconfident armors you against both other people's
    skepticism and your own.
</p><br>
<p>   
    Ignorance has a similar effect. It's safe to make the mistake of judging early work as finished work if you're a
    sufficiently lax judge of finished work. I doubt it's possible to cultivate this kind of ignorance, but empirically it's
    a real advantage, especially for the young.
</p><br>
<p>    
    Another way to get through the lame phase of ambitious projects is to surround yourself with the right people — to
    create an eddy in the social headwind. But it's not enough to collect people who are always encouraging. You'd learn to
    discount that. You need colleagues who can actually tell an ugly duckling from a baby swan. The people best able to do
    this are those working on similar projects of their own, which is why university departments and research labs work so
    well. You don't need institutions to collect colleagues. They naturally coalesce, given the chance. But it's very much
    worth accelerating this process by seeking out other people trying to do new things.
</p><br>
<p>    
    Teachers are in effect a special case of colleagues. It's a teacher's job both to see the promise of early work and to
    encourage you to continue. But teachers who are good at this are unfortunately quite rare, so if you have the
    opportunity to learn from one, take it. [3]
</p><br>
<p>    
    For some it might work to rely on sheer discipline: to tell yourself that you just have to press on through the initial
    crap phase and not get discouraged. But like a lot of "just tell yourself" advice, this is harder than it sounds. And it
    gets still harder as you get older, because your standards rise. The old do have one compensating advantage though:
    they've been through this before.
</p><br>
<p>   
    It can help if you focus less on where you are and more on the rate of change. You won't worry so much about doing bad
    work if you can see it improving. Obviously the faster it improves, the easier this is. So when you start something new,
    it's good if you can spend a lot of time on it. That's another advantage of being young: you tend to have bigger blocks
    of time.
</p><br>
<p>   
    Another common trick is to start by considering new work to be of a different, less exacting type. To start a painting
    saying that it's just a sketch, or a new piece of software saying that it's just a quick hack. Then you judge your
    initial results by a lower standard. Once the project is rolling you can sneakily convert it to something more. [4]
</p><br>
<p>   
    This will be easier if you use a medium that lets you work fast and doesn't require too much commitment up front. It's
    easier to convince yourself that something is just a sketch when you're drawing in a notebook than when you're carving
    stone. Plus you get initial results faster. [5] [6]
</p><br>
<p>    
    It will be easier to try out a risky project if you think of it as a way to learn and not just as a way to make
    something. Then even if the project truly is a failure, you'll still have gained by it. If the problem is sharply enough
    defined, failure itself is knowledge: if the theorem you're trying to prove turns out to be false, or you use a
    structural member of a certain size and it fails under stress, you've learned something, even if it isn't what you
    wanted to learn. [7]
</p><br>
<p>   
    One motivation that works particularly well for me is curiosity. I like to try new things just to see how they'll turn
    out. We started Y Combinator in this spirit, and it was one of main things that kept me going while I was working on
    Bel. Having worked for so long with various dialects of Lisp, I was very curious to see what its inherent shape was:
    what you'd end up with if you followed the axiomatic approach all the way.
</p><br>
<p>    
    But it's a bit strange that you have to play mind games with yourself to avoid being discouraged by lame-looking early
    efforts. The thing you're trying to trick yourself into believing is in fact the truth. A lame-looking early version of
    an ambitious project truly is more valuable than it seems. So the ultimate solution may be to teach yourself that.
</p><br>
<p>   
    One way to do it is to study the histories of people who've done great work. What were they thinking early on? What was
    the very first thing they did? It can sometimes be hard to get an accurate answer to this question, because people are
    often embarrassed by their earliest work and make little effort to publish it. (They too misjudge it.) But when you can
    get an accurate picture of the first steps someone made on the path to some great work, they're often pretty feeble. [8]
</p><br>
<p>   
    Perhaps if you study enough such cases, you can teach yourself to be a better judge of early work. Then you'll be immune
    both to other people's skepticism and your own fear of making something lame. You'll see early work for what it is.
</p><br>
<p>    
    Curiously enough, the solution to the problem of judging early work too harshly is to realize that our attitudes toward
    it are themselves early work. Holding everything to the same standard is a crude version 1. We're already evolving
    better customs, and we can already see signs of how big the payoff will be.
</p><br>
<p>    
     Notes
    </p><br>
    <p>    
    [1] This assumption may be too conservative. There is some evidence that historically the Bay Area has attracted a
    different sort of person than, say, New York City.
</p><br>
<p>   
    [2] One of their great favorites is Theranos. But the most conspicuous feature of Theranos's cap table is the absence of
    Silicon Valley firms. Journalists were fooled by Theranos, but Silicon Valley investors weren't.
</p><br>
<p>   
    [3] I made two mistakes about teachers when I was younger. I cared more about professors' research than their
    reputations as teachers, and I was also wrong about what it meant to be a good teacher. I thought it simply meant to be
    good at explaining things.
</p><br>
<p>     
    [4] Patrick Collison points out that you can go past treating something as a hack in the sense of a prototype and onward
    to the sense of the word that means something closer to a practical joke:
    I think there may be something related to being a hack that can be powerful — the idea of making the tenuousness and
    implausibility a feature. "Yes, it's a bit ridiculous, right? I'm just trying to see how far such a naive approach can
    get." YC seemed to me to have this characteristic.
    [5] Much of the advantage of switching from physical to digital media is not the software per se but that it lets you
    start something new with little upfront commitment.
</p><br>
<p>    
    [6] John Carmack adds:
    The value of a medium without a vast gulf between the early work and the final work is exemplified in game mods. The
    original Quake game was a golden age for mods, because everything was very flexible, but so crude due to technical
    limitations, that quick hacks to try out a gameplay idea weren't all that far from the official game. Many careers were
    born from that, but as the commercial game quality improved over the years, it became almost a full time job to make a
    successful mod that would be appreciated by the community. This was dramatically reversed with Minecraft and later
    Roblox, where the entire esthetic of the experience was so explicitly crude that innovative gameplay concepts became the
    overriding value. These "crude" game mods by single authors are now often bigger deals than massive professional teams'
    work.
    [7] Lisa Randall suggests that we
    treat new things as experiments. That way there's no such thing as failing, since you learn something no matter what.
    You treat it like an experiment in the sense that if it really rules something out, you give up and move on, but if
    there's some way to vary it to make it work better, go ahead and do that
    [8] Michael Nielsen points out that the internet has made this easier, because you can see programmers' first commits,
    musicians' first videos, and so on.
</p><br>
<p>   
    Thanks to Trevor Blackwell, John Carmack, Patrick Collison, Jessica Livingston, Michael Nielsen, and Lisa Randall for
    reading drafts of this.
</p><br>
<br><br>
<span id="title">Modeling A Wealth Tax</span>
                <br>
                <span id="secondText">October 2020</span>
                <br><br><br><br>   
<p>
Some politicians are proposing to introduce wealth taxes in addition to income and capital gains taxes. Let's try
modeling the effects of various levels of wealth tax to see what they would mean in practice for a startup founder.
</p><br>
<p>  
Suppose you start a successful startup in your twenties, and then live for another 60 years. How much of your stock will
a wealth tax consume?
</p><br>
<p>  
If the wealth tax applies to all your assets, it's easy to calculate its effect. A wealth tax of 1% means you get to
keep 99% of your stock each year. After 60 years the proportion of stock you'll have left will be .99^60, or .547. So a
straight 1% wealth tax means the government will over the course of your life take 45% of your stock.
</p><br>
<p>  
(Losing shares does not, obviously, mean becoming net poorer unless the value per share is increasing by less than the
wealth tax rate.)
</p><br>
<p>  
Here's how much stock the government would take over 60 years at various levels of wealth tax:
</p><br>
<p>  
wealth tax government takes<br>
0.1% 6%<br>
0.5% 26%<br>
1.0% 45%<br>
2.0% 70%<br>
3.0% 84%<br>
4.0% 91%<br>
5.0% 95%<br>
</p><br>
<p>  
A wealth tax will usually have a threshold at which it starts. How much difference would a high threshold make? To model
that, we need to make some assumptions about the initial value of your stock and the growth rate.
</p><br>
<p>  
Suppose your stock is initially worth $2 million, and the company's trajectory is as follows: the value of your stock
grows 3x for 2 years, then 2x for 2 years, then 50% for 2 years, after which you just get a typical public company
growth rate, which we'll call 8%. [1] Suppose the wealth tax threshold is $50 million. How much stock does the
government take now?
</p><br>
<p>  
wealth tax government takes<br>
0.1%     5%<br>
0.5%     23%<br>
1.0%    41%<br>
2.0%    65%<br>
3.0%    79%<br>
4.0%    88%<br>
5.0%     93%<br>
</p><br>
<p>  
It may at first seem surprising that such apparently small tax rates produce such dramatic effects. A 2% wealth tax with
a $50 million threshold takes about two thirds of a successful founder's stock.
</p><br>
<p>  
The reason wealth taxes have such dramatic effects is that they're applied over and over to the same money. Income tax
happens every year, but only to that year's income. Whereas if you live for 60 years after acquiring some asset, a
wealth tax will tax that same asset 60 times. A wealth tax compounds.
</p><br>
<p>  
Note
</p><br>
<p>  
[1] In practice, eventually some of this 8% would come in the form of dividends, which are taxed as income at issue, so
this model actually represents the most optimistic case for the founder.
</p><br>
<br><br>
<span id="title">The Four Quadrants Of Conformism</span>
                <br>
                <span id="secondText">July 2020</span>
                <br><br><br><br>   
<p>
    One of the most revealing ways to classify people is by the degree and aggressiveness of their conformism. Imagine a
Cartesian coordinate system whose horizontal axis runs from conventional-minded on the left to independent-minded on the
right, and whose vertical axis runs from passive at the bottom to aggressive at the top. The resulting four quadrants
define four types of people. Starting in the upper left and going counter-clockwise: aggressively conventional-minded,
passively conventional-minded, passively independent-minded, and aggressively independent-minded.
</p><br>
<p> 
I think that you'll find all four types in most societies, and that which quadrant people fall into depends more on
their own personality than the beliefs prevalent in their society. [1]
</p><br>
<p> 
Young children offer some of the best evidence for both points. Anyone who's been to primary school has seen the four
types, and the fact that school rules are so arbitrary is strong evidence that the quadrant people fall into depends
more on them than the rules.
</p><br>
<p> 
The kids in the upper left quadrant, the aggressively conventional-minded ones, are the tattletales. They believe not
only that rules must be obeyed, but that those who disobey them must be punished.
</p><br>
<p> 
The kids in the lower left quadrant, the passively conventional-minded, are the sheep. They're careful to obey the
rules, but when other kids break them, their impulse is to worry that those kids will be punished, not to ensure that
they will.
</p><br>
<p> 
The kids in the lower right quadrant, the passively independent-minded, are the dreamy ones. They don't care much about
rules and probably aren't 100% sure what the rules even are.
</p><br>
<p> 
And the kids in the upper right quadrant, the aggressively independent-minded, are the naughty ones. When they see a
rule, their first impulse is to question it. Merely being told what to do makes them inclined to do the opposite.
</p><br>
<p> 
When measuring conformism, of course, you have to say with respect to what, and this changes as kids get older. For
younger kids it's the rules set by adults. But as kids get older, the source of rules becomes their peers. So a pack of
teenagers who all flout school rules in the same way are not independent-minded; rather the opposite.
</p><br>
<p> 
In adulthood we can recognize the four types by their distinctive calls, much as you could recognize four species of
birds. The call of the aggressively conventional-minded is "Crush <outgroup>!" (It's rather alarming to see an
    exclamation point after a variable, but that's the whole problem with the aggressively conventional-minded.) The
    call of the passively conventional-minded is "What will the neighbors think?" The call of the passively
    independent-minded is "To each his own." And the call of the aggressively independent-minded is "Eppur si muove."
</p><br>
<p> 
    The four types are not equally common. There are more passive people than aggressive ones, and far more
    conventional-minded people than independent-minded ones. So the passively conventional-minded are the largest group,
    and the aggressively independent-minded the smallest.
</p><br>
<p> 
    Since one's quadrant depends more on one's personality than the nature of the rules, most people would occupy the
    same quadrant even if they'd grown up in a quite different society.
</p><br>
<p> 
    Princeton professor Robert George recently wrote:
    I sometimes ask students what their position on slavery would have been had they been white and living in the South
    before abolition. Guess what? They all would have been abolitionists! They all would have bravely spoken out against
    slavery, and worked tirelessly against it.
    He's too polite to say so, but of course they wouldn't. And indeed, our default assumption should not merely be that
    his students would, on average, have behaved the same way people did at the time, but that the ones who are
    aggressively conventional-minded today would have been aggressively conventional-minded then too. In other words,
    that they'd not only not have fought against slavery, but that they'd have been among its staunchest defenders.
</p><br>
<p> 
    I'm biased, I admit, but it seems to me that aggressively conventional-minded people are responsible for a
    disproportionate amount of the trouble in the world, and that a lot of the customs we've evolved since the
    Enlightenment have been designed to protect the rest of us from them. In particular, the retirement of the concept
    of heresy and its replacement by the principle of freely debating all sorts of different ideas, even ones that are
    currently considered unacceptable, without any punishment for those who try them out to see if they work. [2]
</p><br>
<p> 
    Why do the independent-minded need to be protected, though? Because they have all the new ideas. To be a successful
    scientist, for example, it's not enough just to be right. You have to be right when everyone else is wrong.
    Conventional-minded people can't do that. For similar reasons, all successful startup CEOs are not merely
    independent-minded, but aggressively so. So it's no coincidence that societies prosper only to the extent that they
    have customs for keeping the conventional-minded at bay. [3]
</p><br>
<p> 
    In the last few years, many of us have noticed that the customs protecting free inquiry have been weakened. Some say
    we're overreacting — that they haven't been weakened very much, or that they've been weakened in the service of a
    greater good. The latter I'll dispose of immediately. When the conventional-minded get the upper hand, they always
    say it's in the service of a greater good. It just happens to be a different, incompatible greater good each time.
</p><br>
<p> 
    As for the former worry, that the independent-minded are being oversensitive, and that free inquiry hasn't been shut
    down that much, you can't judge that unless you are yourself independent-minded. You can't know how much of the
    space of ideas is being lopped off unless you have them, and only the independent-minded have the ones at the edges.
    Precisely because of this, they tend to be very sensitive to changes in how freely one can explore ideas. They're
    the canaries in this coalmine.
</p><br>
<p> 
    The conventional-minded say, as they always do, that they don't want to shut down the discussion of all ideas, just
    the bad ones.
</p><br>
<p> 
    You'd think it would be obvious just from that sentence what a dangerous game they're playing. But I'll spell it
    out. There are two reasons why we need to be able to discuss even "bad" ideas.
</p><br>
<p> 
    The first is that any process for deciding which ideas to ban is bound to make mistakes. All the more so because no
    one intelligent wants to undertake that kind of work, so it ends up being done by the stupid. And when a process
    makes a lot of mistakes, you need to leave a margin for error. Which in this case means you need to ban fewer ideas
    than you'd like to. But that's hard for the aggressively conventional-minded to do, partly because they enjoy seeing
    people punished, as they have since they were children, and partly because they compete with one another. Enforcers
    of orthodoxy can't allow a borderline idea to exist, because that gives other enforcers an opportunity to one-up
    them in the moral purity department, and perhaps even to turn enforcer upon them. So instead of getting the margin
    for error we need, we get the opposite: a race to the bottom in which any idea that seems at all bannable ends up
    being banned. [4]
</p><br>
<p> 
    The second reason it's dangerous to ban the discussion of ideas is that ideas are more closely related than they
    look. Which means if you restrict the discussion of some topics, it doesn't only affect those topics. The
    restrictions propagate back into any topic that yields implications in the forbidden ones. And that is not an edge
    case. The best ideas do exactly that: they have consequences in fields far removed from their origins. Having ideas
    in a world where some ideas are banned is like playing soccer on a pitch that has a minefield in one corner. You
    don't just play the same game you would have, but on a different shaped pitch. You play a much more subdued game
    even on the ground that's safe.
</p><br>
<p> 
    In the past, the way the independent-minded protected themselves was to congregate in a handful of places — first in
    courts, and later in universities — where they could to some extent make their own rules. Places where people work
    with ideas tend to have customs protecting free inquiry, for the same reason wafer fabs have powerful air filters,
    or recording studios good sound insulation. For the last couple centuries at least, when the aggressively
    conventional-minded were on the rampage for whatever reason, universities were the safest places to be.
</p><br>
<p> 
    That may not work this time though, due to the unfortunate fact that the latest wave of intolerance began in
    universities. It began in the mid 1980s, and by 2000 seemed to have died down, but it has recently flared up again
    with the arrival of social media. This seems, unfortunately, to have been an own goal by Silicon Valley. Though the
    people who run Silicon Valley are almost all independent-minded, they've handed the aggressively conventional-minded
    a tool such as they could only have dreamed of.
</p><br>
<p> 
    On the other hand, perhaps the decline in the spirit of free inquiry within universities is as much the symptom of
    the departure of the independent-minded as the cause. People who would have become professors 50 years ago have
    other options now. Now they can become quants or start startups. You have to be independent-minded to succeed at
    either of those. If these people had been professors, they'd have put up a stiffer resistance on behalf of academic
    freedom. So perhaps the picture of the independent-minded fleeing declining universities is too gloomy. Perhaps the
    universities are declining because so many have already left. [5]
</p><br>
<p> 
    Though I've spent a lot of time thinking about this situation, I can't predict how it plays out. Could some
    universities reverse the current trend and remain places where the independent-minded want to congregate? Or will
    the independent-minded gradually abandon them? I worry a lot about what we might lose if that happened.
</p><br>
<p> 
    But I'm hopeful long term. The independent-minded are good at protecting themselves. If existing institutions are
    compromised, they'll create new ones. That may require some imagination. But imagination is, after all, their
    specialty.
</p><br>
<p> 
Notes
</p><br>
<p> 
    [1] I realize of course that if people's personalities vary in any two ways, you can use them as axes and call the
    resulting four quadrants personality types. So what I'm really claiming is that the axes are orthogonal and that
    there's significant variation in both.
</p><br>
<p> 
    [2] The aggressively conventional-minded aren't responsible for all the trouble in the world. Another big source of
    trouble is the sort of charismatic leader who gains power by appealing to them. They become much more dangerous when
    such leaders emerge.
</p><br>
<p> 
    [3] I never worried about writing things that offended the conventional-minded when I was running Y Combinator. If
    YC were a cookie company, I'd have faced a difficult moral choice. Conventional-minded people eat cookies too. But
    they don't start successful startups. So if I deterred them from applying to YC, the only effect was to save us work
    reading applications.
</p><br>
<p> 
    [4] There has been progress in one area: the punishments for talking about banned ideas are less severe than in the
    past. There's little danger of being killed, at least in richer countries. The aggressively conventional-minded are
    mostly satisfied with getting people fired.
</p><br>
<p> 
    [5] Many professors are independent-minded — especially in math, the hard sciences, and engineering, where you have
    to be to succeed. But students are more representative of the general population, and thus mostly
    conventional-minded. So when professors and students are in conflict, it's not just a conflict between generations
    but also between different types of people.
</p><br>
<p> Thanks to Sam Altman, Trevor Blackwell, Nicholas Christakis, Patrick Collison, Sam Gichuru, Jessica Livingston,
    Patrick McKenzie, Geoff Ralston, and Harj Taggar for reading drafts of this.
</p><br>
<br><br>
<span id="title">Orthodox Privilege</span>
                <br>
                <span id="secondText">July 2020</span>
                <br><br><br><br>   
<p>
    "Few people are capable of expressing with equanimity opinions which differ from the prejudices of their social
    environment. Most people are even incapable of forming such opinions."
</p><br>
<p>   
    — Einstein
</p><br>
<p>   
    
    There has been a lot of talk about privilege lately. Although the concept is overused, there is something to it, and in
    particular to the idea that privilege makes you blind — that you can't see things that are visible to someone whose life
    is very different from yours.
</p><br>
<p>    
    But one of the most pervasive examples of this kind of blindness is one that I haven't seen mentioned explicitly. I'm
    going to call it orthodox privilege: The more conventional-minded someone is, the more it seems to them that it's safe
    for everyone to express their opinions.
</p><br>
<p>    
    It's safe for them to express their opinions, because the source of their opinions is whatever it's currently acceptable
    to believe. So it seems to them that it must be safe for everyone. They literally can't imagine a true statement that
    would get them in trouble.
</p><br>
<p>    
    And yet at every point in history, there were true things that would get you in terrible trouble to say. Is ours the
    first where this isn't so? What an amazing coincidence that would be.
</p><br>
<p>    
    Surely it should at least be the default assumption that our time is not unique, and that there are true things you
    can't say now, just as there have always been. You would think. But even in the face of such overwhelming historical
    evidence, most people will go with their gut on this one.
</p><br>
<p>    
    The spectral signature of orthodox privilege is "Why don't you just say it?" The more extreme will even accuse you of
    specific heresies they imagine you must have in mind, though if there's more than one heresy current in your time, these
    accusations will tend to be nondeterministic: you must either be an xist or a yist.
</p><br>
<p>    
    Frustrating as it is to deal with these people, it's important to realize that they're in earnest. They're not
    pretending they think it's impossible for an idea to be both unorthodox and true. The world really looks that way to
    them.
</p><br>
<p>    
    How do you respond to orthodox privilege? Merely giving it a name may help somewhat, because it will remind you, when
    you encounter it, why the people you're talking to seem so strangely unreasonable. Because this is a uniquely tenacious
    form of privilege. People can overcome the blindness induced by most forms of privilege by learning more about whatever
    they're not. But they can't overcome orthodox privilege just by learning more. They'd have to become more
    independent-minded. If that happens at all, it doesn't happen on the time scale of one conversation.
</p><br>
<p>    
    It may be possible to convince some people that orthodox privilege must exist even though they can't sense it, just as
    one can with, say, dark matter. There may be some who could be convinced, for example, that it's very unlikely that this
    is the first point in history at which there's nothing true you can't say, even if they can't imagine specific examples.
</p><br>
<p>    
    But except with these people, I don't think it will work to say "check your privilege" about this type of privilege,
    because those in its demographic don't realize they're in it. It doesn't seem to conventional-minded people that they're
    conventional-minded. It just seems to them that they're right. Indeed, they tend to be particularly sure of it.
</p><br>
<p>    
    Perhaps the solution is to appeal to politeness. If someone says they can hear a high-pitched noise that you can't, it's
    only polite to take them at their word, instead of demanding evidence that's impossible to produce, or simply denying
    that they hear anything. Imagine how rude that would seem. Similarly, if someone says they can think of things that are
    true but that cannot be said, it's only polite to take them at their word, even if you can't think of any yourself.
</p><br>
<p>    
    Once you realize that orthodox privilege exists, a lot of other things become clearer. For example, how can it be that a
    large number of reasonable, intelligent people worry about something they call "cancel culture," while other reasonable,
    intelligent people deny that it's a problem? Once you understand the concept of orthodox privilege, it's easy to see the
    source of this disagreement. If you believe there's nothing true that you can't say, then anyone who gets in trouble for
    something they say must deserve it.
</p><br>
<p>     
    Thanks to Sam Altman, Trevor Blackwell, Patrick Collison, Antonio Garcia-Martinez, Jessica Livingston, Robert Morris,
    Michael Nielsen, Geoff Ralston, Max Roser, and Harj Taggar for reading drafts of this.
</p><br>
<br><br>
<span id="title">Coronavirus And Credibility</span>
                <br>
                <span id="secondText">April 2020</span>
                <br><br><br><br>   
<p>
    I recently saw a video of TV journalists and politicians confidently saying that the coronavirus would be no worse than
    the flu. What struck me about it was not just how mistaken they seemed, but how daring. How could they feel safe saying
    such things?
</p><br>
<p>    
    The answer, I realized, is that they didn't think they could get caught. They didn't realize there was any danger in
    making false predictions. These people constantly make false predictions, and get away with it, because the things they
    make predictions about either have mushy enough outcomes that they can bluster their way out of trouble, or happen so
    far in the future that few remember what they said.
</p><br>
<p>    
    An epidemic is different. It falsifies your predictions rapidly and unequivocally.
</p><br>
<p>   
    But epidemics are rare enough that these people clearly didn't realize this was even a possibility. Instead they just
    continued to use their ordinary m.o., which, as the epidemic has made clear, is to talk confidently about things they
    don't understand.
</p><br>
<p>    
    An event like this is thus a uniquely powerful way of taking people's measure. As Warren Buffett said, "It's only when
    the tide goes out that you learn who's been swimming naked." And the tide has just gone out like never before.
</p><br>
<p>    
    Now that we've seen the results, let's remember what we saw, because this is the most accurate test of credibility we're
    ever likely to have. I hope.
</p><br>
<br><br>
<span id="title">How To Write Usefully</span>
                <br>
                <span id="secondText">February 2020</span>
                <br><br><br><br>   
<p>
    What should an essay be? Many people would say persuasive. That's what a lot of us were taught essays should be. But I
    think we can aim for something more ambitious: that an essay should be useful.
</p><br>
<p>    
    To start with, that means it should be correct. But it's not enough merely to be correct. It's easy to make a statement
    correct by making it vague. That's a common flaw in academic writing, for example. If you know nothing at all about an
    issue, you can't go wrong by saying that the issue is a complex one, that there are many factors to be considered, that
    it's a mistake to take too simplistic a view of it, and so on.
</p><br>
<p>    
    Though no doubt correct, such statements tell the reader nothing. Useful writing makes claims that are as strong as they
    can be made without becoming false.
</p><br>
<p>     
    For example, it's more useful to say that Pike's Peak is near the middle of Colorado than merely somewhere in Colorado.
    But if I say it's in the exact middle of Colorado, I've now gone too far, because it's a bit east of the middle.
</p><br>
<p>    
    Precision and correctness are like opposing forces. It's easy to satisfy one if you ignore the other. The converse of
    vaporous academic writing is the bold, but false, rhetoric of demagogues. Useful writing is bold, but true.
</p><br>
<p>     
    It's also two other things: it tells people something important, and that at least some of them didn't already know.
</p><br>
<p>     
    Telling people something they didn't know doesn't always mean surprising them. Sometimes it means telling them something
    they knew unconsciously but had never put into words. In fact those may be the more valuable insights, because they tend
    to be more fundamental.
</p><br>
<p>     
    Let's put them all together. Useful writing tells people something true and important that they didn't already know, and
    tells them as unequivocally as possible.
</p><br>
<p>     
    Notice these are all a matter of degree. For example, you can't expect an idea to be novel to everyone. Any insight that
    you have will probably have already been had by at least one of the world's 7 billion people. But it's sufficient if an
    idea is novel to a lot of readers.
</p><br>
<p>     
    Ditto for correctness, importance, and strength. In effect the four components are like numbers you can multiply
    together to get a score for usefulness. Which I realize is almost awkwardly reductive, but nonetheless true.
</p><br>
<p>     
    _____
</p><br>
<p>     
    
    How can you ensure that the things you say are true and novel and important? Believe it or not, there is a trick for
    doing this. I learned it from my friend Robert Morris, who has a horror of saying anything dumb. His trick is not to say
    anything unless he's sure it's worth hearing. This makes it hard to get opinions out of him, but when you do, they're
    usually right.
</p><br>
<p>    
    Translated into essay writing, what this means is that if you write a bad sentence, you don't publish it. You delete it
    and try again. Often you abandon whole branches of four or five paragraphs. Sometimes a whole essay.
</p><br>
<p>     
    You can't ensure that every idea you have is good, but you can ensure that every one you publish is, by simply not
    publishing the ones that aren't.
</p><br>
<p>     
    In the sciences, this is called publication bias, and is considered bad. When some hypothesis you're exploring gets
    inconclusive results, you're supposed to tell people about that too. But with essay writing, publication bias is the way
    to go.
</p><br>
<p>     
    My strategy is loose, then tight. I write the first draft of an essay fast, trying out all kinds of ideas. Then I spend
    days rewriting it very carefully.
</p><br>
<p>      
    I've never tried to count how many times I proofread essays, but I'm sure there are sentences I've read 100 times before
    publishing them. When I proofread an essay, there are usually passages that stick out in an annoying way, sometimes
    because they're clumsily written, and sometimes because I'm not sure they're true. The annoyance starts out unconscious,
    but after the tenth reading or so I'm saying "Ugh, that part" each time I hit it. They become like briars that catch
    your sleeve as you walk past. Usually I won't publish an essay till they're all gone — till I can read through the whole
    thing without the feeling of anything catching.
</p><br>
<p>     
    I'll sometimes let through a sentence that seems clumsy, if I can't think of a way to rephrase it, but I will never
    knowingly let through one that doesn't seem correct. You never have to. If a sentence doesn't seem right, all you have
    to do is ask why it doesn't, and you've usually got the replacement right there in your head.
</p><br>
<p>     
    This is where essayists have an advantage over journalists. You don't have a deadline. You can work for as long on an
    essay as you need to get it right. You don't have to publish the essay at all, if you can't get it right. Mistakes seem
    to lose courage in the face of an enemy with unlimited resources. Or that's what it feels like. What's really going on
    is that you have different expectations for yourself. You're like a parent saying to a child "we can sit here all night
    till you eat your vegetables." Except you're the child too.
</p><br>
<p>      
    I'm not saying no mistake gets through. For example, I added condition (c) in "A Way to Detect Bias" after readers
    pointed out that I'd omitted it. But in practice you can catch nearly all of them.
</p><br>
<p>     
    There's a trick for getting importance too. It's like the trick I suggest to young founders for getting startup ideas:
    to make something you yourself want. You can use yourself as a proxy for the reader. The reader is not completely unlike
    you, so if you write about topics that seem important to you, they'll probably seem important to a significant number of
    readers as well.
</p><br>
<p>     
    Importance has two factors. It's the number of people something matters to, times how much it matters to them. Which
    means of course that it's not a rectangle, but a sort of ragged comb, like a Riemann sum.
</p><br>
<p>     
    The way to get novelty is to write about topics you've thought about a lot. Then you can use yourself as a proxy for the
    reader in this department too. Anything you notice that surprises you, who've thought about the topic a lot, will
    probably also surprise a significant number of readers. And here, as with correctness and importance, you can use the
    Morris technique to ensure that you will. If you don't learn anything from writing an essay, don't publish it.
</p><br>
<p>     
    You need humility to measure novelty, because acknowledging the novelty of an idea means acknowledging your previous
    ignorance of it. Confidence and humility are often seen as opposites, but in this case, as in many others, confidence
    helps you to be humble. If you know you're an expert on some topic, you can freely admit when you learn something you
    didn't know, because you can be confident that most other people wouldn't know it either.
</p><br>
<p>     
    The fourth component of useful writing, strength, comes from two things: thinking well, and the skillful use of
    qualification. These two counterbalance each other, like the accelerator and clutch in a car with a manual transmission.
    As you try to refine the expression of an idea, you adjust the qualification accordingly. Something you're sure of, you
    can state baldly with no qualification at all, as I did the four components of useful writing. Whereas points that seem
    dubious have to be held at arm's length with perhapses.
</p><br>
<p>     
    As you refine an idea, you're pushing in the direction of less qualification. But you can rarely get it down to zero.
    Sometimes you don't even want to, if it's a side point and a fully refined version would be too long.
</p><br>
<p>      
    Some say that qualifications weaken writing. For example, that you should never begin a sentence in an essay with "I
    think," because if you're saying it, then of course you think it. And it's true that "I think x" is a weaker statement
    than simply "x." Which is exactly why you need "I think." You need it to express your degree of certainty.
</p><br>
<p>     
    But qualifications are not scalars. They're not just experimental error. There must be 50 things they can express: how
    broadly something applies, how you know it, how happy you are it's so, even how it could be falsified. I'm not going to
    try to explore the structure of qualification here. It's probably more complex than the whole topic of writing usefully.
    Instead I'll just give you a practical tip: Don't underestimate qualification. It's an important skill in its own right,
    not just a sort of tax you have to pay in order to avoid saying things that are false. So learn and use its full range.
    It may not be fully half of having good ideas, but it's part of having them.
</p><br>
<p>     
    There's one other quality I aim for in essays: to say things as simply as possible. But I don't think this is a
    component of usefulness. It's more a matter of consideration for the reader. And it's a practical aid in getting things
    right; a mistake is more obvious when expressed in simple language. But I'll admit that the main reason I write simply
    is not for the reader's sake or because it helps get things right, but because it bothers me to use more or fancier
    words than I need to. It seems inelegant, like a program that's too long.
</p><br>
<p>     
    I realize florid writing works for some people. But unless you're sure you're one of them, the best advice is to write
    as simply as you can.
</p><br>
<p>      
    _____
    
</p><br>
<p>    
    I believe the formula I've given you, importance + novelty + correctness + strength, is the recipe for a good essay. But
    I should warn you that it's also a recipe for making people mad.
</p><br>
<p>     
    The root of the problem is novelty. When you tell people something they didn't know, they don't always thank you for it.
    Sometimes the reason people don't know something is because they don't want to know it. Usually because it contradicts
    some cherished belief. And indeed, if you're looking for novel ideas, popular but mistaken beliefs are a good place to
    find them. Every popular mistaken belief creates a dead zone of ideas around it that are relatively unexplored because
    they contradict it.
</p><br>
<p>     
    The strength component just makes things worse. If there's anything that annoys people more than having their cherished
    assumptions contradicted, it's having them flatly contradicted.
</p><br>
<p>     
    Plus if you've used the Morris technique, your writing will seem quite confident. Perhaps offensively confident, to
    people who disagree with you. The reason you'll seem confident is that you are confident: you've cheated, by only
    publishing the things you're sure of. It will seem to people who try to disagree with you that you never admit you're
    wrong. In fact you constantly admit you're wrong. You just do it before publishing instead of after.
</p><br>
<p>     
    And if your writing is as simple as possible, that just makes things worse. Brevity is the diction of command. If you
    watch someone delivering unwelcome news from a position of inferiority, you'll notice they tend to use lots of words, to
    soften the blow. Whereas to be short with someone is more or less to be rude to them.
</p><br>
<p>     
    It can sometimes work to deliberately phrase statements more weakly than you mean. To put "perhaps" in front of
    something you're actually quite sure of. But you'll notice that when writers do this, they usually do it with a wink.
</p><br>
<p>     
    I don't like to do this too much. It's cheesy to adopt an ironic tone for a whole essay. I think we just have to face
    the fact that elegance and curtness are two names for the same thing.
</p><br>
<p>     
    You might think that if you work sufficiently hard to ensure that an essay is correct, it will be invulnerable to
    attack. That's sort of true. It will be invulnerable to valid attacks. But in practice that's little consolation.
</p><br>
<p>     
    In fact, the strength component of useful writing will make you particularly vulnerable to misrepresentation. If you've
    stated an idea as strongly as you could without making it false, all anyone has to do is to exaggerate slightly what you
    said, and now it is false.
</p><br>
<p>     
    Much of the time they're not even doing it deliberately. One of the most surprising things you'll discover, if you start
    writing essays, is that people who disagree with you rarely disagree with what you've actually written. Instead they
    make up something you said and disagree with that.
</p><br>
<p>    
    For what it's worth, the countermove is to ask someone who does this to quote a specific sentence or passage you wrote
    that they believe is false, and explain why. I say "for what it's worth" because they never do. So although it might
    seem that this could get a broken discussion back on track, the truth is that it was never on track in the first place.
</p><br>
<p>     
    Should you explicitly forestall likely misinterpretations? Yes, if they're misinterpretations a reasonably smart and
    well-intentioned person might make. In fact it's sometimes better to say something slightly misleading and then add the
    correction than to try to get an idea right in one shot. That can be more efficient, and can also model the way such an
    idea would be discovered.
</p><br>
<p>     
    But I don't think you should explicitly forestall intentional misinterpretations in the body of an essay. An essay is a
    place to meet honest readers. You don't want to spoil your house by putting bars on the windows to protect against
    dishonest ones. The place to protect against intentional misinterpretations is in end-notes. But don't think you can
    predict them all. People are as ingenious at misrepresenting you when you say something they don't want to hear as they
    are at coming up with rationalizations for things they want to do but know they shouldn't. I suspect it's the same
    skill.
</p><br>
<p>     
    _____
    
</p><br>
<p>     
    As with most other things, the way to get better at writing essays is to practice. But how do you start? Now that we've
    examined the structure of useful writing, we can rephrase that question more precisely. Which constraint do you relax
    initially? The answer is, the first component of importance: the number of people who care about what you write.
</p><br>
<p>     
    If you narrow the topic sufficiently, you can probably find something you're an expert on. Write about that to start
    with. If you only have ten readers who care, that's fine. You're helping them, and you're writing. Later you can expand
    the breadth of topics you write about.
</p><br>
<p>     
    The other constraint you can relax is a little surprising: publication. Writing essays doesn't have to mean publishing
    them. That may seem strange now that the trend is to publish every random thought, but it worked for me. I wrote what
    amounted to essays in notebooks for about 15 years. I never published any of them and never expected to. I wrote them as
    a way of figuring things out. But when the web came along I'd had a lot of practice.
</p><br>
<p>     
    Incidentally, Steve Wozniak did the same thing. In high school he designed computers on paper for fun. He couldn't build
    them because he couldn't afford the components. But when Intel launched 4K DRAMs in 1975, he was ready.
</p><br>
<p>     
    _____
</p><br>
<p>     
    
    How many essays are there left to write though? The answer to that question is probably the most exciting thing I've
    learned about essay writing. Nearly all of them are left to write.
</p><br>
<p>     
    Although the essay is an old form, it hasn't been assiduously cultivated. In the print era, publication was expensive,
    and there wasn't enough demand for essays to publish that many. You could publish essays if you were already well known
    for writing something else, like novels. Or you could write book reviews that you took over to express your own ideas.
    But there was not really a direct path to becoming an essayist. Which meant few essays got written, and those that did
    tended to be about a narrow range of subjects.
</p><br>
<p>     
    Now, thanks to the internet, there's a path. Anyone can publish essays online. You start in obscurity, perhaps, but at
    least you can start. You don't need anyone's permission.
</p><br>
<p>     
    It sometimes happens that an area of knowledge sits quietly for years, till some change makes it explode. Cryptography
    did this to number theory. The internet is doing it to the essay.
</p><br>
<p>     
    The exciting thing is not that there's a lot left to write, but that there's a lot left to discover. There's a certain
    kind of idea that's best discovered by writing essays. If most essays are still unwritten, most such ideas are still
    undiscovered.
</p><br>
<p>     
     Notes
    </p><br>
    <p>      
    [1] Put railings on the balconies, but don't put bars on the windows.
</p><br>
<p>    
    [2] Even now I sometimes write essays that are not meant for publication. I wrote several to figure out what Y
    Combinator should do, and they were really helpful.
</p><br>
<p>        
    Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and Robert Morris for reading drafts of this.
</p><br>
<br><br>
<span id="title">Being A Noob</span>
                <br>
                <span id="secondText">January 2020</span>
                <br><br><br><br>   
<p>
    When I was young, I thought old people had everything figured out. Now that I'm old, I know this isn't true.
</p><br>
<p>   
    I constantly feel like a noob. It seems like I'm always talking to some startup working in a new field I know nothing
    about, or reading a book about a topic I don't understand well enough, or visiting some new country where I don't know
    how things work.
</p><br>
<p>    
    It's not pleasant to feel like a noob. And the word "noob" is certainly not a compliment. And yet today I realized
    something encouraging about being a noob: the more of a noob you are locally, the less of a noob you are globally.
</p><br>
<p>    
    For example, if you stay in your home country, you'll feel less of a noob than if you move to Farawavia, where
    everything works differently. And yet you'll know more if you move. So the feeling of being a noob is inversely
    correlated with actual ignorance.
</p><br>
<p>   
    But if the feeling of being a noob is good for us, why do we dislike it? What evolutionary purpose could such an
    aversion serve?
</p><br>
<p>    
    I think the answer is that there are two sources of feeling like a noob: being stupid, and doing something novel. Our
    dislike of feeling like a noob is our brain telling us "Come on, come on, figure this out." Which was the right thing to
    be thinking for most of human history. The life of hunter-gatherers was complex, but it didn't change as much as life
    does now. They didn't suddenly have to figure out what to do about cryptocurrency. So it made sense to be biased toward
    competence at existing problems over the discovery of new ones. It made sense for humans to dislike the feeling of being
    a noob, just as, in a world where food was scarce, it made sense for them to dislike the feeling of being hungry.
</p><br>
<p>    
    Now that too much food is more of a problem than too little, our dislike of feeling hungry leads us astray. And I think
    our dislike of feeling like a noob does too.
</p><br>
<p>    
    Though it feels unpleasant, and people will sometimes ridicule you for it, the more you feel like a noob, the better.
</p><br>
<br><br>
<span id="title">Haters</span><br>
                <span id="secondText">January 2020</span>
                <br><br><br><br>   
<p>
    (I originally intended this for startup founders, who are often surprised by the attention they get as their companies
    grow, but it applies equally to anyone who becomes famous.)
</p><br>
<p>    
    If you become sufficiently famous, you'll acquire some fans who like you too much. These people are sometimes called
    "fanboys," and though I dislike that term, I'm going to have to use it here. We need some word for them, because this is
    a distinct phenomenon from someone simply liking your work.
</p><br>
<p>   
    A fanboy is obsessive and uncritical. Liking you becomes part of their identity, and they create an image of you in
    their own head that is much better than reality. Everything you do is good, because you do it. If you do something bad,
    they find a way to see it as good. And their love for you is not, usually, a quiet, private one. They want everyone to
    know how great you are.
</p><br>
<p>    
    Well, you may be thinking, I could do without this kind of obsessive fan, but I know there are all kinds of people in
    the world, and if this is the worst consequence of fame, that's not so bad.
</p><br>
<p>    
    Unfortunately this is not the worst consequence of fame. As well as fanboys, you'll have haters.
</p><br>
<p>    
    A hater is obsessive and uncritical. Disliking you becomes part of their identity, and they create an image of you in
    their own head that is much worse than reality. Everything you do is bad, because you do it. If you do something good,
    they find a way to see it as bad. And their dislike for you is not, usually, a quiet, private one. They want everyone to
    know how awful you are.
</p><br>
<p>    
    If you're thinking of checking, I'll save you the trouble. The second and fifth paragraphs are identical except for
    "good" being switched to "bad" and so on.
</p><br>
<p>    
    I spent years puzzling about haters. What are they, and where do they come from? Then one day it dawned on me. Haters
    are just fanboys with the sign switched.
</p><br>
<p>    
    Note that by haters, I don't simply mean trolls. I'm not talking about people who say bad things about you and then move
    on. I'm talking about the much smaller group of people for whom this becomes a kind of obsession and who do it
    repeatedly over a long period.
</p><br>
<p>    
    Like fans, haters seem to be an automatic consequence of fame. Anyone sufficiently famous will have them. And like fans,
    haters are energized by the fame of whoever they hate. They hear a song by some pop singer. They don't like it much. If
    the singer were an obscure one, they'd just forget about it. But instead they keep hearing her name, and this seems to
    drive some people crazy. Everyone's always going on about this singer, but she's no good! She's a fraud!
</p><br>
<p>    
    That word "fraud" is an important one. It's the spectral signature of a hater to regard the object of their hatred as a
    fraud. They can't deny their fame. Indeed, their fame is if anything exaggerated in the hater's mind. They notice every
    mention of the singer's name, because every mention makes them angrier. In their own minds they exaggerate both the
    singer's fame and her lack of talent, and the only way to reconcile those two ideas is to conclude that she has tricked
    everyone.
</p><br>
<p>    
    What sort of people become haters? Can anyone become one? I'm not sure about this, but I've noticed some patterns.
    Haters are generally losers in a very specific sense: although they are occasionally talented, they have never achieved
    much. And indeed, anyone successful enough to have achieved significant fame would be unlikely to regard another famous
    person as a fraud on that account, because anyone famous knows how random fame is.
</p><br>
<p>     
    But haters are not always complete losers. They are not always the proverbial guy living in his mom's basement. Many
    are, but some have some amount of talent. In fact I suspect that a sense of frustrated talent is what drives some people
    to become haters. They're not just saying "It's unfair that so-and-so is famous," but "It's unfair that so-and-so is
    famous, and not me."
</p><br>
<p>    
    Could a hater be cured if they achieved something impressive? My guess is that's a moot point, because they never will.
    I've been able to observe for long enough that I'm fairly confident the pattern works both ways: not only do people who
    do great work never become haters, haters never do great work. Although I dislike the word "fanboy," it's evocative of
    something important about both haters and fanboys. It implies that the fanboy is so slavishly predictable in his
    admiration that he's diminished as a result, that he's less than a man.
</p><br>
<p>    
    Haters seem even more diminished. I can imagine being a fanboy. I can think of people whose work I admire so much that I
    could abase myself before them out of sheer gratitude. If P. G. Wodehouse were still alive, I could see myself being a
    Wodehouse fanboy. But I could not imagine being a hater.
</p><br>
<p>     
    Knowing that haters are just fanboys with the sign bit flipped makes it much easier to deal with them. We don't need a
    separate theory of haters. We can just use existing techniques for dealing with obsessive fans.
</p><br>
<p>    
    The most important of which is simply not to think much about them. If you're like most people who become famous enough
    to acquire haters, your initial reaction will be one of mystification. Why does this guy seem to have it in for me?
    Where does his obsessive energy come from, and what makes him so appallingly nasty? What did I do to set him off? Is it
    something I can fix?
</p><br>
<p>    
    The mistake here is to think of the hater as someone you have a dispute with. When you have a dispute with someone, it's
    usually a good idea to try to understand why they're upset and then fix things if you can. Disputes are distracting. But
    it's a false analogy to think of a hater as someone you have a dispute with. It's an understandable mistake, if you've
    never encountered haters before. But when you realize that you're dealing with a hater, and what a hater is, it's clear
    that it's a waste of time even to think about them. If you have obsessive fans, do you spend any time wondering what
    makes them love you so excessively? No, you just think "some people are kind of crazy," and that's the end of it.
</p><br>
<p>   
    Since haters are equivalent to fanboys, that's the way to deal with them too. There may have been something that set
    them off. But it's not something that would have set off a normal person, so there's no reason to spend any time
    thinking about it. It's not you, it's them.
</p><br>
<p>    Notes
</p><br>
<p>   
    [1] There are of course some people who are genuine frauds. How can you distinguish between x calling y a fraud because
    x is a hater, and because y is a fraud? Look at neutral opinion. Actual frauds are usually pretty conspicuous.
    Thoughtful people are rarely taken in by them. So if there are some thoughtful people who like y, you can usually assume
    y is not a fraud.
</p><br>
<p>    
    [2] I would make an exception for teenagers, who sometimes act in such extreme ways that they are literally not
    themselves. I can imagine a teenage kid being a hater and then growing out of it. But not anyone over 25.
</p><br>
<p>    
    [3] I have a much worse memory for misdeeds than my wife Jessica, who is a connoisseur of character, but I don't wish it
    were better. Most disputes are a waste of time even if you're in the right, and it's easy to bury the hatchet with
    someone if you can't remember why you were mad at them.
</p><br>
<p>    
    [4] A competent hater will not merely attack you individually but will try to get mobs after you. In some cases you may
    want to refute whatever bogus claim they made in order to do so. But err on the side of not, because ultimately it
    probably won't matter.
</p><br>
<p>    
    Thanks to Austen Allred, Trevor Blackwell, Patrick Collison, Christine Ford, Daniel Gackle, Jessica Livingston, Robert
    Morris, Elon Musk, Harj Taggar, and Peter Thiel for reading drafts of this.
</p><br>
<br><br>
<span id="title">The Two Kinds Of Moderate</span><br>
                <span id="secondText">December 2019</span>
                <br><br><br><br>   
<p>
    There are two distinct ways to be politically moderate: on purpose and by accident. Intentional moderates are trimmers,
    deliberately choosing a position mid-way between the extremes of right and left. Accidental moderates end up in the
    middle, on average, because they make up their own minds about each question, and the far right and far left are roughly
    equally wrong.
</p><br>
<p>   
    You can distinguish intentional from accidental moderates by the distribution of their opinions. If the far left opinion
    on some matter is 0 and the far right opinion 100, an intentional moderate's opinion on every question will be near 50.
    Whereas an accidental moderate's opinions will be scattered over a broad range, but will, like those of the intentional
    moderate, average to about 50.
</p><br>
<p>    
    Intentional moderates are similar to those on the far left and the far right in that their opinions are, in a sense, not
    their own. The defining quality of an ideologue, whether on the left or the right, is to acquire one's opinions in bulk.
    You don't get to pick and choose. Your opinions about taxation can be predicted from your opinions about sex. And
    although intentional moderates might seem to be the opposite of ideologues, their beliefs (though in their case the word
    "positions" might be more accurate) are also acquired in bulk. If the median opinion shifts to the right or left, the
    intentional moderate must shift with it. Otherwise they stop being moderate.
</p><br>
<p>   
    Accidental moderates, on the other hand, not only choose their own answers, but choose their own questions. They may not
    care at all about questions that the left and right both think are terribly important. So you can only even measure the
    politics of an accidental moderate from the intersection of the questions they care about and those the left and right
    care about, and this can sometimes be vanishingly small.
</p><br>
<p>    
    It is not merely a manipulative rhetorical trick to say "if you're not with us, you're against us," but often simply
    false.
</p><br>
<p>    
    Moderates are sometimes derided as cowards, particularly by the extreme left. But while it may be accurate to call
    intentional moderates cowards, openly being an accidental moderate requires the most courage of all, because you get
    attacked from both right and left, and you don't have the comfort of being an orthodox member of a large group to
    sustain you.
</p><br>
<p>    
    Nearly all the most impressive people I know are accidental moderates. If I knew a lot of professional athletes, or
    people in the entertainment business, that might be different. Being on the far left or far right doesn't affect how
    fast you run or how well you sing. But someone who works with ideas has to be independent-minded to do it well.
</p><br>
<p>    
    Or more precisely, you have to be independent-minded about the ideas you work with. You could be mindlessly doctrinaire
    in your politics and still be a good mathematician. In the 20th century, a lot of very smart people were Marxists — just
    no one who was smart about the subjects Marxism involves. But if the ideas you use in your work intersect with the
    politics of your time, you have two choices: be an accidental moderate, or be mediocre.
</p><br>
<p>    
    Notes
</p><br>
<p>     
    [1] It's possible in theory for one side to be entirely right and the other to be entirely wrong. Indeed, ideologues
    must always believe this is the case. But historically it rarely has been.
</p><br>
<p>   
    [2] For some reason the far right tend to ignore moderates rather than despise them as backsliders. I'm not sure why.
    Perhaps it means that the far right is less ideological than the far left. Or perhaps that they are more confident, or
    more resigned, or simply more disorganized. I just don't know.
</p><br>
<p>    
    [3] Having heretical opinions doesn't mean you have to express them openly. It may be easier to have them if you don't.
</p><br>
<p>    
    Thanks to Austen Allred, Trevor Blackwell, Patrick Collison, Jessica Livingston, Amjad Masad, Ryan Petersen, and Harj
    Taggar for reading drafts of this.
</p><br>
<br><br>
<span id="title">Fashionable Problems</span><br>
                <span id="secondText">December 2019</span>
                <br><br><br><br>   
<p>
    I've seen the same pattern in many different fields: even though lots of people have worked hard in the field, only a
    small fraction of the space of possibilities has been explored, because they've all worked on similar things.
</p><br>
<p>   
    Even the smartest, most imaginative people are surprisingly conservative when deciding what to work on. People who would
    never dream of being fashionable in any other way get sucked into working on fashionable problems.
</p><br>
<p>    
    If you want to try working on unfashionable problems, one of the best places to look is in fields that people think have
    already been fully explored: essays, Lisp, venture funding — you may notice a pattern here. If you can find a new
    approach into a big but apparently played out field, the value of whatever you discover will be multiplied by its
    enormous surface area.
</p><br>
<p>    
    The best protection against getting drawn into working on the same things as everyone else may be to genuinely love what
    you're doing. Then you'll continue to work on it even if you make the same mistake as other people and think that it's
    too marginal to matter.
</p><br>
<br><br>
<span id="title">Having Kids</span><br>
                <span id="secondText">December 2019</span>
                <br><br><br><br>   
<p>
    Before I had kids, I was afraid of having kids. Up to that point I felt about kids the way the young Augustine felt
    about living virtuously. I'd have been sad to think I'd never have children. But did I want them now? No.
</p><br>
<p>   
    If I had kids, I'd become a parent, and parents, as I'd known since I was a kid, were uncool. They were dull and
    responsible and had no fun. And while it's not surprising that kids would believe that, to be honest I hadn't seen much
    as an adult to change my mind. Whenever I'd noticed parents with kids, the kids seemed to be terrors, and the parents
    pathetic harried creatures, even when they prevailed.
</p><br>
<p>    
    When people had babies, I congratulated them enthusiastically, because that seemed to be what one did. But I didn't feel
    it at all. "Better you than me," I was thinking.
</p><br>
<p>   
    Now when people have babies I congratulate them enthusiastically and I mean it. Especially the first one. I feel like
    they just got the best gift in the world.
</p><br>
<p>   
    What changed, of course, is that I had kids. Something I dreaded turned out to be wonderful.
</p><br>
<p>   
    Partly, and I won't deny it, this is because of serious chemical changes that happened almost instantly when our first
    child was born. It was like someone flipped a switch. I suddenly felt protective not just toward our child, but toward
    all children. As I was driving my wife and new son home from the hospital, I approached a crosswalk full of pedestrians,
    and I found myself thinking "I have to be really careful of all these people. Every one of them is someone's child!"
</p><br>
<p>   
    So to some extent you can't trust me when I say having kids is great. To some extent I'm like a religious cultist
    telling you that you'll be happy if you join the cult too — but only because joining the cult will alter your mind in a
    way that will make you happy to be a cult member.
</p><br>
<p>   
    But not entirely. There were some things about having kids that I clearly got wrong before I had them.
</p><br>
<p>    
    For example, there was a huge amount of selection bias in my observations of parents and children. Some parents may have
    noticed that I wrote "Whenever I'd noticed parents with kids." Of course the times I noticed kids were when things were
    going wrong. I only noticed them when they made noise. And where was I when I noticed them? Ordinarily I never went to
    places with kids, so the only times I encountered them were in shared bottlenecks like airplanes. Which is not exactly a
    representative sample. Flying with a toddler is something very few parents enjoy.
</p><br>
<p>     
    What I didn't notice, because they tend to be much quieter, were all the great moments parents had with kids. People
    don't talk about these much — the magic is hard to put into words, and all other parents know about them anyway — but
    one of the great things about having kids is that there are so many times when you feel there is nowhere else you'd
    rather be, and nothing else you'd rather be doing. You don't have to be doing anything special. You could just be going
    somewhere together, or putting them to bed, or pushing them on the swings at the park. But you wouldn't trade these
    moments for anything. One doesn't tend to associate kids with peace, but that's what you feel. You don't need to look
    any further than where you are right now.
</p><br>
<p>   
    Before I had kids, I had moments of this kind of peace, but they were rarer. With kids it can happen several times a
    day.
</p><br>
<p>   
    My other source of data about kids was my own childhood, and that was similarly misleading. I was pretty bad, and was
    always in trouble for something or other. So it seemed to me that parenthood was essentially law enforcement. I didn't
    realize there were good times too.
</p><br>
<p>    
    I remember my mother telling me once when I was about 30 that she'd really enjoyed having me and my sister. My god, I
    thought, this woman is a saint. She not only endured all the pain we subjected her to, but actually enjoyed it? Now I
    realize she was simply telling the truth.
</p><br>
<p>    
    She said that one reason she liked having us was that we'd been interesting to talk to. That took me by surprise when I
    had kids. You don't just love them. They become your friends too. They're really interesting. And while I admit small
    children are disastrously fond of repetition (anything worth doing once is worth doing fifty times) it's often genuinely
    fun to play with them. That surprised me too. Playing with a 2 year old was fun when I was 2 and definitely not fun when
    I was 6. Why would it become fun again later? But it does.
</p><br>
<p>    
    There are of course times that are pure drudgery. Or worse still, terror. Having kids is one of those intense types of
    experience that are hard to imagine unless you've had them. But it is not, as I implicitly believed before having kids,
    simply your DNA heading for the lifeboats.
</p><br>
<p>    
    Some of my worries about having kids were right, though. They definitely make you less productive. I know having kids
    makes some people get their act together, but if your act was already together, you're going to have less time to do it
    in. In particular, you're going to have to work to a schedule. Kids have schedules. I'm not sure if it's because that's
    how kids are, or because it's the only way to integrate their lives with adults', but once you have kids, you tend to
    have to work on their schedule.
</p><br>
<p>    
    You will have chunks of time to work. But you can't let work spill promiscuously through your whole life, like I used to
    before I had kids. You're going to have to work at the same time every day, whether inspiration is flowing or not, and
    there are going to be times when you have to stop, even if it is.
</p><br>
<p>     
    I've been able to adapt to working this way. Work, like love, finds a way. If there are only certain times it can
    happen, it happens at those times. So while I don't get as much done as before I had kids, I get enough done.
</p><br>
<p>    
    I hate to say this, because being ambitious has always been a part of my identity, but having kids may make one less
    ambitious. It hurts to see that sentence written down. I squirm to avoid it. But if there weren't something real there,
    why would I squirm? The fact is, once you have kids, you're probably going to care more about them than you do about
    yourself. And attention is a zero-sum game. Only one idea at a time can be the top idea in your mind. Once you have
    kids, it will often be your kids, and that means it will less often be some project you're working on.
</p><br>
<p>   
    I have some hacks for sailing close to this wind. For example, when I write essays, I think about what I'd want my kids
    to know. That drives me to get things right. And when I was writing Bel, I told my kids that once I finished it I'd take
    them to Africa. When you say that sort of thing to a little kid, they treat it as a promise. Which meant I had to finish
    or I'd be taking away their trip to Africa. Maybe if I'm really lucky such tricks could put me net ahead. But the wind
    is there, no question.
</p><br>
<p>   
    On the other hand, what kind of wimpy ambition do you have if it won't survive having kids? Do you have so little to
    spare?
</p><br>
<p>   
    And while having kids may be warping my present judgement, it hasn't overwritten my memory. I remember perfectly well
    what life was like before. Well enough to miss some things a lot, like the ability to take off for some other country at
    a moment's notice. That was so great. Why did I never do that?
</p><br>
<p>   
    See what I did there? The fact is, most of the freedom I had before kids, I never used. I paid for it in loneliness, but
    I never used it.
</p><br>
<p>   
    I had plenty of happy times before I had kids. But if I count up happy moments, not just potential happiness but actual
    happy moments, there are more after kids than before. Now I practically have it on tap, almost any bedtime.
</p><br>
<p>   
    People's experiences as parents vary a lot, and I know I've been lucky. But I think the worries I had before having kids
    must be pretty common, and judging by other parents' faces when they see their kids, so must the happiness that kids
    bring.
</p><br>
<p>    Note
</p><br>
<p>    
    [1] Adults are sophisticated enough to see 2 year olds for the fascinatingly complex characters they are, whereas to
    most 6 year olds, 2 year olds are just defective 6 year olds.
</p><br>
<p>    
    Thanks to Trevor Blackwell, Jessica Livingston, and Robert Morris for reading drafts of this.
</p><br>
<br><br>
<span id="title">The Lesson To Unlearn</span><br>
                <span id="secondText">December 2019</span>
                <br><br><br><br>   
<p>
    The most damaging thing you learned in school wasn't something you learned in any specific class. It was learning to get
    good grades.
</p><br>
<p>     
    When I was in college, a particularly earnest philosophy grad student once told me that he never cared what grade he got
    in a class, only what he learned in it. This stuck in my mind because it was the only time I ever heard anyone say such
    a thing.
</p><br>
<p>  
    For me, as for most students, the measurement of what I was learning completely dominated actual learning in college. I
    was fairly earnest; I was genuinely interested in most of the classes I took, and I worked hard. And yet I worked by far
    the hardest when I was studying for a test.
</p><br>
<p>    
    In theory, tests are merely what their name implies: tests of what you've learned in the class. In theory you shouldn't
    have to prepare for a test in a class any more than you have to prepare for a blood test. In theory you learn from
    taking the class, from going to the lectures and doing the reading and/or assignments, and the test that comes afterward
    merely measures how well you learned.
</p><br>
<p>    
    In practice, as almost everyone reading this will know, things are so different that hearing this explanation of how
    classes and tests are meant to work is like hearing the etymology of a word whose meaning has changed completely. In
    practice, the phrase "studying for a test" was almost redundant, because that was when one really studied. The
    difference between diligent and slack students was that the former studied hard for tests and the latter didn't. No one
    was pulling all-nighters two weeks into the semester.
</p><br>
<p>    
    Even though I was a diligent student, almost all the work I did in school was aimed at getting a good grade on
    something.
</p><br>
<p>   
    To many people, it would seem strange that the preceding sentence has a "though" in it. Aren't I merely stating a
    tautology? Isn't that what a diligent student is, a straight-A student? That's how deeply the conflation of learning
    with grades has infused our culture.
</p><br>
<p>   
    Is it so bad if learning is conflated with grades? Yes, it is bad. And it wasn't till decades after college, when I was
    running Y Combinator, that I realized how bad it is.
</p><br>
<p>   
    I knew of course when I was a student that studying for a test is far from identical with actual learning. At the very
    least, you don't retain knowledge you cram into your head the night before an exam. But the problem is worse than that.
    The real problem is that most tests don't come close to measuring what they're supposed to.
</p><br>
<p>   
    If tests truly were tests of learning, things wouldn't be so bad. Getting good grades and learning would converge, just
    a little late. The problem is that nearly all tests given to students are terribly hackable. Most people who've gotten
    good grades know this, and know it so well they've ceased even to question it. You'll see when you realize how naive it
    sounds to act otherwise.
</p><br>
<p>    
    Suppose you're taking a class on medieval history and the final exam is coming up. The final exam is supposed to be a
    test of your knowledge of medieval history, right? So if you have a couple days between now and the exam, surely the
    best way to spend the time, if you want to do well on the exam, is to read the best books you can find about medieval
    history. Then you'll know a lot about it, and do well on the exam.
</p><br>
<p>    
    No, no, no, experienced students are saying to themselves. If you merely read good books on medieval history, most of
    the stuff you learned wouldn't be on the test. It's not good books you want to read, but the lecture notes and assigned
    reading in this class. And even most of that you can ignore, because you only have to worry about the sort of thing that
    could turn up as a test question. You're looking for sharply-defined chunks of information. If one of the assigned
    readings has an interesting digression on some subtle point, you can safely ignore that, because it's not the sort of
    thing that could be turned into a test question. But if the professor tells you that there were three underlying causes
    of the Schism of 1378, or three main consequences of the Black Death, you'd better know them. And whether they were in
    fact the causes or consequences is beside the point. For the purposes of this class they are.
</p><br>
<p>    
    At a university there are often copies of old exams floating around, and these narrow still further what you have to
    learn. As well as learning what kind of questions this professor asks, you'll often get actual exam questions. Many
    professors re-use them. After teaching a class for 10 years, it would be hard not to, at least inadvertently.
</p><br>
<p>    
    In some classes, your professor will have had some sort of political axe to grind, and if so you'll have to grind it
    too. The need for this varies. In classes in math or the hard sciences or engineering it's rarely necessary, but at the
    other end of the spectrum there are classes where you couldn't get a good grade without it.
</p><br>
<p>    
    Getting a good grade in a class on x is so different from learning a lot about x that you have to choose one or the
    other, and you can't blame students if they choose grades. Everyone judges them by their grades — graduate programs,
    employers, scholarships, even their own parents.
</p><br>
<p>    
    I liked learning, and I really enjoyed some of the papers and programs I wrote in college. But did I ever, after turning
    in a paper in some class, sit down and write another just for fun? Of course not. I had things due in other classes. If
    it ever came to a choice of learning or grades, I chose grades. I hadn't come to college to do badly.
</p><br>
<p>    
    Anyone who cares about getting good grades has to play this game, or they'll be surpassed by those who do. And at elite
    universities, that means nearly everyone, since someone who didn't care about getting good grades probably wouldn't be
    there in the first place. The result is that students compete to maximize the difference between learning and getting
    good grades.
</p><br>
<p>    
    Why are tests so bad? More precisely, why are they so hackable? Any experienced programmer could answer that. How
    hackable is software whose author hasn't paid any attention to preventing it from being hacked? Usually it's as porous
    as a colander.
</p><br>
<p>    
    Hackable is the default for any test imposed by an authority. The reason the tests you're given are so consistently bad
    — so consistently far from measuring what they're supposed to measure — is simply that the people creating them haven't
    made much effort to prevent them from being hacked.
</p><br>
<p>    
    But you can't blame teachers if their tests are hackable. Their job is to teach, not to create unhackable tests. The
    real problem is grades, or more precisely, that grades have been overloaded. If grades were merely a way for teachers to
    tell students what they were doing right and wrong, like a coach giving advice to an athlete, students wouldn't be
    tempted to hack tests. But unfortunately after a certain age grades become more than advice. After a certain age,
    whenever you're being taught, you're usually also being judged.
</p><br>
<p>    
    I've used college tests as an example, but those are actually the least hackable. All the tests most students take their
    whole lives are at least as bad, including, most spectacularly of all, the test that gets them into college. If getting
    into college were merely a matter of having the quality of one's mind measured by admissions officers the way scientists
    measure the mass of an object, we could tell teenage kids "learn a lot" and leave it at that. You can tell how bad
    college admissions are, as a test, from how unlike high school that sounds. In practice, the freakishly specific nature
    of the stuff ambitious kids have to do in high school is directly proportionate to the hackability of college
    admissions. The classes you don't care about that are mostly memorization, the random "extracurricular activities" you
    have to participate in to show you're "well-rounded," the standardized tests as artificial as chess, the "essay" you
    have to write that's presumably meant to hit some very specific target, but you're not told what.
</p><br>
<p>    
    As well as being bad in what it does to kids, this test is also bad in the sense of being very hackable. So hackable
    that whole industries have grown up to hack it. This is the explicit purpose of test-prep companies and admissions
    counsellors, but it's also a significant part of the function of private schools.
</p><br>
<p>    
    Why is this particular test so hackable? I think because of what it's measuring. Although the popular story is that the
    way to get into a good college is to be really smart, admissions officers at elite colleges neither are, nor claim to
    be, looking only for that. What are they looking for? They're looking for people who are not simply smart, but admirable
    in some more general sense. And how is this more general admirableness measured? The admissions officers feel it. In
    other words, they accept who they like.
</p><br>
<p>    
    So what college admissions is a test of is whether you suit the taste of some group of people. Well, of course a test
    like that is going to be hackable. And because it's both very hackable and there's (thought to be) a lot at stake, it's
    hacked like nothing else. That's why it distorts your life so much for so long.
</p><br>
<p>    
    It's no wonder high school students often feel alienated. The shape of their lives is completely artificial.
</p><br>
<p>   
    But wasting your time is not the worst thing the educational system does to you. The worst thing it does is to train you
    that the way to win is by hacking bad tests. This is a much subtler problem that I didn't recognize until I saw it
    happening to other people.
</p><br>
<p>   
    When I started advising startup founders at Y Combinator, especially young ones, I was puzzled by the way they always
    seemed to make things overcomplicated. How, they would ask, do you raise money? What's the trick for making venture
    capitalists want to invest in you? The best way to make VCs want to invest in you, I would explain, is to actually be a
    good investment. Even if you could trick VCs into investing in a bad startup, you'd be tricking yourselves too. You're
    investing time in the same company you're asking them to invest money in. If it's not a good investment, why are you
    even doing it?
</p><br>
<p>   
    Oh, they'd say, and then after a pause to digest this revelation, they'd ask: What makes a startup a good investment?
</p><br>
<p>    
    So I would explain that what makes a startup promising, not just in the eyes of investors but in fact, is growth.
    Ideally in revenue, but failing that in usage. What they needed to do was get lots of users.
</p><br>
<p>    
    How does one get lots of users? They had all kinds of ideas about that. They needed to do a big launch that would get
    them "exposure." They needed influential people to talk about them. They even knew they needed to launch on a tuesday,
    because that's when one gets the most attention.
</p><br>
<p>    
    No, I would explain, that is not how to get lots of users. The way you get lots of users is to make the product really
    great. Then people will not only use it but recommend it to their friends, so your growth will be exponential once you
    get it started.
</p><br>
<p>    
    At this point I've told the founders something you'd think would be completely obvious: that they should make a good
    company by making a good product. And yet their reaction would be something like the reaction many physicists must have
    had when they first heard about the theory of relativity: a mixture of astonishment at its apparent genius, combined
    with a suspicion that anything so weird couldn't possibly be right. Ok, they would say, dutifully. And could you
    introduce us to such-and-such influential person? And remember, we want to launch on Tuesday.
</p><br>
<p>   
    It would sometimes take founders years to grasp these simple lessons. And not because they were lazy or stupid. They
    just seemed blind to what was right in front of them.
</p><br>
<p>   
    Why, I would ask myself, do they always make things so complicated? And then one day I realized this was not a
    rhetorical question.
</p><br>
<p>    
    Why did founders tie themselves in knots doing the wrong things when the answer was right in front of them? Because that
    was what they'd been trained to do. Their education had taught them that the way to win was to hack the test. And
    without even telling them they were being trained to do this. The younger ones, the recent graduates, had never faced a
    non-artificial test. They thought this was just how the world worked: that the first thing you did, when facing any kind
    of challenge, was to figure out what the trick was for hacking the test. That's why the conversation would always start
    with how to raise money, because that read as the test. It came at the end of YC. It had numbers attached to it, and
    higher numbers seemed to be better. It must be the test.
</p><br>
<p>    
    There are certainly big chunks of the world where the way to win is to hack the test. This phenomenon isn't limited to
    schools. And some people, either due to ideology or ignorance, claim that this is true of startups too. But it isn't. In
    fact, one of the most striking things about startups is the degree to which you win by simply doing good work. There are
    edge cases, as there are in anything, but in general you win by getting users, and what users care about is whether the
    product does what they want.
</p><br>
<p>    
    Why did it take me so long to understand why founders made startups overcomplicated? Because I hadn't realized
    explicitly that schools train us to win by hacking bad tests. And not just them, but me! I'd been trained to hack bad
    tests too, and hadn't realized it till decades later.
</p><br>
<p>    
    I had lived as if I realized it, but without knowing why. For example, I had avoided working for big companies. But if
    you'd asked why, I'd have said it was because they were bogus, or bureaucratic. Or just yuck. I never understood how
    much of my dislike of big companies was due to the fact that you win by hacking bad tests.
</p><br>
<p>    
    Similarly, the fact that the tests were unhackable was a lot of what attracted me to startups. But again, I hadn't
    realized that explicitly.
</p><br>
<p>    
    I had in effect achieved by successive approximations something that may have a closed-form solution. I had gradually
    undone my training in hacking bad tests without knowing I was doing it. Could someone coming out of school banish this
    demon just by knowing its name, and saying begone? It seems worth trying.
</p><br>
<p>    
    Merely talking explicitly about this phenomenon is likely to make things better, because much of its power comes from
    the fact that we take it for granted. After you've noticed it, it seems the elephant in the room, but it's a pretty well
    camouflaged elephant. The phenomenon is so old, and so pervasive. And it's simply the result of neglect. No one meant
    things to be this way. This is just what happens when you combine learning with grades, competition, and the naive
    assumption of unhackability.
</p><br>
<p>    
    It was mind-blowing to realize that two of the things I'd puzzled about the most — the bogusness of high school, and the
    difficulty of getting founders to see the obvious — both had the same cause. It's rare for such a big block to slide
    into place so late.
</p><br>
<p>    
    Usually when that happens it has implications in a lot of different areas, and this case seems no exception. For
    example, it suggests both that education could be done better, and how you might fix it. But it also suggests a
    potential answer to the question all big companies seem to have: how can we be more like a startup? I'm not going to
    chase down all the implications now. What I want to focus on here is what it means for individuals.
</p><br>
<p>    
    To start with, it means that most ambitious kids graduating from college have something they may want to unlearn. But it
    also changes how you look at the world. Instead of looking at all the different kinds of work people do and thinking of
    them vaguely as more or less appealing, you can now ask a very specific question that will sort them in an interesting
    way: to what extent do you win at this kind of work by hacking bad tests?
</p><br>
<p>    
    It would help if there was a way to recognize bad tests quickly. Is there a pattern here? It turns out there is.
</p><br>
<p>    
    Tests can be divided into two kinds: those that are imposed by authorities, and those that aren't. Tests that aren't
    imposed by authorities are inherently unhackable, in the sense that no one is claiming they're tests of anything more
    than they actually test. A football match, for example, is simply a test of who wins, not which team is better. You can
    tell that from the fact that commentators sometimes say afterward that the better team won. Whereas tests imposed by
    authorities are usually proxies for something else. A test in a class is supposed to measure not just how well you did
    on that particular test, but how much you learned in the class. While tests that aren't imposed by authorities are
    inherently unhackable, those imposed by authorities have to be made unhackable. Usually they aren't. So as a first
    approximation, bad tests are roughly equivalent to tests imposed by authorities.
</p><br>
<p>    
    You might actually like to win by hacking bad tests. Presumably some people do. But I bet most people who find
    themselves doing this kind of work don't like it. They just take it for granted that this is how the world works, unless
    you want to drop out and be some kind of hippie artisan.
</p><br>
<p>    
    I suspect many people implicitly assume that working in a field with bad tests is the price of making lots of money. But
    that, I can tell you, is false. It used to be true. In the mid-twentieth century, when the economy was composed of
    oligopolies, the only way to the top was by playing their game. But it's not true now. There are now ways to get rich by
    doing good work, and that's part of the reason people are so much more excited about getting rich than they used to be.
    When I was a kid, you could either become an engineer and make cool things, or make lots of money by becoming an
    "executive." Now you can make lots of money by making cool things.
</p><br>
<p>    
    Hacking bad tests is becoming less important as the link between work and authority erodes. The erosion of that link is
    one of the most important trends happening now, and we see its effects in almost every kind of work people do. Startups
    are one of the most visible examples, but we see much the same thing in writing. Writers no longer have to submit to
    publishers and editors to reach readers; now they can go direct.
</p><br>
<p>    
    The more I think about this question, the more optimistic I get. This seems one of those situations where we don't
    realize how much something was holding us back until it's eliminated. And I can foresee the whole bogus edifice
    crumbling. Imagine what happens as more and more people start to ask themselves if they want to win by hacking bad
    tests, and decide that they don't. The kinds of work where you win by hacking bad tests will be starved of talent, and
    the kinds where you win by doing good work will see an influx of the most ambitious people. And as hacking bad tests
    shrinks in importance, education will evolve to stop training us to do it. Imagine what the world could look like if
    that happened.
</p><br>
<p>    
    This is not just a lesson for individuals to unlearn, but one for society to unlearn, and we'll be amazed at the energy
    that's liberated when we do.
</p><br>
<p>   
    Notes
</p><br>
<p>   
    [1] If using tests only to measure learning sounds impossibly utopian, that is already the way things work at Lambda
    School. Lambda School doesn't have grades. You either graduate or you don't. The only purpose of tests is to decide at
    each stage of the curriculum whether you can continue to the next. So in effect the whole school is pass/fail.
</p><br>
<p>     
    [2] If the final exam consisted of a long conversation with the professor, you could prepare for it by reading good
    books on medieval history. A lot of the hackability of tests in schools is due to the fact that the same test has to be
    given to large numbers of students.
</p><br>
<p>    
    [3] Learning is the naive algorithm for getting good grades.
</p><br>
<p>    
    [4] Hacking has multiple senses. There's a narrow sense in which it means to compromise something. That's the sense in
    which one hacks a bad test. But there's another, more general sense, meaning to find a surprising solution to a problem,
    often by thinking differently about it. Hacking in this sense is a wonderful thing. And indeed, some of the hacks people
    use on bad tests are impressively ingenious; the problem is not so much the hacking as that, because the tests are
    hackable, they don't test what they're meant to.
</p><br>
<p>    
    [5] The people who pick startups at Y Combinator are similar to admissions officers, except that instead of being
    arbitrary, their acceptance criteria are trained by a very tight feedback loop. If you accept a bad startup or reject a
    good one, you will usually know it within a year or two at the latest, and often within a month.
</p><br>
<p>    
    [6] I'm sure admissions officers are tired of reading applications from kids who seem to have no personality beyond
    being willing to seem however they're supposed to seem to get accepted. What they don't realize is that they are, in a
    sense, looking in a mirror. The lack of authenticity in the applicants is a reflection of the arbitrariness of the
    application process. A dictator might just as well complain about the lack of authenticity in the people around him.
</p><br>
<p>    
    [7] By good work, I don't mean morally good, but good in the sense in which a good craftsman does good work.
</p><br>
<p>    
    [8] There are borderline cases where it's hard to say which category a test falls in. For example, is raising venture
    capital like college admissions, or is it like selling to a customer?
</p><br>
<p>    
    [9] Note that a good test is merely one that's unhackable. Good here doesn't mean morally good, but good in the sense of
    working well. The difference between fields with bad tests and good ones is not that the former are bad and the latter
    are good, but that the former are bogus and the latter aren't. But those two measures are not unrelated. As Tara
    Ploughman said, the path from good to evil goes through bogus.
</p><br>
<p>    
    [10] People who think the recent increase in economic inequality is due to changes in tax policy seem very naive to
    anyone with experience in startups. Different people are getting rich now than used to, and they're getting much richer
    than mere tax savings could make them.
</p><br>
<p>   
    [11] Note to tiger parents: you may think you're training your kids to win, but if you're training them to win by
    hacking bad tests, you are, as parents so often do, training them to fight the last war.
</p><br>
<p>    Thanks to Austen Allred, Trevor Blackwell, Patrick Collison, Jessica Livingston, Robert Morris, and Harj Taggar for
    reading drafts of this.
</p><br>
<br><br>
<span id="title">Novelty And Heresy</span><br>
                <span id="secondText">November 2019</span>
                <br><br><br><br>   
<p>
    If you discover something new, there's a significant chance you'll be accused of some form of heresy.
</p><br>
<p>    
    To discover new things, you have to work on ideas that are good but non-obvious; if an idea is obviously good, other
    people are probably already working on it. One common way for a good idea to be non-obvious is for it to be hidden in
    the shadow of some mistaken assumption that people are very attached to. But anything you discover from working on such
    an idea will tend to contradict the mistaken assumption that was concealing it. And you will thus get a lot of heat from
    people attached to the mistaken assumption. Galileo and Darwin are famous examples of this phenomenon, but it's probably
    always an ingredient in the resistance to new ideas.
</p><br>
<p>    
    So it's particularly dangerous for an organization or society to have a culture of pouncing on heresy. When you suppress
    heresies, you don't just prevent people from contradicting the mistaken assumption you're trying to protect. You also
    suppress any idea that implies indirectly that it's false.
</p><br>
<p>    
    Every cherished mistaken assumption has a dead zone of unexplored ideas around it. And the more preposterous the
    assumption, the bigger the dead zone it creates.
</p><br>
<p>   
    There is a positive side to this phenomenon though. If you're looking for new ideas, one way to find them is by looking
    for heresies. When you look at the question this way, the depressingly large dead zones around mistaken assumptions
    become excitingly large mines of new ideas.
</p><br>
<br><br>
<span id="title">The Bus Ticket Theory Of Genius</span><br>
                <span id="secondText">November 2019</span>
                <br><br><br><br>   
<p>
    Everyone knows that to do great work you need both natural ability and determination. But there's a third ingredient
    that's not as well understood: an obsessive interest in a particular topic.
</p><br>
<p>    
    To explain this point I need to burn my reputation with some group of people, and I'm going to choose bus ticket
    collectors. There are people who collect old bus tickets. Like many collectors, they have an obsessive interest in the
    minutiae of what they collect. They can keep track of distinctions between different types of bus tickets that would be
    hard for the rest of us to remember. Because we don't care enough. What's the point of spending so much time thinking
    about old bus tickets?
</p><br>
<p>    
    Which leads us to the second feature of this kind of obsession: there is no point. A bus ticket collector's love is
    disinterested. They're not doing it to impress us or to make themselves rich, but for its own sake.
</p><br>
<p>   
    When you look at the lives of people who've done great work, you see a consistent pattern. They often begin with a bus
    ticket collector's obsessive interest in something that would have seemed pointless to most of their contemporaries. One
    of the most striking features of Darwin's book about his voyage on the Beagle is the sheer depth of his interest in
    natural history. His curiosity seems infinite. Ditto for Ramanujan, sitting by the hour working out on his slate what
    happens to series.
</p><br>
<p>    
    It's a mistake to think they were "laying the groundwork" for the discoveries they made later. There's too much
    intention in that metaphor. Like bus ticket collectors, they were doing it because they liked it.
</p><br>
<p>    
    But there is a difference between Ramanujan and a bus ticket collector. Series matter, and bus tickets don't.
</p><br>
<p>   
    If I had to put the recipe for genius into one sentence, that might be it: to have a disinterested obsession with
    something that matters.
</p><br>
<p>    
    Aren't I forgetting about the other two ingredients? Less than you might think. An obsessive interest in a topic is both
    a proxy for ability and a substitute for determination. Unless you have sufficient mathematical aptitude, you won't find
    series interesting. And when you're obsessively interested in something, you don't need as much determination: you don't
    need to push yourself as hard when curiosity is pulling you.  
</p><br>
<p>    An obsessive interest will even bring you luck, to the extent anything can. Chance, as Pasteur said, favors the prepared
    mind, and if there's one thing an obsessed mind is, it's prepared.
</p><br>
<p>    
    The disinterestedness of this kind of obsession is its most important feature. Not just because it's a filter for
    earnestness, but because it helps you discover new ideas.
</p><br>
<p>    
    The paths that lead to new ideas tend to look unpromising. If they looked promising, other people would already have
    explored them. How do the people who do great work discover these paths that others overlook? The popular story is that
    they simply have better vision: because they're so talented, they see paths that others miss. But if you look at the way
    great discoveries are made, that's not what happens. Darwin didn't pay closer attention to individual species than other
    people because he saw that this would lead to great discoveries, and they didn't. He was just really, really interested
    in such things.
</p><br>
<p>    
    Darwin couldn't turn it off. Neither could Ramanujan. They didn't discover the hidden paths that they did because they
    seemed promising, but because they couldn't help it. That's what allowed them to follow paths that someone who was
    merely ambitious would have ignored.
</p><br>
<p>   
    What rational person would decide that the way to write great novels was to begin by spending several years creating an
    imaginary elvish language, like Tolkien, or visiting every household in southwestern Britain, like Trollope? No one,
    including Tolkien and Trollope.
</p><br>
<p>    
    The bus ticket theory is similar to Carlyle's famous definition of genius as an infinite capacity for taking pains. But
    there are two differences. The bus ticket theory makes it clear that the source of this infinite capacity for taking
    pains is not infinite diligence, as Carlyle seems to have meant, but the sort of infinite interest that collectors have.
    It also adds an important qualification: an infinite capacity for taking pains about something that matters.
</p><br>
<p>    
    So what matters? You can never be sure. It's precisely because no one can tell in advance which paths are promising that
    you can discover new ideas by working on what you're interested in.
</p><br>
<p>    
    But there are some heuristics you can use to guess whether an obsession might be one that matters. For example, it's
    more promising if you're creating something, rather than just consuming something someone else creates. It's more
    promising if something you're interested in is difficult, especially if it's more difficult for other people than it is
    for you. And the obsessions of talented people are more likely to be promising. When talented people become interested
    in random things, they're not truly random.
</p><br>
<p>    
    But you can never be sure. In fact, here's an interesting idea that's also rather alarming if it's true: it may be that
    to do great work, you also have to waste a lot of time.
</p><br>
<p>    
    In many different areas, reward is proportionate to risk. If that rule holds here, then the way to find paths that lead
    to truly great work is to be willing to expend a lot of effort on things that turn out to be every bit as unpromising as
    they seem.
</p><br>
<p>    
    I'm not sure if this is true. On one hand, it seems surprisingly difficult to waste your time so long as you're working
    hard on something interesting. So much of what you do ends up being useful. But on the other hand, the rule about the
    relationship between risk and reward is so powerful that it seems to hold wherever risk occurs. Newton's case, at least,
    suggests that the risk/reward rule holds here. He's famous for one particular obsession of his that turned out to be
    unprecedentedly fruitful: using math to describe the world. But he had two other obsessions, alchemy and theology, that
    seem to have been complete wastes of time. He ended up net ahead. His bet on what we now call physics paid off so well
    that it more than compensated for the other two. But were the other two necessary, in the sense that he had to take big
    risks to make such big discoveries? I don't know.
</p><br>
<p>    
    Here's an even more alarming idea: might one make all bad bets? It probably happens quite often. But we don't know how
    often, because these people don't become famous.
</p><br>
<p>    
    It's not merely that the returns from following a path are hard to predict. They change dramatically over time. 1830 was
    a really good time to be obsessively interested in natural history. If Darwin had been born in 1709 instead of 1809, we
    might never have heard of him.
</p><br>
<p>    
    What can one do in the face of such uncertainty? One solution is to hedge your bets, which in this case means to follow
    the obviously promising paths instead of your own private obsessions. But as with any hedge, you're decreasing reward
    when you decrease risk. If you forgo working on what you like in order to follow some more conventionally ambitious
    path, you might miss something wonderful that you'd otherwise have discovered. That too must happen all the time,
    perhaps even more often than the genius whose bets all fail.
</p><br>
<p>    
    The other solution is to let yourself be interested in lots of different things. You don't decrease your upside if you
    switch between equally genuine interests based on which seems to be working so far. But there is a danger here too: if
    you work on too many different projects, you might not get deeply enough into any of them.
</p><br>
<p>    
    One interesting thing about the bus ticket theory is that it may help explain why different types of people excel at
    different kinds of work. Interest is much more unevenly distributed than ability. If natural ability is all you need to
    do great work, and natural ability is evenly distributed, you have to invent elaborate theories to explain the skewed
    distributions we see among those who actually do great work in various fields. But it may be that much of the skew has a
    simpler explanation: different people are interested in different things.
</p><br>
<p>    
    The bus ticket theory also explains why people are less likely to do great work after they have children. Here interest
    has to compete not just with external obstacles, but with another interest, and one that for most people is extremely
    powerful. It's harder to find time for work after you have kids, but that's the easy part. The real change is that you
    don't want to.
</p><br>
<p>    
    But the most exciting implication of the bus ticket theory is that it suggests ways to encourage great work. If the
    recipe for genius is simply natural ability plus hard work, all we can do is hope we have a lot of ability, and work as
    hard as we can. But if interest is a critical ingredient in genius, we may be able, by cultivating interest, to
    cultivate genius.
</p><br>
<p>    
    For example, for the very ambitious, the bus ticket theory suggests that the way to do great work is to relax a little.
    Instead of gritting your teeth and diligently pursuing what all your peers agree is the most promising line of research,
    maybe you should try doing something just for fun. And if you're stuck, that may be the vector along which to break out.
</p><br>
<p>    
    I've always liked Hamming's famous double-barrelled question: what are the most important problems in your field, and
    why aren't you working on one of them? It's a great way to shake yourself up. But it may be overfitting a bit. It might
    be at least as useful to ask yourself: if you could take a year off to work on something that probably wouldn't be
    important but would be really interesting, what would it be?
</p><br>
<p>    
    The bus ticket theory also suggests a way to avoid slowing down as you get older. Perhaps the reason people have fewer
    new ideas as they get older is not simply that they're losing their edge. It may also be because once you become
    established, you can no longer mess about with irresponsible side projects the way you could when you were young and no
    one cared what you did.
</p><br>
<p>    
    The solution to that is obvious: remain irresponsible. It will be hard, though, because the apparently random projects
    you take up to stave off decline will read to outsiders as evidence of it. And you yourself won't know for sure that
    they're wrong. But it will at least be more fun to work on what you want.
</p><br>
<p>    
    It may even be that we can cultivate a habit of intellectual bus ticket collecting in kids. The usual plan in education
    is to start with a broad, shallow focus, then gradually become more specialized. But I've done the opposite with my
    kids. I know I can count on their school to handle the broad, shallow part, so I take them deep.
</p><br>
<p>    
    When they get interested in something, however random, I encourage them to go preposterously, bus ticket collectorly,
    deep. I don't do this because of the bus ticket theory. I do it because I want them to feel the joy of learning, and
    they're never going to feel that about something I'm making them learn. It has to be something they're interested in.
    I'm just following the path of least resistance; depth is a byproduct. But if in trying to show them the joy of learning
    I also end up training them to go deep, so much the better.
</p><br>
<p>    
    Will it have any effect? I have no idea. But that uncertainty may be the most interesting point of all. There is so much
    more to learn about how to do great work. As old as human civilization feels, it's really still very young if we haven't
    nailed something so basic. It's exciting to think there are still discoveries to make about discovery. If that's the
    sort of thing you're interested in.
</p><br>
<p>    Notes
    
    [1] There are other types of collecting that illustrate this point better than bus tickets, but they're also more
    popular. It seemed just as well to use an inferior example rather than offend more people by telling them their hobby
    doesn't matter.
</p><br>
<p>   
    [2] I worried a little about using the word "disinterested," since some people mistakenly believe it means not
    interested. But anyone who expects to be a genius will have to know the meaning of such a basic word, so I figure they
    may as well start now.
</p><br>
<p>   
    [3] Think how often genius must have been nipped in the bud by people being told, or telling themselves, to stop messing
    about and be responsible. Ramanujan's mother was a huge enabler. Imagine if she hadn't been. Imagine if his parents had
    made him go out and get a job instead of sitting around at home doing math.
</p><br>
<p>   
    On the other hand, anyone quoting the preceding paragraph to justify not getting a job is probably mistaken.
</p><br>
<p>    
    [4] 1709 Darwin is to time what the Milanese Leonardo is to space.
</p><br>
<p>    
    [5] "An infinite capacity for taking pains" is a paraphrase of what Carlyle wrote. What he wrote, in his History of
    Frederick the Great, was "... it is the fruit of 'genius' (which means transcendent capacity of taking trouble, first of
    all)...." Since the paraphrase seems the name of the idea at this point, I kept it.
</p><br>
<p>    
    Carlyle's History was published in 1858. In 1785 Hérault de Séchelles quoted Buffon as saying "Le génie n'est qu'une
    plus grande aptitude à la patience." (Genius is only a greater aptitude for patience.)
</p><br>
<p>    
    [6] Trollope was establishing the system of postal routes. He himself sensed the obsessiveness with which he pursued
    this goal.
    It is amusing to watch how a passion will grow upon a man. During those two years it was the ambition of my life to
    cover the country with rural letter-carriers.
    Even Newton occasionally sensed the degree of his obsessiveness. After computing pi to 15 digits, he wrote in a letter
    to a friend:
    I am ashamed to tell you to how many figures I carried these computations, having no other business at the time.
    Incidentally, Ramanujan was also a compulsive calculator. As Kanigel writes in his excellent biography:
    One Ramanujan scholar, B. M. Wilson, later told how Ramanujan's research into number theory was often "preceded by a
    table of numerical results, carried usually to a length from which most of us would shrink."
    [7] Working to understand the natural world counts as creating rather than consuming.
</p><br>
<p>    
    Newton tripped over this distinction when he chose to work on theology. His beliefs did not allow him to see it, but
    chasing down paradoxes in nature is fruitful in a way that chasing down paradoxes in sacred texts is not.
</p><br>
<p>     
    [8] How much of people's propensity to become interested in a topic is inborn? My experience so far suggests the answer
    is: most of it. Different kids get interested in different things, and it's hard to make a child interested in something
    they wouldn't otherwise be. Not in a way that sticks. The most you can do on behalf of a topic is to make sure it gets a
    fair showing — to make it clear to them, for example, that there's more to math than the dull drills they do in school.
    After that it's up to the child.
</p><br>
<p>     
    Thanks to Marc Andreessen, Trevor Blackwell, Patrick Collison, Kevin Lacker, Jessica Livingston, Jackie McDonough,
    Robert Morris, Lisa Randall, Zak Stone, and my 7 year old for reading drafts of this.
</p><br>
<br><br>
<span id="title">General And Surprising</span><br>
                <span id="secondText">September 2017</span>
                <br><br><br><br>   
<p>
    The most valuable insights are both general and surprising. F = ma for example. But general and surprising is a hard
    combination to achieve. That territory tends to be picked clean, precisely because those insights are so valuable.
</p><br>
<p>    
    Ordinarily, the best that people can do is one without the other: either surprising without being general (e.g. gossip),
    or general without being surprising (e.g. platitudes).
</p><br>
<p>    
    Where things get interesting is the moderately valuable insights. You get those from small additions of whichever
    quality was missing. The more common case is a small addition of generality: a piece of gossip that's more than just
    gossip, because it teaches something interesting about the world. But another less common approach is to focus on the
    most general ideas and see if you can find something new to say about them. Because these start out so general, you only
    need a small delta of novelty to produce a useful insight.
</p><br>
<p>    
    A small delta of novelty is all you'll be able to get most of the time. Which means if you take this route, your ideas
    will seem a lot like ones that already exist. Sometimes you'll find you've merely rediscovered an idea that did already
    exist. But don't be discouraged. Remember the huge multiplier that kicks in when you do manage to think of something
    even a little new.
</p><br>
<p>    
    Corollary: the more general the ideas you're talking about, the less you should worry about repeating yourself. If you
    write enough, it's inevitable you will. Your brain is much the same from year to year and so are the stimuli that hit
    it. I feel slightly bad when I find I've said something close to what I've said before, as if I were plagiarizing
    myself. But rationally one shouldn't. You won't say something exactly the same way the second time, and that variation
    increases the chance you'll get that tiny but critical delta of novelty.
</p><br>
<p>    
    And of course, ideas beget ideas. (That sounds familiar.) An idea with a small amount of novelty could lead to one with
    more. But only if you keep going. So it's doubly important not to let yourself be discouraged by people who say there's
    not much new about something you've discovered. "Not much new" is a real achievement when you're talking about the most
    general ideas.
</p><br>
<p>     
    It's not true that there's nothing new under the sun. There are some domains where there's almost nothing new. But
    there's a big difference between nothing and almost nothing, when it's multiplied by the area under the sun.
</p><br>
<p>    
    Thanks to Sam Altman, Patrick Collison, and Jessica Livingston for reading drafts of this.        
</p><br>
<br><br>
<span id="title">Charisma / Power </span><br>
                <span id="secondText">January 2017</span>
                <br><br><br><br>   
<p>
    People who are powerful but uncharismatic will tend to be disliked. Their power makes them a target for criticism that
    they don't have the charisma to disarm. That was Hillary Clinton's problem. It also tends to be a problem for any CEO
    who is more of a builder than a schmoozer. And yet the builder-type CEO is (like Hillary) probably the best person for
    the job.
</p><br>
<p>    
    I don't think there is any solution to this problem. It's human nature. The best we can do is to recognize that it's
    happening, and to understand that being a magnet for criticism is sometimes a sign not that someone is the wrong person
    for a job, but that they're the right one.
</p><br>
<br><br>
<span id="title">The Risk Of Discovery </span><br>
                <span id="secondText">January 2017</span>
                <br><br><br><br>   
<p>
    Because biographies of famous scientists tend to edit out their mistakes, we underestimate the degree of risk they were
    willing to take. And because anything a famous scientist did that wasn't a mistake has probably now become the
    conventional wisdom, those choices don't seem risky either.
</p><br>
<p>   
    Biographies of Newton, for example, understandably focus more on physics than alchemy or theology. The impression we get
    is that his unerring judgment led him straight to truths no one else had noticed. How to explain all the time he spent
    on alchemy and theology? Well, smart people are often kind of crazy.
</p><br>
<p>
    But maybe there is a simpler explanation. Maybe the smartness and the craziness were not as separate as we think.
    Physics seems to us a promising thing to work on, and alchemy and theology obvious wastes of time. But that's because we
    know how things turned out. In Newton's day the three problems seemed roughly equally promising. No one knew yet what
    the payoff would be for inventing what we now call physics; if they had, more people would have been working on it. And
    alchemy and theology were still then in the category Marc Andreessen would describe as "huge, if true."
</p><br>
<p>  
    Newton made three bets. One of them worked. But they were all risky.
</p><br>
<br><br>
<span id="title">How To Make Pittsburgh A Startup Hub</span><br>
                <span id="secondText">April 2016</span>
                <br><br><br><br>   
<p>
    (This is a talk I gave at an event called Opt412 in Pittsburgh. Much of it will apply to other towns. But not all,
    because as I say in the talk, Pittsburgh has some important advantages over most would-be startup hubs.)
</p><br>
<p>    
    What would it take to make Pittsburgh into a startup hub, like Silicon Valley? I understand Pittsburgh pretty well,
    because I grew up here, in Monroeville. And I understand Silicon Valley pretty well because that's where I live now.
    Could you get that kind of startup ecosystem going here?
</p><br>
<p>  
    When I agreed to speak here, I didn't think I'd be able to give a very optimistic talk. I thought I'd be talking about
    what Pittsburgh could do to become a startup hub, very much in the subjunctive. Instead I'm going to talk about what
    Pittsburgh can do.
</p><br>
<p>  
    What changed my mind was an article I read in, of all places, the New York Times food section. The title was
    "Pittsburgh's Youth-Driven Food Boom." To most people that might not even sound interesting, let alone something related
    to startups. But it was electrifying to me to read that title. I don't think I could pick a more promising one if I
    tried. And when I read the article I got even more excited. It said "people ages 25 to 29 now make up 7.6 percent of all
    residents, up from 7 percent about a decade ago." Wow, I thought, Pittsburgh could be the next Portland. It could become
    the cool place all the people in their twenties want to go live.
</p><br>
<p>  
    When I got here a couple days ago, I could feel the difference. I lived here from 1968 to 1984. I didn't realize it at
    the time, but during that whole period the city was in free fall. On top of the flight to the suburbs that happened
    everywhere, the steel and nuclear businesses were both dying. Boy are things different now. It's not just that downtown
    seems a lot more prosperous. There is an energy here that was not here when I was a kid.
</p><br>
<p>   
    When I was a kid, this was a place young people left. Now it's a place that attracts them.
</p><br>
<p>  
    What does that have to do with startups? Startups are made of people, and the average age of the people in a typical
    startup is right in that 25 to 29 bracket.
</p><br>
<p>  
    I've seen how powerful it is for a city to have those people. Five years ago they shifted the center of gravity of
    Silicon Valley from the peninsula to San Francisco. Google and Facebook are on the peninsula, but the next generation of
    big winners are all in SF. The reason the center of gravity shifted was the talent war, for programmers especially. Most
    25 to 29 year olds want to live in the city, not down in the boring suburbs. So whether they like it or not, founders
    know they have to be in the city. I know multiple founders who would have preferred to live down in the Valley proper,
    but who made themselves move to SF because they knew otherwise they'd lose the talent war.
</p><br>
<p>  
    So being a magnet for people in their twenties is a very promising thing to be. It's hard to imagine a place becoming a
    startup hub without also being that. When I read that statistic about the increasing percentage of 25 to 29 year olds, I
    had exactly the same feeling of excitement I get when I see a startup's graphs start to creep upward off the x axis.
</p><br>
<p>  
    Nationally the percentage of 25 to 29 year olds is 6.8%. That means you're .8% ahead. The population is 306,000, so
    we're talking about a surplus of about 2500 people. That's the population of a small town, and that's just the surplus.
    So you have a toehold. Now you just have to expand it.
</p><br>
<p>  
    And though "youth-driven food boom" may sound frivolous, it is anything but. Restaurants and cafes are a big part of the
    personality of a city. Imagine walking down a street in Paris. What are you walking past? Little restaurants and cafes.
    Imagine driving through some depressing random exurb. What are you driving past? Starbucks and McDonalds and Pizza Hut.
    As Gertrude Stein said, there is no there there. You could be anywhere.
</p><br>
<p>  
    These independent restaurants and cafes are not just feeding people. They're making there be a there here.
</p><br>
<p>   
    So here is my first concrete recommendation for turning Pittsburgh into the next Silicon Valley: do everything you can
    to encourage this youth-driven food boom. What could the city do? Treat the people starting these little restaurants and
    cafes as your users, and go ask them what they want. I can guess at least one thing they might want: a fast permit
    process. San Francisco has left you a huge amount of room to beat them in that department.
</p><br>
<p>   
    I know restaurants aren't the prime mover though. The prime mover, as the Times article said, is cheap housing. That's a
    big advantage. But that phrase "cheap housing" is a bit misleading. There are plenty of places that are cheaper. What's
    special about Pittsburgh is not that it's cheap, but that it's a cheap place you'd actually want to live.
</p><br>
<p>   
    Part of that is the buildings themselves. I realized a long time ago, back when I was a poor twenty-something myself,
    that the best deals were places that had once been rich, and then became poor. If a place has always been rich, it's
    nice but too expensive. If a place has always been poor, it's cheap but grim. But if a place was once rich and then got
    poor, you can find palaces for cheap. And that's what's bringing people here. When Pittsburgh was rich, a hundred years
    ago, the people who lived here built big solid buildings. Not always in the best taste, but definitely solid. So here is
    another piece of advice for becoming a startup hub: don't destroy the buildings that are bringing people here. When
    cities are on the way back up, like Pittsburgh is now, developers race to tear down the old buildings. Don't let that
    happen. Focus on historic preservation. Big real estate development projects are not what's bringing the
    twenty-somethings here. They're the opposite of the new restaurants and cafes; they subtract personality from the city.
</p><br>
<p>   
    The empirical evidence suggests you cannot be too strict about historic preservation. The tougher cities are about it,
    the better they seem to do.
</p><br>
<p>   
    But the appeal of Pittsburgh is not just the buildings themselves. It's the neighborhoods they're in. Like San Francisco
    and New York, Pittsburgh is fortunate in being a pre-car city. It's not too spread out. Because those 25 to 29 year olds
    do not like driving. They prefer walking, or bicycling, or taking public transport. If you've been to San Francisco
    recently you can't help noticing the huge number of bicyclists. And this is not just a fad that the twenty-somethings
    have adopted. In this respect they have discovered a better way to live. The beards will go, but not the bikes. Cities
    where you can get around without driving are just better period. So I would suggest you do everything you can to
    capitalize on this. As with historic preservation, it seems impossible to go too far.
</p><br>
<p>   
    Why not make Pittsburgh the most bicycle and pedestrian friendly city in the country? See if you can go so far that you
    make San Francisco seem backward by comparison. If you do, it's very unlikely you'll regret it. The city will seem like
    a paradise to the young people you want to attract. If they do leave to get jobs elsewhere, it will be with regret at
    leaving behind such a place. And what's the downside? Can you imagine a headline "City ruined by becoming too
    bicycle-friendly?" It just doesn't happen.
</p><br>
<p>   
    So suppose cool old neighborhoods and cool little restaurants make this the next Portland. Will that be enough? It will
    put you in a way better position than Portland itself, because Pittsburgh has something Portland lacks: a first-rate
    research university. CMU plus little cafes means you have more than hipsters drinking lattes. It means you have hipsters
    drinking lattes while talking about distributed systems. Now you're getting really close to San Francisco.
</p><br>
<p>   
    In fact you're better off than San Francisco in one way, because CMU is downtown, but Stanford and Berkeley are out in
    the suburbs.
</p><br>
<p>   
    What can CMU do to help Pittsburgh become a startup hub? Be an even better research university. CMU is one of the best
    universities in the world, but imagine what things would be like if it were the very best, and everyone knew it. There
    are a lot of ambitious people who must go to the best place, wherever it is. If CMU were it, they would all come here.
    There would be kids in Kazakhstan dreaming of one day living in Pittsburgh.
</p><br>
<p>  
    Being that kind of talent magnet is the most important contribution universities can make toward making their city a
    startup hub. In fact it is practically the only contribution they can make.
</p><br>
<p>   
    But wait, shouldn't universities be setting up programs with words like "innovation" and "entrepreneurship" in their
    names? No, they should not. These kind of things almost always turn out to be disappointments. They're pursuing the
    wrong targets. The way to get innovation is not to aim for innovation but to aim for something more specific, like
    better batteries or better 3D printing. And the way to learn about entrepreneurship is to do it, which you can't in
    school.
</p><br>
<p>   
    I know it may disappoint some administrators to hear that the best thing a university can do to encourage startups is to
    be a great university. It's like telling people who want to lose weight that the way to do it is to eat less.
</p><br>
<p>   
    But if you want to know where startups come from, look at the empirical evidence. Look at the histories of the most
    successful startups, and you'll find they grow organically out of a couple of founders building something that starts as
    an interesting side project. Universities are great at bringing together founders, but beyond that the best thing they
    can do is get out of the way. For example, by not claiming ownership of "intellectual property" that students and
    faculty develop, and by having liberal rules about deferred admission and leaves of absence.
</p><br>
<p>   
    In fact, one of the most effective things a university could do to encourage startups is an elaborate form of getting
    out of the way invented by Harvard. Harvard used to have exams for the fall semester after Christmas. At the beginning
    of January they had something called "Reading Period" when you were supposed to be studying for exams. And Microsoft and
    Facebook have something in common that few people realize: they were both started during Reading Period. It's the
    perfect situation for producing the sort of side projects that turn into startups. The students are all on campus, but
    they don't have to do anything because they're supposed to be studying for exams.
</p><br>
<p>   
    Harvard may have closed this window, because a few years ago they moved exams before Christmas and shortened reading
    period from 11 days to 7. But if a university really wanted to help its students start startups, the empirical evidence,
    weighted by market cap, suggests the best thing they can do is literally nothing.
</p><br>
<p>   
    The culture of Pittsburgh is another of its strengths. It seems like a city has to be socially liberal to be a startup
    hub, and it's pretty clear why. A city has to tolerate strangeness to be a home for startups, because startups are so
    strange. And you can't choose to allow just the forms of strangeness that will turn into big startups, because they're
    all intermingled. You have to tolerate all strangeness.
</p><br>
<p>   
    That immediately rules out big chunks of the US. I'm optimistic it doesn't rule out Pittsburgh. One of the things I
    remember from growing up here, though I didn't realize at the time that there was anything unusual about it, is how well
    people got along. I'm still not sure why. Maybe one reason was that everyone felt like an immigrant. When I was a kid in
    Monroeville, people didn't call themselves American. They called themselves Italian or Serbian or Ukranian. Just imagine
    what it must have been like here a hundred years ago, when people were pouring in from twenty different countries.
    Tolerance was the only option.
</p><br>
<p>   
    What I remember about the culture of Pittsburgh is that it was both tolerant and pragmatic. That's how I'd describe the
    culture of Silicon Valley too. And it's not a coincidence, because Pittsburgh was the Silicon Valley of its time. This
    was a city where people built new things. And while the things people build have changed, the spirit you need to do that
    kind of work is the same.
</p><br>
<p>   
    So although an influx of latte-swilling hipsters may be annoying in some ways, I would go out of my way to encourage
    them. And more generally to tolerate strangeness, even unto the degree wacko Californians do. For Pittsburgh that is a
    conservative choice: it's a return to the city's roots.
</p><br>
<p>   
    Unfortunately I saved the toughest part for last. There is one more thing you need to be a startup hub, and Pittsburgh
    hasn't got it: investors. Silicon Valley has a big investor community because it's had 50 years to grow one. New York
    has a big investor community because it's full of people who like money a lot and are quick to notice new ways to get
    it. But Pittsburgh has neither of these. And the cheap housing that draws other people here has no effect on investors.
</p><br>
<p>   
    If an investor community grows up here, it will happen the same way it did in Silicon Valley: slowly and organically. So
    I would not bet on having a big investor community in the short term. But fortunately there are three trends that make
    that less necessary than it used to be. One is that startups are increasingly cheap to start, so you just don't need as
    much outside money as you used to. The second is that thanks to things like Kickstarter, a startup can get to revenue
    faster. You can put something on Kickstarter from anywhere. The third is programs like Y Combinator. A startup from
    anywhere in the world can go to YC for 3 months, pick up funding, and then return home if they want.
</p><br>
<p>   
    My advice is to make Pittsburgh a great place for startups, and gradually more of them will stick. Some of those will
    succeed; some of their founders will become investors; and still more startups will stick.
</p><br>
<p>   
    This is not a fast path to becoming a startup hub. But it is at least a path, which is something few other cities have.
    And it's not as if you have to make painful sacrifices in the meantime. Think about what I've suggested you should do.
    Encourage local restaurants, save old buildings, take advantage of density, make CMU the best, promote tolerance. These
    are the things that make Pittsburgh good to live in now. All I'm saying is that you should do even more of them.
</p><br>
<p>   
    And that's an encouraging thought. If Pittsburgh's path to becoming a startup hub is to be even more itself, then it has
    a good chance of succeeding. In fact it probably has the best chance of any city its size. It will take some effort, and
    a lot of time, but if any city can do it, Pittsburgh can.
</p><br>
<p>   
    Thanks to Charlie Cheever and Jessica Livingston for reading drafts of this, and to Meg Cheever for organizing Opt412
    and inviting me to speak.
</p><br>
<br><br>
<span id="title">Life Is Short</span><br>
                <span id="secondText">January 2016</span>
                <br><br><br><br>   
<p>
    Life is short, as everyone knows. When I was a kid I used to wonder about this. Is life actually short, or are we really
    complaining about its finiteness? Would we be just as likely to feel life was short if we lived 10 times as long?
</p><br>
<p>   
    Since there didn't seem any way to answer this question, I stopped wondering about it. Then I had kids. That gave me a
    way to answer the question, and the answer is that life actually is short.
</p><br>
<p>   
    Having kids showed me how to convert a continuous quantity, time, into discrete quantities. You only get 52 weekends
    with your 2 year old. If Christmas-as-magic lasts from say ages 3 to 10, you only get to watch your child experience it
    8 times. And while it's impossible to say what is a lot or a little of a continuous quantity like time, 8 is not a lot
    of something. If you had a handful of 8 peanuts, or a shelf of 8 books to choose from, the quantity would definitely
    seem limited, no matter what your lifespan was.
</p><br>
<p>   
    Ok, so life actually is short. Does it make any difference to know that?
</p><br>
<p>    
    It has for me. It means arguments of the form "Life is too short for x" have great force. It's not just a figure of
    speech to say that life is too short for something. It's not just a synonym for annoying. If you find yourself thinking
    that life is too short for something, you should try to eliminate it if you can.
</p><br>
<p>   
    When I ask myself what I've found life is too short for, the word that pops into my head is "bullshit." I realize that
    answer is somewhat tautological. It's almost the definition of bullshit that it's the stuff that life is too short for.
    And yet bullshit does have a distinctive character. There's something fake about it. It's the junk food of experience.
    [1]
</p><br>
<p>    
    If you ask yourself what you spend your time on that's bullshit, you probably already know the answer. Unnecessary
    meetings, pointless disputes, bureaucracy, posturing, dealing with other people's mistakes, traffic jams, addictive but
    unrewarding pastimes.
</p><br>
<p>    
    There are two ways this kind of thing gets into your life: it's either forced on you, or it tricks you. To some extent
    you have to put up with the bullshit forced on you by circumstances. You need to make money, and making money consists
    mostly of errands. Indeed, the law of supply and demand insures that: the more rewarding some kind of work is, the
    cheaper people will do it. It may be that less bullshit is forced on you than you think, though. There has always been a
    stream of people who opt out of the default grind and go live somewhere where opportunities are fewer in the
    conventional sense, but life feels more authentic. This could become more common.
</p><br>
<p>    
    You can do it on a smaller scale without moving. The amount of time you have to spend on bullshit varies between
    employers. Most large organizations (and many small ones) are steeped in it. But if you consciously prioritize bullshit
    avoidance over other factors like money and prestige, you can probably find employers that will waste less of your time.
</p><br>
<p>    
    If you're a freelancer or a small company, you can do this at the level of individual customers. If you fire or avoid
    toxic customers, you can decrease the amount of bullshit in your life by more than you decrease your income.
</p><br>
<p>    
    But while some amount of bullshit is inevitably forced on you, the bullshit that sneaks into your life by tricking you
    is no one's fault but your own. And yet the bullshit you choose may be harder to eliminate than the bullshit that's
    forced on you. Things that lure you into wasting your time have to be really good at tricking you. An example that will
    be familiar to a lot of people is arguing online. When someone contradicts you, they're in a sense attacking you.
    Sometimes pretty overtly. Your instinct when attacked is to defend yourself. But like a lot of instincts, this one
    wasn't designed for the world we now live in. Counterintuitive as it feels, it's better most of the time not to defend
    yourself. Otherwise these people are literally taking your life. [2]
</p><br>
<p>     
    Arguing online is only incidentally addictive. There are more dangerous things than that. As I've written before, one
    byproduct of technical progress is that things we like tend to become more addictive. Which means we will increasingly
    have to make a conscious effort to avoid addictions — to stand outside ourselves and ask "is this how I want to be
    spending my time?"
</p><br>
<p>    
    As well as avoiding bullshit, one should actively seek out things that matter. But different things matter to different
    people, and most have to learn what matters to them. A few are lucky and realize early on that they love math or taking
    care of animals or writing, and then figure out a way to spend a lot of time doing it. But most people start out with a
    life that's a mix of things that matter and things that don't, and only gradually learn to distinguish between them.
</p><br>
<p>   
    For the young especially, much of this confusion is induced by the artificial situations they find themselves in. In
    middle school and high school, what the other kids think of you seems the most important thing in the world. But when
    you ask adults what they got wrong at that age, nearly all say they cared too much what other kids thought of them.
</p><br>
<p>   
    One heuristic for distinguishing stuff that matters is to ask yourself whether you'll care about it in the future. Fake
    stuff that matters usually has a sharp peak of seeming to matter. That's how it tricks you. The area under the curve is
    small, but its shape jabs into your consciousness like a pin.
</p><br>
<p>   
    The things that matter aren't necessarily the ones people would call "important." Having coffee with a friend matters.
    You won't feel later like that was a waste of time.
</p><br>
<p>     
    One great thing about having small children is that they make you spend time on things that matter: them. They grab your
    sleeve as you're staring at your phone and say "will you play with me?" And odds are that is in fact the
    bullshit-minimizing option.
</p><br>
<p>   
    If life is short, we should expect its shortness to take us by surprise. And that is just what tends to happen. You take
    things for granted, and then they're gone. You think you can always write that book, or climb that mountain, or
    whatever, and then you realize the window has closed. The saddest windows close when other people die. Their lives are
    short too. After my mother died, I wished I'd spent more time with her. I lived as if she'd always be there. And in her
    typical quiet way she encouraged that illusion. But an illusion it was. I think a lot of people make the same mistake I
    did.
</p><br>
<p>   
    The usual way to avoid being taken by surprise by something is to be consciously aware of it. Back when life was more
    precarious, people used to be aware of death to a degree that would now seem a bit morbid. I'm not sure why, but it
    doesn't seem the right answer to be constantly reminding oneself of the grim reaper hovering at everyone's shoulder.
    Perhaps a better solution is to look at the problem from the other end. Cultivate a habit of impatience about the things
    you most want to do. Don't wait before climbing that mountain or writing that book or visiting your mother. You don't
    need to be constantly reminding yourself why you shouldn't wait. Just don't wait.
</p><br>
<p>   
    I can think of two more things one does when one doesn't have much of something: try to get more of it, and savor what
    one has. Both make sense here.
</p><br>
<p>   
    How you live affects how long you live. Most people could do better. Me among them.
</p><br>
<p>    
    But you can probably get even more effect by paying closer attention to the time you have. It's easy to let the days
    rush by. The "flow" that imaginative people love so much has a darker cousin that prevents you from pausing to savor
    life amid the daily slurry of errands and alarms. One of the most striking things I've read was not in a book, but the
    title of one: James Salter's Burning the Days.
</p><br>
<p>    
    It is possible to slow time somewhat. I've gotten better at it. Kids help. When you have small children, there are a lot
    of moments so perfect that you can't help noticing.
</p><br>
<p>   
    It does help too to feel that you've squeezed everything out of some experience. The reason I'm sad about my mother is
    not just that I miss her but that I think of all the things we could have done that we didn't. My oldest son will be 7
    soon. And while I miss the 3 year old version of him, I at least don't have any regrets over what might have been. We
    had the best time a daddy and a 3 year old ever had.
</p><br>
<p>   
    Relentlessly prune bullshit, don't wait to do things that matter, and savor the time you have. That's what you do when
    life is short.
</p><br>
<p>    
    Notes
</p><br>
<p>    
    [1] At first I didn't like it that the word that came to mind was one that had other meanings. But then I realized the
    other meanings are fairly closely related. Bullshit in the sense of things you waste your time on is a lot like
    intellectual bullshit.
</p><br>
<p>   
    [2] I chose this example deliberately as a note to self. I get attacked a lot online. People tell the craziest lies
    about me. And I have so far done a pretty mediocre job of suppressing the natural human inclination to say "Hey, that's
    not true!"
</p><br>
<p>    
    Thanks to Jessica Livingston and Geoff Ralston for reading drafts of this.
</p><br>
<br><br>
<span id="title">Economic Inequality</span><br>
                <span id="secondText">January 2016</span>
                <br><br><br><br>   
<p>
    Since the 1970s, economic inequality in the US has increased dramatically. And in particular, the rich have gotten a lot
    richer. Nearly everyone who writes about the topic says that economic inequality should be decreased.
</p><br>
<p>    
    I'm interested in this question because I was one of the founders of a company called Y Combinator that helps people
    start startups. Almost by definition, if a startup succeeds, its founders become rich. Which means by helping startup
    founders I've been helping to increase economic inequality. If economic inequality should be decreased, I shouldn't be
    helping founders. No one should be.
</p><br>
<p>    
    But that doesn't sound right. What's going on here? What's going on is that while economic inequality is a single
    measure (or more precisely, two: variation in income, and variation in wealth), it has multiple causes. Many of these
    causes are bad, like tax loopholes and drug addiction. But some are good, like Larry Page and Sergey Brin starting the
    company you use to find things online.
</p><br>
<p>    
    If you want to understand economic inequality — and more importantly, if you actually want to fix the bad aspects of it
    — you have to tease apart the components. And yet the trend in nearly everything written about the subject is to do the
    opposite: to squash together all the aspects of economic inequality as if it were a single phenomenon.
</p><br>
<p>    
    Sometimes this is done for ideological reasons. Sometimes it's because the writer only has very high-level data and so
    draws conclusions from that, like the proverbial drunk who looks for his keys under the lamppost, instead of where he
    dropped them, because the light is better there. Sometimes it's because the writer doesn't understand critical aspects
    of inequality, like the role of technology in wealth creation. Much of the time, perhaps most of the time, writing about
    economic inequality combines all three.
</p><br>
<p>    
    ___
</p><br>
<p>    
    
    The most common mistake people make about economic inequality is to treat it as a single phenomenon. The most naive
    version of which is the one based on the pie fallacy: that the rich get rich by taking money from the poor.
</p><br>
<p>    
    Usually this is an assumption people start from rather than a conclusion they arrive at by examining the evidence.
    Sometimes the pie fallacy is stated explicitly:
    ...those at the top are grabbing an increasing fraction of the nation's income — so much of a larger share that what's
    left over for the rest is diminished.... [1]
    Other times it's more unconscious. But the unconscious form is very widespread. I think because we grow up in a world
    where the pie fallacy is actually true. To kids, wealth is a fixed pie that's shared out, and if one person gets more,
    it's at the expense of another. It takes a conscious effort to remind oneself that the real world doesn't work that way.
</p><br>
<p>    
    In the real world you can create wealth as well as taking it from others. A woodworker creates wealth. He makes a chair,
    and you willingly give him money in return for it. A high-frequency trader does not. He makes a dollar only when someone
    on the other end of a trade loses a dollar.
</p><br>
<p>    
    If the rich people in a society got that way by taking wealth from the poor, then you have the degenerate case of
    economic inequality, where the cause of poverty is the same as the cause of wealth. But instances of inequality don't
    have to be instances of the degenerate case. If one woodworker makes 5 chairs and another makes none, the second
    woodworker will have less money, but not because anyone took anything from him.
</p><br>
<p>    
    Even people sophisticated enough to know about the pie fallacy are led toward it by the custom of describing economic
    inequality as a ratio of one quantile's income or wealth to another's. It's so easy to slip from talking about income
    shifting from one quantile to another, as a figure of speech, into believing that is literally what's happening.
</p><br>
<p>    
    Except in the degenerate case, economic inequality can't be described by a ratio or even a curve. In the general case it
    consists of multiple ways people become poor, and multiple ways people become rich. Which means to understand economic
    inequality in a country, you have to go find individual people who are poor or rich and figure out why. [2]
</p><br>
<p>    
    If you want to understand change in economic inequality, you should ask what those people would have done when it was
    different. This is one way I know the rich aren't all getting richer simply from some new system for transferring wealth
    to them from everyone else. When you use the would-have method with startup founders, you find what most would have done
    back in 1960, when economic inequality was lower, was to join big companies or become professors. Before Mark Zuckerberg
    started Facebook, his default expectation was that he'd end up working at Microsoft. The reason he and most other
    startup founders are richer than they would have been in the mid 20th century is not because of some right turn the
    country took during the Reagan administration, but because progress in technology has made it much easier to start a new
    company that grows fast.
</p><br>
<p>    
    Traditional economists seem strangely averse to studying individual humans. It seems to be a rule with them that
    everything has to start with statistics. So they give you very precise numbers about variation in wealth and income,
    then follow it with the most naive speculation about the underlying causes.
</p><br>
<p>    
    But while there are a lot of people who get rich through rent-seeking of various forms, and a lot who get rich by
    playing zero-sum games, there are also a significant number who get rich by creating wealth. And creating wealth, as a
    source of economic inequality, is different from taking it — not just morally, but also practically, in the sense that
    it is harder to eradicate. One reason is that variation in productivity is accelerating. The rate at which individuals
    can create wealth depends on the technology available to them, and that grows exponentially. The other reason creating
    wealth is such a tenacious source of inequality is that it can expand to accommodate a lot of people.
</p><br>
<p>   
    ___
</p><br>
<p>     
    
    I'm all for shutting down the crooked ways to get rich. But that won't eliminate great variations in wealth, because as
    long as you leave open the option of getting rich by creating wealth, people who want to get rich will do that instead.
</p><br>
<p>    
    Most people who get rich tend to be fairly driven. Whatever their other flaws, laziness is usually not one of them.
    Suppose new policies make it hard to make a fortune in finance. Does it seem plausible that the people who currently go
    into finance to make their fortunes will continue to do so, but be content to work for ordinary salaries? The reason
    they go into finance is not because they love finance but because they want to get rich. If the only way left to get
    rich is to start startups, they'll start startups. They'll do well at it too, because determination is the main factor
    in the success of a startup. [3] And while it would probably be a good thing for the world if people who wanted to get
    rich switched from playing zero-sum games to creating wealth, that would not only not eliminate great variations in
    wealth, but might even exacerbate them. In a zero-sum game there is at least a limit to the upside. Plus a lot of the
    new startups would create new technology that further accelerated variation in productivity.
</p><br>
<p>    
    Variation in productivity is far from the only source of economic inequality, but it is the irreducible core of it, in
    the sense that you'll have that left when you eliminate all other sources. And if you do, that core will be big, because
    it will have expanded to include the efforts of all the refugees. Plus it will have a large Baumol penumbra around it:
    anyone who could get rich by creating wealth on their own account will have to be paid enough to prevent them from doing
    it.
</p><br>
<p>    
    You can't prevent great variations in wealth without preventing people from getting rich, and you can't do that without
    preventing them from starting startups.
</p><br>
<p>    
    So let's be clear about that. Eliminating great variations in wealth would mean eliminating startups. And that doesn't
    seem a wise move. Especially since it would only mean you eliminated startups in your own country. Ambitious people
    already move halfway around the world to further their careers, and startups can operate from anywhere nowadays. So if
    you made it impossible to get rich by creating wealth in your country, people who wanted to do that would just leave and
    do it somewhere else. Which would certainly get you a lower Gini coefficient, along with a lesson in being careful what
    you ask for. [4]
</p><br>
<p>    
    I think rising economic inequality is the inevitable fate of countries that don't choose something worse. We had a 40
    year stretch in the middle of the 20th century that convinced some people otherwise. But as I explained in The
    Refragmentation, that was an anomaly — a unique combination of circumstances that compressed American society not just
    economically but culturally too. [5]
</p><br>
<p>    
    And while some of the growth in economic inequality we've seen since then has been due to bad behavior of various kinds,
    there has simultaneously been a huge increase in individuals' ability to create wealth. Startups are almost entirely a
    product of this period. And even within the startup world, there has been a qualitative change in the last 10 years.
    Technology has decreased the cost of starting a startup so much that founders now have the upper hand over investors.
    Founders get less diluted, and it is now common for them to retain board control as well. Both further increase economic
    inequality, the former because founders own more stock, and the latter because, as investors have learned, founders tend
    to be better at running their companies than investors.
</p><br>
<p>    
    While the surface manifestations change, the underlying forces are very, very old. The acceleration of productivity we
    see in Silicon Valley has been happening for thousands of years. If you look at the history of stone tools, technology
    was already accelerating in the Mesolithic. The acceleration would have been too slow to perceive in one lifetime. Such
    is the nature of the leftmost part of an exponential curve. But it was the same curve.
</p><br>
<p>    
    You do not want to design your society in a way that's incompatible with this curve. The evolution of technology is one
    of the most powerful forces in history.
</p><br>
<p>    
    Louis Brandeis said "We may have democracy, or we may have wealth concentrated in the hands of a few, but we can't have
    both." That sounds plausible. But if I have to choose between ignoring him and ignoring an exponential curve that has
    been operating for thousands of years, I'll bet on the curve. Ignoring any trend that has been operating for thousands
    of years is dangerous. But exponential growth, especially, tends to bite you.
</p><br>
<p>   
    ___
</p><br>
<p>    
    
    If accelerating variation in productivity is always going to produce some baseline growth in economic inequality, it
    would be a good idea to spend some time thinking about that future. Can you have a healthy society with great variation
    in wealth? What would it look like?
</p><br>
<p>    
    Notice how novel it feels to think about that. The public conversation so far has been exclusively about the need to
    decrease economic inequality. We've barely given a thought to how to live with it.
</p><br>
<p>   
    I'm hopeful we'll be able to. Brandeis was a product of the Gilded Age, and things have changed since then. It's harder
    to hide wrongdoing now. And to get rich now you don't have to buy politicians the way railroad or oil magnates did. [6]
    The great concentrations of wealth I see around me in Silicon Valley don't seem to be destroying democracy.
</p><br>
<p>    
    There are lots of things wrong with the US that have economic inequality as a symptom. We should fix those things. In
    the process we may decrease economic inequality. But we can't start from the symptom and hope to fix the underlying
    causes. [7]
</p><br>
<p>    
    The most obvious is poverty. I'm sure most of those who want to decrease economic inequality want to do it mainly to
    help the poor, not to hurt the rich. [8] Indeed, a good number are merely being sloppy by speaking of decreasing
    economic inequality when what they mean is decreasing poverty. But this is a situation where it would be good to be
    precise about what we want. Poverty and economic inequality are not identical. When the city is turning off your water
    because you can't pay the bill, it doesn't make any difference what Larry Page's net worth is compared to yours. He
    might only be a few times richer than you, and it would still be just as much of a problem that your water was getting
    turned off.
</p><br>
<p>    
    Closely related to poverty is lack of social mobility. I've seen this myself: you don't have to grow up rich or even
    upper middle class to get rich as a startup founder, but few successful founders grew up desperately poor. But again,
    the problem here is not simply economic inequality. There is an enormous difference in wealth between the household
    Larry Page grew up in and that of a successful startup founder, but that didn't prevent him from joining their ranks.
    It's not economic inequality per se that's blocking social mobility, but some specific combination of things that go
    wrong when kids grow up sufficiently poor.
</p><br>
<p>    
    One of the most important principles in Silicon Valley is that "you make what you measure." It means that if you pick
    some number to focus on, it will tend to improve, but that you have to choose the right number, because only the one you
    choose will improve; another that seems conceptually adjacent might not. For example, if you're a university president
    and you decide to focus on graduation rates, then you'll improve graduation rates. But only graduation rates, not how
    much students learn. Students could learn less, if to improve graduation rates you made classes easier.
</p><br>
<p>    
    Economic inequality is sufficiently far from identical with the various problems that have it as a symptom that we'll
    probably only hit whichever of the two we aim at. If we aim at economic inequality, we won't fix these problems. So I
    say let's aim at the problems.
</p><br>
<p>    
    For example, let's attack poverty, and if necessary damage wealth in the process. That's much more likely to work than
    attacking wealth in the hope that you will thereby fix poverty. [9] And if there are people getting rich by tricking
    consumers or lobbying the government for anti-competitive regulations or tax loopholes, then let's stop them. Not
    because it's causing economic inequality, but because it's stealing. [10]
</p><br>
<p>    
    If all you have is statistics, it seems like that's what you need to fix. But behind a broad statistical measure like
    economic inequality there are some things that are good and some that are bad, some that are historical trends with
    immense momentum and others that are random accidents. If we want to fix the world behind the statistics, we have to
    understand it, and focus our efforts where they'll do the most good.
</p><br>
<p>    Notes
</p><br>
<p>    
    [1] Stiglitz, Joseph. The Price of Inequality. Norton, 2012. p. 32.
</p><br>
<p>    
    [2] Particularly since economic inequality is a matter of outliers, and outliers are disproportionately likely to have
    gotten where they are by ways that have little do with the sort of things economists usually think about, like wages and
    productivity, but rather by, say, ending up on the wrong side of the "War on Drugs."
</p><br>
<p>    
    [3] Determination is the most important factor in deciding between success and failure, which in startups tend to be
    sharply differentiated. But it takes more than determination to create one of the hugely successful startups. Though
    most founders start out excited about the idea of getting rich, purely mercenary founders will usually take one of the
    big acquisition offers most successful startups get on the way up. The founders who go on to the next stage tend to be
    driven by a sense of mission. They have the same attachment to their companies that an artist or writer has to their
    work. But it is very hard to predict at the outset which founders will do that. It's not simply a function of their
    initial attitude. Starting a company changes people.
</p><br>
<p>    
    [4] After reading a draft of this essay, Richard Florida told me how he had once talked to a group of Europeans "who
    said they wanted to make Europe more entrepreneurial and more like Silicon Valley. I said by definition this will give
    you more inequality. They thought I was insane — they could not process it."
</p><br>
<p>    
    [5] Economic inequality has been decreasing globally. But this is mainly due to the erosion of the kleptocracies that
    formerly dominated all the poorer countries. Once the playing field is leveler politically, we'll see economic
    inequality start to rise again. The US is the bellwether. The situation we face here, the rest of the world will sooner
    or later.
</p><br>
<p>    
    [6] Some people still get rich by buying politicians. My point is that it's no longer a precondition.
</p><br>
<p>    
    [7] As well as problems that have economic inequality as a symptom, there are those that have it as a cause. But in most
    if not all, economic inequality is not the primary cause. There is usually some injustice that is allowing economic
    inequality to turn into other forms of inequality, and that injustice is what we need to fix. For example, the police in
    the US treat the poor worse than the rich. But the solution is not to make people richer. It's to make the police treat
    people more equitably. Otherwise they'll continue to maltreat people who are weak in other ways.
</p><br>
<p>    
    [8] Some who read this essay will say that I'm clueless or even being deliberately misleading by focusing so much on the
    richer end of economic inequality — that economic inequality is really about poverty. But that is exactly the point I'm
    making, though sloppier language than I'd use to make it. The real problem is poverty, not economic inequality. And if
    you conflate them you're aiming at the wrong target.
</p><br>
<p>   
    Others will say I'm clueless or being misleading by focusing on people who get rich by creating wealth — that startups
    aren't the problem, but corrupt practices in finance, healthcare, and so on. Once again, that is exactly my point. The
    problem is not economic inequality, but those specific abuses.
</p><br>
<p>   
    It's a strange task to write an essay about why something isn't the problem, but that's the situation you find yourself
    in when so many people mistakenly think it is.
</p><br>
<p>   
    [9] Particularly since many causes of poverty are only partially driven by people trying to make money from them. For
    example, America's abnormally high incarceration rate is a major cause of poverty. But although for-profit prison
    companies and prison guard unions both spend a lot lobbying for harsh sentencing laws, they are not the original source
    of them.
</p><br>
<p>   
    [10] Incidentally, tax loopholes are definitely not a product of some power shift due to recent increases in economic
    inequality. The golden age of economic equality in the mid 20th century was also the golden age of tax avoidance.
    Indeed, it was so widespread and so effective that I'm skeptical whether economic inequality was really so low then as
    we think. In a period when people are trying to hide wealth from the government, it will tend to be hidden from
    statistics too. One sign of the potential magnitude of the problem is the discrepancy between government receipts as a
    percentage of GDP, which have remained more or less constant during the entire period from the end of World War II to
    the present, and tax rates, which have varied dramatically.
</p><br>
<p>    
    Thanks to Sam Altman, Tiffani Ashley Bell, Patrick Collison, Ron Conway, Richard Florida, Ben Horowitz, Jessica
    Livingston, Robert Morris, Tim O'Reilly, Max Roser, and Alexia Tsotsis for reading drafts of this.
</p><br>
<p>    
    Note: This is a new version from which I removed a pair of metaphors that made a lot of people mad, essentially by
    macroexpanding them. If anyone wants to see the old version, I put it here.
</p><br>

<br><br>
<span id="title">The Refragmentation</span><br>
                <span id="secondText">January 2016</span>
                <br><br><br><br>   
<p>
    One advantage of being old is that you can see change happen in your lifetime. A lot of the change I've seen is
    fragmentation. US politics is much more polarized than it used to be. Culturally we have ever less common ground. The
    creative class flocks to a handful of happy cities, abandoning the rest. And increasing economic inequality means the
    spread between rich and poor is growing too. I'd like to propose a hypothesis: that all these trends are instances of
    the same phenomenon. And moreover, that the cause is not some force that's pulling us apart, but rather the erosion of
    forces that had been pushing us together.
</p><br>
<p>   
    Worse still, for those who worry about these trends, the forces that were pushing us together were an anomaly, a
    one-time combination of circumstances that's unlikely to be repeated — and indeed, that we would not want to repeat.
</p><br>
<p>   
    The two forces were war (above all World War II), and the rise of large corporations.
</p><br>
<p>    
    The effects of World War II were both economic and social. Economically, it decreased variation in income. Like all
    modern armed forces, America's were socialist economically. From each according to his ability, to each according to his
    need. More or less. Higher ranking members of the military got more (as higher ranking members of socialist societies
    always do), but what they got was fixed according to their rank. And the flattening effect wasn't limited to those under
    arms, because the US economy was conscripted too. Between 1942 and 1945 all wages were set by the National War Labor
    Board. Like the military, they defaulted to flatness. And this national standardization of wages was so pervasive that
    its effects could still be seen years after the war ended. [1]
</p><br>
<p>    
    Business owners weren't supposed to be making money either. FDR said "not a single war millionaire" would be permitted.
    To ensure that, any increase in a company's profits over prewar levels was taxed at 85%. And when what was left after
    corporate taxes reached individuals, it was taxed again at a marginal rate of 93%. [2]
</p><br>
<p>    
    Socially too the war tended to decrease variation. Over 16 million men and women from all sorts of different backgrounds
    were brought together in a way of life that was literally uniform. Service rates for men born in the early 1920s
    approached 80%. And working toward a common goal, often under stress, brought them still closer together.
</p><br>
<p>    
    Though strictly speaking World War II lasted less than 4 years for the US, its effects lasted longer. Wars make central
    governments more powerful, and World War II was an extreme case of this. In the US, as in all the other Allied
    countries, the federal government was slow to give up the new powers it had acquired. Indeed, in some respects the war
    didn't end in 1945; the enemy just switched to the Soviet Union. In tax rates, federal power, defense spending,
    conscription, and nationalism, the decades after the war looked more like wartime than prewar peacetime. [3] And the
    social effects lasted too. The kid pulled into the army from behind a mule team in West Virginia didn't simply go back
    to the farm afterward. Something else was waiting for him, something that looked a lot like the army.
</p><br>
<p>    
    If total war was the big political story of the 20th century, the big economic story was the rise of a new kind of
    company. And this too tended to produce both social and economic cohesion. [4]
</p><br>
<p>     
    The 20th century was the century of the big, national corporation. General Electric, General Foods, General Motors.
    Developments in finance, communications, transportation, and manufacturing enabled a new type of company whose goal was
    above all scale. Version 1 of this world was low-res: a Duplo world of a few giant companies dominating each big market.
    [5]
</p><br>
<p>   
    The late 19th and early 20th centuries had been a time of consolidation, led especially by J. P. Morgan. Thousands of
    companies run by their founders were merged into a couple hundred giant ones run by professional managers. Economies of
    scale ruled the day. It seemed to people at the time that this was the final state of things. John D. Rockefeller said
    in 1880
    The day of combination is here to stay. Individualism has gone, never to return.
    He turned out to be mistaken, but he seemed right for the next hundred years.
</p><br>
<p>   
    The consolidation that began in the late 19th century continued for most of the 20th. By the end of World War II, as
    Michael Lind writes, "the major sectors of the economy were either organized as government-backed cartels or dominated
    by a few oligopolistic corporations."
</p><br>
<p>    
    For consumers this new world meant the same choices everywhere, but only a few of them. When I grew up there were only 2
    or 3 of most things, and since they were all aiming at the middle of the market there wasn't much to differentiate them.
</p><br>
<p>   
    One of the most important instances of this phenomenon was in TV. Here there were 3 choices: NBC, CBS, and ABC. Plus
    public TV for eggheads and communists. The programs that the 3 networks offered were indistinguishable. In fact, here
    there was a triple pressure toward the center. If one show did try something daring, local affiliates in conservative
    markets would make them stop. Plus since TVs were expensive, whole families watched the same shows together, so they had
    to be suitable for everyone.
</p><br>
<p>    
    And not only did everyone get the same thing, they got it at the same time. It's difficult to imagine now, but every
    night tens of millions of families would sit down together in front of their TV set watching the same show, at the same
    time, as their next door neighbors. What happens now with the Super Bowl used to happen every night. We were literally
    in sync. [6]
</p><br>
<p>    
    In a way mid-century TV culture was good. The view it gave of the world was like you'd find in a children's book, and it
    probably had something of the effect that (parents hope) children's books have in making people behave better. But, like
    children's books, TV was also misleading. Dangerously misleading, for adults. In his autobiography, Robert MacNeil talks
    of seeing gruesome images that had just come in from Vietnam and thinking, we can't show these to families while they're
    having dinner.
</p><br>
<p>   
    I know how pervasive the common culture was, because I tried to opt out of it, and it was practically impossible to find
    alternatives. When I was 13 I realized, more from internal evidence than any outside source, that the ideas we were
    being fed on TV were crap, and I stopped watching it. [7] But it wasn't just TV. It seemed like everything around me was
    crap. The politicians all saying the same things, the consumer brands making almost identical products with different
    labels stuck on to indicate how prestigious they were meant to be, the balloon-frame houses with fake "colonial" skins,
    the cars with several feet of gratuitous metal on each end that started to fall apart after a couple years, the "red
    delicious" apples that were red but only nominally apples. And in retrospect, it was crap. [8]
</p><br>
<p>    
    But when I went looking for alternatives to fill this void, I found practically nothing. There was no Internet then. The
    only place to look was in the chain bookstore in our local shopping mall. [9] There I found a copy of The Atlantic. I
    wish I could say it became a gateway into a wider world, but in fact I found it boring and incomprehensible. Like a kid
    tasting whisky for the first time and pretending to like it, I preserved that magazine as carefully as if it had been a
    book. I'm sure I still have it somewhere. But though it was evidence that there was, somewhere, a world that wasn't red
    delicious, I didn't find it till college.
</p><br>
<p>    
    It wasn't just as consumers that the big companies made us similar. They did as employers too. Within companies there
    were powerful forces pushing people toward a single model of how to look and act. IBM was particularly notorious for
    this, but they were only a little more extreme than other big companies. And the models of how to look and act varied
    little between companies. Meaning everyone within this world was expected to seem more or less the same. And not just
    those in the corporate world, but also everyone who aspired to it — which in the middle of the 20th century meant most
    people who weren't already in it. For most of the 20th century, working-class people tried hard to look middle class.
    You can see it in old photos. Few adults aspired to look dangerous in 1950.
</p><br>
<p>   
    But the rise of national corporations didn't just compress us culturally. It compressed us economically too, and on both
    ends.
</p><br>
<p>    
    Along with giant national corporations, we got giant national labor unions. And in the mid 20th century the corporations
    cut deals with the unions where they paid over market price for labor. Partly because the unions were monopolies. [10]
    Partly because, as components of oligopolies themselves, the corporations knew they could safely pass the cost on to
    their customers, because their competitors would have to as well. And partly because in mid-century most of the giant
    companies were still focused on finding new ways to milk economies of scale. Just as startups rightly pay AWS a premium
    over the cost of running their own servers so they can focus on growth, many of the big national corporations were
    willing to pay a premium for labor. [11]
</p><br>
<p>    
    As well as pushing incomes up from the bottom, by overpaying unions, the big companies of the 20th century also pushed
    incomes down at the top, by underpaying their top management. Economist J. K. Galbraith wrote in 1967 that "There are
    few corporations in which it would be suggested that executive salaries are at a maximum." [12]
</p><br>
<p>     
    To some extent this was an illusion. Much of the de facto pay of executives never showed up on their income tax returns,
    because it took the form of perks. The higher the rate of income tax, the more pressure there was to pay employees
    upstream of it. (In the UK, where taxes were even higher than in the US, companies would even pay their kids' private
    school tuitions.) One of the most valuable things the big companies of the mid 20th century gave their employees was job
    security, and this too didn't show up in tax returns or income statistics. So the nature of employment in these
    organizations tended to yield falsely low numbers about economic inequality. But even accounting for that, the big
    companies paid their best people less than market price. There was no market; the expectation was that you'd work for
    the same company for decades if not your whole career. [13]
</p><br>
<p>    
    Your work was so illiquid there was little chance of getting market price. But that same illiquidity also encouraged you
    not to seek it. If the company promised to employ you till you retired and give you a pension afterward, you didn't want
    to extract as much from it this year as you could. You needed to take care of the company so it could take care of you.
    Especially when you'd been working with the same group of people for decades. If you tried to squeeze the company for
    more money, you were squeezing the organization that was going to take care of them. Plus if you didn't put the company
    first you wouldn't be promoted, and if you couldn't switch ladders, promotion on this one was the only way up. [14]
</p><br>
<p>    
    To someone who'd spent several formative years in the armed forces, this situation didn't seem as strange as it does to
    us now. From their point of view, as big company executives, they were high-ranking officers. They got paid a lot more
    than privates. They got to have expense account lunches at the best restaurants and fly around on the company's
    Gulfstreams. It probably didn't occur to most of them to ask if they were being paid market price.
</p><br>
<p>    
    The ultimate way to get market price is to work for yourself, by starting your own company. That seems obvious to any
    ambitious person now. But in the mid 20th century it was an alien concept. Not because starting one's own company seemed
    too ambitious, but because it didn't seem ambitious enough. Even as late as the 1970s, when I grew up, the ambitious
    plan was to get lots of education at prestigious institutions, and then join some other prestigious institution and work
    one's way up the hierarchy. Your prestige was the prestige of the institution you belonged to. People did start their
    own businesses of course, but educated people rarely did, because in those days there was practically zero concept of
    starting what we now call a startup: a business that starts small and grows big. That was much harder to do in the mid
    20th century. Starting one's own business meant starting a business that would start small and stay small. Which in
    those days of big companies often meant scurrying around trying to avoid being trampled by elephants. It was more
    prestigious to be one of the executive class riding the elephant.
</p><br>
<p>    
    By the 1970s, no one stopped to wonder where the big prestigious companies had come from in the first place. It seemed
    like they'd always been there, like the chemical elements. And indeed, there was a double wall between ambitious kids in
    the 20th century and the origins of the big companies. Many of the big companies were roll-ups that didn't have clear
    founders. And when they did, the founders didn't seem like us. Nearly all of them had been uneducated, in the sense of
    not having been to college. They were what Shakespeare called rude mechanicals. College trained one to be a member of
    the professional classes. Its graduates didn't expect to do the sort of grubby menial work that Andrew Carnegie or Henry
    Ford started out doing. [15]
</p><br>
<p>    
    And in the 20th century there were more and more college graduates. They increased from about 2% of the population in
    1900 to about 25% in 2000. In the middle of the century our two big forces intersect, in the form of the GI Bill, which
    sent 2.2 million World War II veterans to college. Few thought of it in these terms, but the result of making college
    the canonical path for the ambitious was a world in which it was socially acceptable to work for Henry Ford, but not to
    be Henry Ford. [16]
</p><br>
<p>    
    I remember this world well. I came of age just as it was starting to break up. In my childhood it was still dominant.
    Not quite so dominant as it had been. We could see from old TV shows and yearbooks and the way adults acted that people
    in the 1950s and 60s had been even more conformist than us. The mid-century model was already starting to get old. But
    that was not how we saw it at the time. We would at most have said that one could be a bit more daring in 1975 than
    1965. And indeed, things hadn't changed much yet.
</p><br>
<p>    
    But change was coming soon. And when the Duplo economy started to disintegrate, it disintegrated in several different
    ways at once. Vertically integrated companies literally dis-integrated because it was more efficient to. Incumbents
    faced new competitors as (a) markets went global and (b) technical innovation started to trump economies of scale,
    turning size from an asset into a liability. Smaller companies were increasingly able to survive as formerly narrow
    channels to consumers broadened. Markets themselves started to change faster, as whole new categories of products
    appeared. And last but not least, the federal government, which had previously smiled upon J. P. Morgan's world as the
    natural state of things, began to realize it wasn't the last word after all.
</p><br>
<p>    
    What J. P. Morgan was to the horizontal axis, Henry Ford was to the vertical. He wanted to do everything himself. The
    giant plant he built at River Rouge between 1917 and 1928 literally took in iron ore at one end and sent cars out the
    other. 100,000 people worked there. At the time it seemed the future. But that is not how car companies operate today.
    Now much of the design and manufacturing happens in a long supply chain, whose products the car companies ultimately
    assemble and sell. The reason car companies operate this way is that it works better. Each company in the supply chain
    focuses on what they know best. And they each have to do it well or they can be swapped out for another supplier.
</p><br>
<p>    
    Why didn't Henry Ford realize that networks of cooperating companies work better than a single big company? One reason
    is that supplier networks take a while to evolve. In 1917, doing everything himself seemed to Ford the only way to get
    the scale he needed. And the second reason is that if you want to solve a problem using a network of cooperating
    companies, you have to be able to coordinate their efforts, and you can do that much better with computers. Computers
    reduce the transaction costs that Coase argued are the raison d'etre of corporations. That is a fundamental change.
</p><br>
<p>    
    In the early 20th century, big companies were synonymous with efficiency. In the late 20th century they were synonymous
    with inefficiency. To some extent this was because the companies themselves had become sclerotic. But it was also
    because our standards were higher.
</p><br>
<p>    
    It wasn't just within existing industries that change occurred. The industries themselves changed. It became possible to
    make lots of new things, and sometimes the existing companies weren't the ones who did it best.
</p><br>
<p>    
    Microcomputers are a classic example. The market was pioneered by upstarts like Apple. When it got big enough, IBM
    decided it was worth paying attention to. At the time IBM completely dominated the computer industry. They assumed that
    all they had to do, now that this market was ripe, was to reach out and pick it. Most people at the time would have
    agreed with them. But what happened next illustrated how much more complicated the world had become. IBM did launch a
    microcomputer. Though quite successful, it did not crush Apple. But even more importantly, IBM itself ended up being
    supplanted by a supplier coming in from the side — from software, which didn't even seem to be the same business. IBM's
    big mistake was to accept a non-exclusive license for DOS. It must have seemed a safe move at the time. No other
    computer manufacturer had ever been able to outsell them. What difference did it make if other manufacturers could offer
    DOS too? The result of that miscalculation was an explosion of inexpensive PC clones. Microsoft now owned the PC
    standard, and the customer. And the microcomputer business ended up being Apple vs Microsoft.
</p><br>
<p>  
    Basically, Apple bumped IBM and then Microsoft stole its wallet. That sort of thing did not happen to big companies in
    mid-century. But it was going to happen increasingly often in the future.
</p><br>
<p>   
    Change happened mostly by itself in the computer business. In other industries, legal obstacles had to be removed first.
    Many of the mid-century oligopolies had been anointed by the federal government with policies (and in wartime, large
    orders) that kept out competitors. This didn't seem as dubious to government officials at the time as it sounds to us.
    They felt a two-party system ensured sufficient competition in politics. It ought to work for business too.
</p><br>
<p>    
    Gradually the government realized that anti-competitive policies were doing more harm than good, and during the Carter
    administration it started to remove them. The word used for this process was misleadingly narrow: deregulation. What was
    really happening was de-oligopolization. It happened to one industry after another. Two of the most visible to consumers
    were air travel and long-distance phone service, which both became dramatically cheaper after deregulation.
</p><br>
<p>    
    Deregulation also contributed to the wave of hostile takeovers in the 1980s. In the old days the only limit on the
    inefficiency of companies, short of actual bankruptcy, was the inefficiency of their competitors. Now companies had to
    face absolute rather than relative standards. Any public company that didn't generate sufficient returns on its assets
    risked having its management replaced with one that would. Often the new managers did this by breaking companies up into
    components that were more valuable separately. [17]
</p><br>
<p>    
    Version 1 of the national economy consisted of a few big blocks whose relationships were negotiated in back rooms by a
    handful of executives, politicians, regulators, and labor leaders. Version 2 was higher resolution: there were more
    companies, of more different sizes, making more different things, and their relationships changed faster. In this world
    there were still plenty of back room negotiations, but more was left to market forces. Which further accelerated the
    fragmentation.
</p><br>
<p>    
    It's a little misleading to talk of versions when describing a gradual process, but not as misleading as it might seem.
    There was a lot of change in a few decades, and what we ended up with was qualitatively different. The companies in the
    S&P 500 in 1958 had been there an average of 61 years. By 2012 that number was 18 years. [18]
</p><br>
<p>    
    The breakup of the Duplo economy happened simultaneously with the spread of computing power. To what extent were
    computers a precondition? It would take a book to answer that. Obviously the spread of computing power was a
    precondition for the rise of startups. I suspect it was for most of what happened in finance too. But was it a
    precondition for globalization or the LBO wave? I don't know, but I wouldn't discount the possibility. It may be that
    the refragmentation was driven by computers in the way the industrial revolution was driven by steam engines. Whether or
    not computers were a precondition, they have certainly accelerated it.
</p><br>
<p>    
    The new fluidity of companies changed people's relationships with their employers. Why climb a corporate ladder that
    might be yanked out from under you? Ambitious people started to think of a career less as climbing a single ladder than
    as a series of jobs that might be at different companies. More movement (or even potential movement) between companies
    introduced more competition in salaries. Plus as companies became smaller it became easier to estimate how much an
    employee contributed to the company's revenue. Both changes drove salaries toward market price. And since people vary
    dramatically in productivity, paying market price meant salaries started to diverge.
</p><br>
<p>    
    By no coincidence it was in the early 1980s that the term "yuppie" was coined. That word is not much used now, because
    the phenomenon it describes is so taken for granted, but at the time it was a label for something novel. Yuppies were
    young professionals who made lots of money. To someone in their twenties today, this wouldn't seem worth naming. Why
    wouldn't young professionals make lots of money? But until the 1980s, being underpaid early in your career was part of
    what it meant to be a professional. Young professionals were paying their dues, working their way up the ladder. The
    rewards would come later. What was novel about yuppies was that they wanted market price for the work they were doing
    now.
</p><br>
<p>  
    The first yuppies did not work for startups. That was still in the future. Nor did they work for big companies. They
    were professionals working in fields like law, finance, and consulting. But their example rapidly inspired their peers.
    Once they saw that new BMW 325i, they wanted one too.
</p><br>
<p>   
    Underpaying people at the beginning of their career only works if everyone does it. Once some employer breaks ranks,
    everyone else has to, or they can't get good people. And once started this process spreads through the whole economy,
    because at the beginnings of people's careers they can easily switch not merely employers but industries.
</p><br>
<p>   
    But not all young professionals benefitted. You had to produce to get paid a lot. It was no coincidence that the first
    yuppies worked in fields where it was easy to measure that.
</p><br>
<p>   
    More generally, an idea was returning whose name sounds old-fashioned precisely because it was so rare for so long: that
    you could make your fortune. As in the past there were multiple ways to do it. Some made their fortunes by creating
    wealth, and others by playing zero-sum games. But once it became possible to make one's fortune, the ambitious had to
    decide whether or not to. A physicist who chose physics over Wall Street in 1990 was making a sacrifice that a physicist
    in 1960 didn't have to think about.
</p><br>
<p>   
    The idea even flowed back into big companies. CEOs of big companies make more now than they used to, and I think much of
    the reason is prestige. In 1960, corporate CEOs had immense prestige. They were the winners of the only economic game in
    town. But if they made as little now as they did then, in real dollar terms, they'd seem like small fry compared to
    professional athletes and whiz kids making millions from startups and hedge funds. They don't like that idea, so now
    they try to get as much as they can, which is more than they had been getting. [19]
</p><br>
<p>    
    Meanwhile a similar fragmentation was happening at the other end of the economic scale. As big companies' oligopolies
    became less secure, they were less able to pass costs on to customers and thus less willing to overpay for labor. And as
    the Duplo world of a few big blocks fragmented into many companies of different sizes — some of them overseas — it
    became harder for unions to enforce their monopolies. As a result workers' wages also tended toward market price. Which
    (inevitably, if unions had been doing their job) tended to be lower. Perhaps dramatically so, if automation had
    decreased the need for some kind of work.
</p><br>
<p>   
    And just as the mid-century model induced social as well as economic cohesion, its breakup brought social as well as
    economic fragmentation. People started to dress and act differently. Those who would later be called the "creative
    class" became more mobile. People who didn't care much for religion felt less pressure to go to church for appearances'
    sake, while those who liked it a lot opted for increasingly colorful forms. Some switched from meat loaf to tofu, and
    others to Hot Pockets. Some switched from driving Ford sedans to driving small imported cars, and others to driving
    SUVs. Kids who went to private schools or wished they did started to dress "preppy," and kids who wanted to seem
    rebellious made a conscious effort to look disreputable. In a hundred ways people spread apart. [20]
</p><br>
<p>    
    Almost four decades later, fragmentation is still increasing. Has it been net good or bad? I don't know; the question
    may be unanswerable. Not entirely bad though. We take for granted the forms of fragmentation we like, and worry only
    about the ones we don't. But as someone who caught the tail end of mid-century conformism, I can tell you it was no
    utopia. [21]
</p><br>
<p>   
    My goal here is not to say whether fragmentation has been good or bad, just to explain why it's happening. With the
    centripetal forces of total war and 20th century oligopoly mostly gone, what will happen next? And more specifically, is
    it possible to reverse some of the fragmentation we've seen?
</p><br>
<p>   
    If it is, it will have to happen piecemeal. You can't reproduce mid-century cohesion the way it was originally produced.
    It would be insane to go to war just to induce more national unity. And once you understand the degree to which the
    economic history of the 20th century was a low-res version 1, it's clear you can't reproduce that either.
</p><br>
<p>   
    20th century cohesion was something that happened at least in a sense naturally. The war was due mostly to external
    forces, and the Duplo economy was an evolutionary phase. If you want cohesion now, you'd have to induce it deliberately.
    And it's not obvious how. I suspect the best we'll be able to do is address the symptoms of fragmentation. But that may
    be enough.
</p><br>
<p>   
    The form of fragmentation people worry most about lately is economic inequality, and if you want to eliminate that
    you're up against a truly formidable headwind — one that has been in operation since the stone age: technology.
</p><br>
<p>   
    Technology is a lever. It magnifies work. And the lever not only grows increasingly long, but the rate at which it grows
    is itself increasing.
</p><br>
<p>    
    Which in turn means the variation in the amount of wealth people can create has not only been increasing, but
    accelerating. The unusual conditions that prevailed in the mid 20th century masked this underlying trend. The ambitious
    had little choice but to join large organizations that made them march in step with lots of other people — literally in
    the case of the armed forces, figuratively in the case of big corporations. Even if the big corporations had wanted to
    pay people proportionate to their value, they couldn't have figured out how. But that constraint has gone now. Ever
    since it started to erode in the 1970s, we've seen the underlying forces at work again. [22]
</p><br>
<p>    
    Not everyone who gets rich now does it by creating wealth, certainly. But a significant number do, and the Baumol Effect
    means all their peers get dragged along too. [23] And as long as it's possible to get rich by creating wealth, the
    default tendency will be for economic inequality to increase. Even if you eliminate all the other ways to get rich. You
    can mitigate this with subsidies at the bottom and taxes at the top, but unless taxes are high enough to discourage
    people from creating wealth, you're always going to be fighting a losing battle against increasing variation in
    productivity. [24]
</p><br>
<p>    
    That form of fragmentation, like the others, is here to stay. Or rather, back to stay. Nothing is forever, but the
    tendency toward fragmentation should be more forever than most things, precisely because it's not due to any particular
    cause. It's simply a reversion to the mean. When Rockefeller said individualism was gone, he was right for a hundred
    years. It's back now, and that's likely to be true for longer.
</p><br>
<p>   
    I worry that if we don't acknowledge this, we're headed for trouble. If we think 20th century cohesion disappeared
    because of few policy tweaks, we'll be deluded into thinking we can get it back (minus the bad parts, somehow) with a
    few countertweaks. And then we'll waste our time trying to eliminate fragmentation, when we'd be better off thinking
    about how to mitigate its consequences.
</p><br>
<p>    
    Notes
</p><br>
<p>     
    [1] Lester Thurow, writing in 1975, said the wage differentials prevailing at the end of World War II had become so
    embedded that they "were regarded as 'just' even after the egalitarian pressures of World War II had disappeared.
    Basically, the same differentials exist to this day, thirty years later." But Goldin and Margo think market forces in
    the postwar period also helped preserve the wartime compression of wages — specifically increased demand for unskilled
    workers, and oversupply of educated ones.
</p><br>
<p>    
    (Oddly enough, the American custom of having employers pay for health insurance derives from efforts by businesses to
    circumvent NWLB wage controls in order to attract workers.)
</p><br>
<p>    
    [2] As always, tax rates don't tell the whole story. There were lots of exemptions, especially for individuals. And in
    World War II the tax codes were so new that the government had little acquired immunity to tax avoidance. If the rich
    paid high taxes during the war it was more because they wanted to than because they had to.
</p><br>
<p>    
    After the war, federal tax receipts as a percentage of GDP were about the same as they are now. In fact, for the entire
    period since the war, tax receipts have stayed close to 18% of GDP, despite dramatic changes in tax rates. The lowest
    point occurred when marginal income tax rates were highest: 14.1% in 1950. Looking at the data, it's hard to avoid the
    conclusion that tax rates have had little effect on what people actually paid.
</p><br>
<p>    
    [3] Though in fact the decade preceding the war had been a time of unprecedented federal power, in response to the
    Depression. Which is not entirely a coincidence, because the Depression was one of the causes of the war. In many ways
    the New Deal was a sort of dress rehearsal for the measures the federal government took during wartime. The wartime
    versions were much more drastic and more pervasive though. As Anthony Badger wrote, "for many Americans the decisive
    change in their experiences came not with the New Deal but with World War II."
</p><br>
<p>    
    [4] I don't know enough about the origins of the world wars to say, but it's not inconceivable they were connected to
    the rise of big corporations. If that were the case, 20th century cohesion would have a single cause.
</p><br>
<p>    
    [5] More precisely, there was a bimodal economy consisting, in Galbraith's words, of "the world of the technically
    dynamic, massively capitalized and highly organized corporations on the one hand and the hundreds of thousands of small
    and traditional proprietors on the other." Money, prestige, and power were concentrated in the former, and there was
    near zero crossover.
</p><br>
<p>     
    [6] I wonder how much of the decline in families eating together was due to the decline in families watching TV together
    afterward.
</p><br>
<p>     
    [7] I know when this happened because it was the season Dallas premiered. Everyone else was talking about what was
    happening on Dallas, and I had no idea what they meant.
</p><br>
<p>    
    [8] I didn't realize it till I started doing research for this essay, but the meretriciousness of the products I grew up
    with is a well-known byproduct of oligopoly. When companies can't compete on price, they compete on tailfins.
</p><br>
<p>   
    [9] Monroeville Mall was at the time of its completion in 1969 the largest in the country. In the late 1970s the movie
    Dawn of the Dead was shot there. Apparently the mall was not just the location of the movie, but its inspiration; the
    crowds of shoppers drifting through this huge mall reminded George Romero of zombies. My first job was scooping ice
    cream in the Baskin-Robbins.
</p><br>
<p>    
    [10] Labor unions were exempted from antitrust laws by the Clayton Antitrust Act in 1914 on the grounds that a person's
    work is not "a commodity or article of commerce." I wonder if that means service companies are also exempt.
</p><br>
<p>    
    [11] The relationships between unions and unionized companies can even be symbiotic, because unions will exert political
    pressure to protect their hosts. According to Michael Lind, when politicians tried to attack the A&P supermarket chain
    because it was putting local grocery stores out of business, "A&P successfully defended itself by allowing the
    unionization of its workforce in 1938, thereby gaining organized labor as a constituency." I've seen this phenomenon
    myself: hotel unions are responsible for more of the political pressure against Airbnb than hotel companies.
</p><br>
<p>    
    [12] Galbraith was clearly puzzled that corporate executives would work so hard to make money for other people (the
    shareholders) instead of themselves. He devoted much of The New Industrial State to trying to figure this out.
</p><br>
<p>   
    His theory was that professionalism had replaced money as a motive, and that modern corporate executives were, like
    (good) scientists, motivated less by financial rewards than by the desire to do good work and thereby earn the respect
    of their peers. There is something in this, though I think lack of movement between companies combined with
    self-interest explains much of observed behavior.
</p><br>
<p>   
    [13] Galbraith (p. 94) says a 1952 study of the 800 highest paid executives at 300 big corporations found that three
    quarters of them had been with their company for more than 20 years.
</p><br>
<p>    
    [14] It seems likely that in the first third of the 20th century executive salaries were low partly because companies
    then were more dependent on banks, who would have disapproved if executives got too much. This was certainly true in the
    beginning. The first big company CEOs were J. P. Morgan's hired hands.
 </p><br>
<p>    
    Companies didn't start to finance themselves with retained earnings till the 1920s. Till then they had to pay out their
    earnings in dividends, and so depended on banks for capital for expansion. Bankers continued to sit on corporate boards
    till the Glass-Steagall act in 1933.
</p><br>
<p>  
    By mid-century big companies funded 3/4 of their growth from earnings. But the early years of bank dependence,
    reinforced by the financial controls of World War II, must have had a big effect on social conventions about executive
    salaries. So it may be that the lack of movement between companies was as much the effect of low salaries as the cause.
</p><br>
<p>   
    Incidentally, the switch in the 1920s to financing growth with retained earnings was one cause of the 1929 crash. The
    banks now had to find someone else to lend to, so they made more margin loans.
</p><br>
<p>    
    [15] Even now it's hard to get them to. One of the things I find hardest to get into the heads of would-be startup
    founders is how important it is to do certain kinds of menial work early in the life of a company. Doing things that
    don't scale is to how Henry Ford got started as a high-fiber diet is to the traditional peasant's diet: they had no
    choice but to do the right thing, while we have to make a conscious effort.
</p><br>
<p>    
    [16] Founders weren't celebrated in the press when I was a kid. "Our founder" meant a photograph of a severe-looking man
    with a walrus mustache and a wing collar who had died decades ago. The thing to be when I was a kid was an executive. If
    you weren't around then it's hard to grasp the cachet that term had. The fancy version of everything was called the
    "executive" model.
</p><br>
<p>    
    [17] The wave of hostile takeovers in the 1980s was enabled by a combination of circumstances: court decisions striking
    down state anti-takeover laws, starting with the Supreme Court's 1982 decision in Edgar v. MITE Corp.; the Reagan
    administration's comparatively sympathetic attitude toward takeovers; the Depository Institutions Act of 1982, which
    allowed banks and savings and loans to buy corporate bonds; a new SEC rule issued in 1982 (rule 415) that made it
    possible to bring corporate bonds to market faster; the creation of the junk bond business by Michael Milken; a vogue
    for conglomerates in the preceding period that caused many companies to be combined that never should have been; a
    decade of inflation that left many public companies trading below the value of their assets; and not least, the
    increasing complacency of managements.
</p><br>
<p>   
    [18] Foster, Richard. "Creative Destruction Whips through Corporate America." Innosight, February 2012.
</p><br>
<p>  
    [19] CEOs of big companies may be overpaid. I don't know enough about big companies to say. But it is certainly not
    impossible for a CEO to make 200x as much difference to a company's revenues as the average employee. Look at what Steve
    Jobs did for Apple when he came back as CEO. It would have been a good deal for the board to give him 95% of the
    company. Apple's market cap the day Steve came back in July 1997 was 1.73 billion. 5% of Apple now (January 2016) would
    be worth about 30 billion. And it would not be if Steve hadn't come back; Apple probably wouldn't even exist anymore.
</p><br>
<p>   
    Merely including Steve in the sample might be enough to answer the question of whether public company CEOs in the
    aggregate are overpaid. And that is not as facile a trick as it might seem, because the broader your holdings, the more
    the aggregate is what you care about.
</p><br>
<p>   
    [20] The late 1960s were famous for social upheaval. But that was more rebellion (which can happen in any era if people
    are provoked sufficiently) than fragmentation. You're not seeing fragmentation unless you see people breaking off to
    both left and right.
</p><br>
<p>   
    [21] Globally the trend has been in the other direction. While the US is becoming more fragmented, the world as a whole
    is becoming less fragmented, and mostly in good ways.
</p><br>
<p>   
    [22] There were a handful of ways to make a fortune in the mid 20th century. The main one was drilling for oil, which
    was open to newcomers because it was not something big companies could dominate through economies of scale. How did
    individuals accumulate large fortunes in an era of such high taxes? Giant tax loopholes defended by two of the most
    powerful men in Congress, Sam Rayburn and Lyndon Johnson.
</p><br>
<p>   
    But becoming a Texas oilman was not in 1950 something one could aspire to the way starting a startup or going to work on
    Wall Street were in 2000, because (a) there was a strong local component and (b) success depended so much on luck.
</p><br>
<p>   
    [23] The Baumol Effect induced by startups is very visible in Silicon Valley. Google will pay people millions of dollars
    a year to keep them from leaving to start or join startups.
</p><br>
<p>   
    [24] I'm not claiming variation in productivity is the only cause of economic inequality in the US. But it's a
    significant cause, and it will become as big a cause as it needs to, in the sense that if you ban other ways to get
    rich, people who want to get rich will use this route instead.
</p><br>
<p>   
    Thanks to Sam Altman, Trevor Blackwell, Paul Buchheit, Patrick Collison, Ron Conway, Chris Dixon, Benedict Evans,
    Richard Florida, Ben Horowitz, Jessica Livingston, Robert Morris, Tim O'Reilly, Geoff Ralston, Max Roser, Alexia
    Tsotsis, and Qasar Younis for reading drafts of this. Max also told me about several valuable sources.
</p><br>
<p>    
    
    
    Bibliography
</p><br>
<p>   
    Allen, Frederick Lewis. The Big Change. Harper, 1952.
</p><br>
<p>    
    Averitt, Robert. The Dual Economy. Norton, 1968.
</p><br>
<p>    
    Badger, Anthony. The New Deal. Hill and Wang, 1989.
</p><br>
<p>   
    Bainbridge, John. The Super-Americans. Doubleday, 1961.
</p><br>
<p>    
    Beatty, Jack. Collossus. Broadway, 2001.
</p><br>
<p>   
    Brinkley, Douglas. Wheels for the World. Viking, 2003.
</p><br>
<p>     
    Brownleee, W. Elliot. Federal Taxation in America. Cambridge, 1996.
</p><br>
<p>   
    Chandler, Alfred. The Visible Hand. Harvard, 1977.
</p><br>
<p>    
    Chernow, Ron. The House of Morgan. Simon & Schuster, 1990.
</p><br>
<p>   
    Chernow, Ron. Titan: The Life of John D. Rockefeller. Random House, 1998.
</p><br>
<p>    
    Galbraith, John. The New Industrial State. Houghton Mifflin, 1967.
</p><br>
<p>    
    Goldin, Claudia and Robert A. Margo. "The Great Compression: The Wage Structure in the United States at Mid-Century."
    NBER Working Paper 3817, 1991.
</p><br>
<p>   
    Gordon, John. An Empire of Wealth. HarperCollins, 2004.
</p><br>
<p>    
    Klein, Maury. The Genesis of Industrial America, 1870-1920. Cambridge, 2007.
</p><br>
<p>    
    Lind, Michael. Land of Promise. HarperCollins, 2012.
</p><br>
<p>    
    Mickelthwaite, John, and Adrian Wooldridge. The Company. Modern Library, 2003.
</p><br>
<p>   
    Nasaw, David. Andrew Carnegie. Penguin, 2006.
</p><br>
<p>   
    Sobel, Robert. The Age of Giant Corporations. Praeger, 1993.
</p><br>
<p>    
    Thurow, Lester. Generating Inequality: Mechanisms of Distribution. Basic Books, 1975.
</p><br>
<p>    
    Witte, John. The Politics and Development of the Federal Income Tax. Wisconsin, 1985.
</p><br>
<br><br>
<span id="title">Jessica Livingston</span>
                <br>
                <span id="secondText">November 2015</span>
                <br><br><br><br>   
<p>
    A few months ago an article about Y Combinator said that early on it had been a "one-man show." It's sadly common to
    read that sort of thing. But the problem with that description is not just that it's unfair. It's also misleading.
    Much of what's most novel about YC is due to Jessica Livingston. If you don't understand her, you don't understand
    YC. So let me tell you a little about Jessica.
</p><br>
<p>
    YC had 4 founders. Jessica and I decided one night to start it, and the next day we recruited my friends Robert
    Morris and Trevor Blackwell. Jessica and I ran YC day to day, and Robert and Trevor read applications and did
    interviews with us.
</p><br>
<p>
    Jessica and I were already dating when we started YC. At first we tried to act "professional" about this, meaning we
    tried to conceal it. In retrospect that seems ridiculous, and we soon dropped the pretense. And the fact that
    Jessica and I were a couple is a big part of what made YC what it was. YC felt like a family. The founders early on
    were mostly young. We all had dinner together once a week, cooked for the first couple years by me. Our first
    building had been a private home. The overall atmosphere was shockingly different from a VC's office on Sand Hill
    Road, in a way that was entirely for the better. There was an authenticity that everyone who walked in could sense.
    And that didn't just mean that people trusted us. It was the perfect quality to instill in startups. Authenticity is
    one of the most important things YC looks for in founders, not just because fakers and opportunists are annoying,
    but because authenticity is one of the main things that separates the most successful startups from the rest.
</p><br>
<p>
    Early YC was a family, and Jessica was its mom. And the culture she defined was one of YC's most important
    innovations. Culture is important in any organization, but at YC culture wasn't just how we behaved when we built
    the product. At YC, the culture was the product.
</p><br>
<p>
    Jessica was also the mom in another sense: she had the last word. Everything we did as an organization went through
    her first — who to fund, what to say to the public, how to deal with other companies, who to hire, everything.
</p><br>
<p>
    Before we had kids, YC was more or less our life. There was no real distinction between working hours and not. We
    talked about YC all the time. And while there might be some businesses that it would be tedious to let infect your
    private life, we liked it. We'd started YC because it was something we were interested in. And some of the problems
    we were trying to solve were endlessly difficult. How do you recognize good founders? You could talk about that for
    years, and we did; we still do.
</p><br>
<p>
    I'm better at some things than Jessica, and she's better at some things than me. One of the things she's best at is
    judging people. She's one of those rare individuals with x-ray vision for character. She can see through any kind of
    faker almost immediately. Her nickname within YC was the Social Radar, and this special power of hers was critical
    in making YC what it is. The earlier you pick startups, the more you're picking the founders. Later stage investors
    get to try products and look at growth numbers. At the stage where YC invests, there is often neither a product nor
    any numbers.
</p><br>
<p>
    Others thought YC had some special insight about the future of technology. Mostly we had the same sort of insight
    Socrates claimed: we at least knew we knew nothing. What made YC successful was being able to pick good founders. We
    thought Airbnb was a bad idea. We funded it because we liked the founders.
</p><br>
<p>
    During interviews, Robert and Trevor and I would pepper the applicants with technical questions. Jessica would
    mostly watch. A lot of the applicants probably read her as some kind of secretary, especially early on, because she
    was the one who'd go out and get each new group and she didn't ask many questions. She was ok with that. It was
    easier for her to watch people if they didn't notice her. But after the interview, the three of us would turn to
    Jessica and ask "What does the Social Radar say?" [1]
</p><br>
<p>
    Having the Social Radar at interviews wasn't just how we picked founders who'd be successful. It was also how we
    picked founders who were good people. At first we did this because we couldn't help it. Imagine what it would feel
    like to have x-ray vision for character. Being around bad people would be intolerable. So we'd refuse to fund
    founders whose characters we had doubts about even if we thought they'd be successful.
</p><br>
<p>
    Though we initially did this out of self-indulgence, it turned out to be very valuable to YC. We didn't realize it
    in the beginning, but the people we were picking would become the YC alumni network. And once we picked them, unless
    they did something really egregious, they were going to be part of it for life. Some now think YC's alumni network
    is its most valuable feature. I personally think YC's advice is pretty good too, but the alumni network is certainly
    among the most valuable features. The level of trust and helpfulness is remarkable for a group of such size. And
    Jessica is the main reason why.
</p><br>
<p>
    (As we later learned, it probably cost us little to reject people whose characters we had doubts about, because how
    good founders are and how well they do are not orthogonal. If bad founders succeed at all, they tend to sell early.
    The most successful founders are almost all good.)
</p><br>
<p>
    If Jessica was so important to YC, why don't more people realize it? Partly because I'm a writer, and writers always
    get disproportionate attention. YC's brand was initially my brand, and our applicants were people who'd read my
    essays. But there is another reason: Jessica hates attention. Talking to reporters makes her nervous. The thought of
    giving a talk paralyzes her. She was even uncomfortable at our wedding, because the bride is always the center of
    attention. [2]
</p><br>
<p>
    It's not just because she's shy that she hates attention, but because it throws off the Social Radar. She can't be
    herself. You can't watch people when everyone is watching you.
</p><br>
<p>
    Another reason attention worries her is that she hates bragging. In anything she does that's publicly visible, her
    biggest fear (after the obvious fear that it will be bad) is that it will seem ostentatious. She says being too
    modest is a common problem for women. But in her case it goes beyond that. She has a horror of ostentation so
    visceral it's almost a phobia.
</p><br>
<p>
    She also hates fighting. She can't do it; she just shuts down. And unfortunately there is a good deal of fighting in
    being the public face of an organization.
</p><br>
<p>
    So although Jessica more than anyone made YC unique, the very qualities that enabled her to do it mean she tends to
    get written out of YC's history. Everyone buys this story that PG started YC and his wife just kind of helped. Even
    YC's haters buy it. A couple years ago when people were attacking us for not funding more female founders (than
    exist), they all treated YC as identical with PG. It would have spoiled the narrative to acknowledge Jessica's
    central role at YC.
</p><br>
<p>
    Jessica was boiling mad that people were accusing her company of sexism. I've never seen her angrier about anything.
    But she did not contradict them. Not publicly. In private there was a great deal of profanity. And she wrote three
    separate essays about the question of female founders. But she could never bring herself to publish any of them.
    She'd seen the level of vitriol in this debate, and she shrank from engaging. [3]
</p><br>
<p> It wasn't just because she disliked fighting. She's so sensitive to character that it repels her even to fight with
    dishonest people. The idea of mixing it up with linkbait journalists or Twitter trolls would seem to her not merely
    frightening, but disgusting.
</p><br>
<p>
    But Jessica knew her example as a successful female founder would encourage more women to start companies, so last
    year she did something YC had never done before and hired a PR firm to get her some interviews. At one of the first
    she did, the reporter brushed aside her insights about startups and turned it into a sensationalistic story about
    how some guy had tried to chat her up as she was waiting outside the bar where they had arranged to meet. Jessica
    was mortified, partly because the guy had done nothing wrong, but more because the story treated her as a victim
    significant only for being a woman, rather than one of the most knowledgeable investors in the Valley.
</p><br>
<p>
    After that she told the PR firm to stop.
</p><br>
<p>
    You're not going to be hearing in the press about what Jessica has achieved. So let me tell you what Jessica has
    achieved. Y Combinator is fundamentally a nexus of people, like a university. It doesn't make a product. What
    defines it is the people. Jessica more than anyone curated and nurtured that collection of people. In that sense she
    literally made YC.
</p><br>
<p>
    Jessica knows more about the qualities of startup founders than anyone else ever has. Her immense data set and x-ray
    vision are the perfect storm in that respect. The qualities of the founders are the best predictor of how a startup
    will do. And startups are in turn the most important source of growth in mature economies.
</p><br>
<p>
    The person who knows the most about the most important factor in the growth of mature economies — that is who
    Jessica Livingston is. Doesn't that sound like someone who should be better known?
</p><br>
<p>
 Notes
</p><br>
<p>
    [1] Harj Taggar reminded me that while Jessica didn't ask many questions, they tended to be important ones:
</p><br>
<p>
    "She was always good at sniffing out any red flags about the team or their determination and disarmingly asking the
    right question, which usually revealed more than the founders realized."
</p><br>
<p>
    [2] Or more precisely, while she likes getting attention in the sense of getting credit for what she has done, she
    doesn't like getting attention in the sense of being watched in real time. Unfortunately, not just for her but for a
    lot of people, how much you get of the former depends a lot on how much you get of the latter.
</p><br>
<p>
    Incidentally, if you saw Jessica at a public event, you would never guess she hates attention, because (a) she is
    very polite and (b) when she's nervous, she expresses it by smiling more.
</p><br>
<p>
    [3] The existence of people like Jessica is not just something the mainstream media needs to learn to acknowledge,
    but something feminists need to learn to acknowledge as well. There are successful women who don't like to fight.
    Which means if the public conversation about women consists of fighting, their voices will be silenced.
</p><br>
<p>
    There's a sort of Gresham's Law of conversations. If a conversation reaches a certain level of incivility, the more
    thoughtful people start to leave. No one understands female founders better than Jessica. But it's unlikely anyone
    will ever hear her speak candidly about the topic. She ventured a toe in that water a while ago, and the reaction
    was so violent that she decided "never again."
</p><br>
<p>
    Thanks to Sam Altman, Paul Buchheit, Patrick Collison, Daniel Gackle, Carolynn Levy, Jon Levy, Kirsty Nathoo, Robert
    Morris, Geoff Ralston, and Harj Taggar for reading drafts of this. And yes, Jessica Livingston, who made me cut
    surprisingly little.
</p><br>
<br><br>
<span id="title">A Way To Detect Bias</span>
                <br>
                <span id="secondText">October 2015</span>
                <br><br><br><br>   
<p>
    This will come as a surprise to a lot of people, but in some cases it's possible to detect bias in a selection process
    without knowing anything about the applicant pool. Which is exciting because among other things it means third parties
    can use this technique to detect bias whether those doing the selecting want them to or not.
</p><br>
<p>  
    You can use this technique whenever (a) you have at least a random sample of the applicants that were selected, (b)
    their subsequent performance is measured, and (c) the groups of applicants you're comparing have roughly equal
    distribution of ability.
</p><br>
<p> 
    How does it work? Think about what it means to be biased. What it means for a selection process to be biased against
    applicants of type x is that it's harder for them to make it through. Which means applicants of type x have to be better
    to get selected than applicants not of type x. [1] Which means applicants of type x who do make it through the selection
    process will outperform other successful applicants. And if the performance of all the successful applicants is
    measured, you'll know if they do.
</p><br>
<p>  
    Of course, the test you use to measure performance must be a valid one. And in particular it must not be invalidated by
    the bias you're trying to measure. But there are some domains where performance can be measured, and in those detecting
    bias is straightforward. Want to know if the selection process was biased against some type of applicant? Check whether
    they outperform the others. This is not just a heuristic for detecting bias. It's what bias means.
</p><br>
<p>  
    For example, many suspect that venture capital firms are biased against female founders. This would be easy to detect:
    among their portfolio companies, do startups with female founders outperform those without? A couple months ago, one VC
    firm (almost certainly unintentionally) published a study showing bias of this type. First Round Capital found that
    among its portfolio companies, startups with female founders outperformed those without by 63%. [2]
</p><br>
<p>  
    The reason I began by saying that this technique would come as a surprise to many people is that we so rarely see
    analyses of this type. I'm sure it will come as a surprise to First Round that they performed one. I doubt anyone there
    realized that by limiting their sample to their own portfolio, they were producing a study not of startup trends but of
    their own biases when selecting companies.
</p><br>
<p>  
    I predict we'll see this technique used more in the future. The information needed to conduct such studies is
    increasingly available. Data about who applies for things is usually closely guarded by the organizations selecting
    them, but nowadays data about who gets selected is often publicly available to anyone who takes the trouble to aggregate
    it.
</p><br>
<p>  
    Notes
</p><br>
<p>  
    [1] This technique wouldn't work if the selection process looked for different things from different types of
    applicants—for example, if an employer hired men based on their ability but women based on their appearance.
</p><br>
<p>  
    [2] As Paul Buchheit points out, First Round excluded their most successful investment, Uber, from the study. And while
    it makes sense to exclude outliers from some types of studies, studies of returns from startup investing, which is all
    about hitting outliers, are not one of them.
</p><br>
<p>   
    Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading drafts of this.
</p><br>
<br><br>
<span id="title">Wrtie Like You Talk</span>
                <br>
                <span id="secondText">October 2015</span>
                <br><br><br><br>   
<p>
    Here's a simple trick for getting more people to read what you write: write in spoken language.
</p><br>
<p>  
    Something comes over most people when they start writing. They write in a different language than they'd use if they
    were talking to a friend. The sentence structure and even the words are different. No one uses "pen" as a verb in spoken
    English. You'd feel like an idiot using "pen" instead of "write" in a conversation with a friend.
</p><br>
<p>  
    The last straw for me was a sentence I read a couple days ago:
    The mercurial Spaniard himself declared: "After Altamira, all is decadence."
    It's from Neil Oliver's A History of Ancient Britain. I feel bad making an example of this book, because it's no worse
    than lots of others. But just imagine calling Picasso "the mercurial Spaniard" when talking to a friend. Even one
    sentence of this would raise eyebrows in conversation. And yet people write whole books of it.
</p><br>
<p>  
    Ok, so written and spoken language are different. Does that make written language worse?
</p><br>
<p>   
    If you want people to read and understand what you write, yes. Written language is more complex, which makes it more
    work to read. It's also more formal and distant, which gives the reader's attention permission to drift. But perhaps
    worst of all, the complex sentences and fancy words give you, the writer, the false impression that you're saying more
    than you actually are.
</p><br>
<p>  
    You don't need complex sentences to express complex ideas. When specialists in some abstruse topic talk to one another
    about ideas in their field, they don't use sentences any more complex than they do when talking about what to have for
    lunch. They use different words, certainly. But even those they use no more than necessary. And in my experience, the
    harder the subject, the more informally experts speak. Partly, I think, because they have less to prove, and partly
    because the harder the ideas you're talking about, the less you can afford to let language get in the way.
</p><br>
<p>   
    Informal language is the athletic clothing of ideas.
</p><br>
<p>   
    I'm not saying spoken language always works best. Poetry is as much music as text, so you can say things you wouldn't
    say in conversation. And there are a handful of writers who can get away with using fancy language in prose. And then of
    course there are cases where writers don't want to make it easy to understand what they're saying—in corporate
    announcements of bad news, for example, or at the more bogus end of the humanities. But for nearly everyone else, spoken
    language is better.
</p><br>
<p>  
    It seems to be hard for most people to write in spoken language. So perhaps the best solution is to write your first
    draft the way you usually would, then afterward look at each sentence and ask "Is this the way I'd say this if I were
    talking to a friend?" If it isn't, imagine what you would say, and use that instead. After a while this filter will
    start to operate as you write. When you write something you wouldn't say, you'll hear the clank as it hits the page.
</p><br>
<p>  
    Before I publish a new essay, I read it out loud and fix everything that doesn't sound like conversation. I even fix
    bits that are phonetically awkward; I don't know if that's necessary, but it doesn't cost much.
</p><br>
<p>   
    This trick may not always be enough. I've seen writing so far removed from spoken language that it couldn't be fixed
    sentence by sentence. For cases like that there's a more drastic solution. After writing the first draft, try explaining
    to a friend what you just wrote. Then replace the draft with what you said to your friend.
</p><br>
<p>  
    People often tell me how much my essays sound like me talking. The fact that this seems worthy of comment shows how
    rarely people manage to write in spoken language. Otherwise everyone's writing would sound like them talking.
</p><br>
<p>  
    If you simply manage to write in spoken language, you'll be ahead of 95% of writers. And it's so easy to do: just don't
    let a sentence through unless it's the way you'd say it to a friend.
</p><br>
<p>   
    Thanks to Patrick Collison and Jessica Livingston for reading drafts of this.
</p><br>
<br><br>
<span id="title">Default Alive Or Default Dead?</span>
                <br>
                <span id="secondText">October 2015</span>
                <br><br><br><br>   
<p>
    When I talk to a startup that's been operating for more than 8 or 9 months, the first thing I want to know is almost
    always the same. Assuming their expenses remain constant and their revenue growth is what it has been over the last
    several months, do they make it to profitability on the money they have left? Or to put it more dramatically, by default
    do they live or die?
</p><br>
<p> 
    The startling thing is how often the founders themselves don't know. Half the founders I talk to don't know whether
    they're default alive or default dead.
</p><br>
<p>   
    If you're among that number, Trevor Blackwell has made a handy calculator you can use to find out.
</p><br>
<p>    
    The reason I want to know first whether a startup is default alive or default dead is that the rest of the conversation
    depends on the answer. If the company is default alive, we can talk about ambitious new things they could do. If it's
    default dead, we probably need to talk about how to save it. We know the current trajectory ends badly. How can they get
    off that trajectory?
</p><br>
<p>    
    Why do so few founders know whether they're default alive or default dead? Mainly, I think, because they're not used to
    asking that. It's not a question that makes sense to ask early on, any more than it makes sense to ask a 3 year old how
    he plans to support himself. But as the company grows older, the question switches from meaningless to critical. That
    kind of switch often takes people by surprise.
</p><br>
<p>    
    I propose the following solution: instead of starting to ask too late whether you're default alive or default dead,
    start asking too early. It's hard to say precisely when the question switches polarity. But it's probably not that
    dangerous to start worrying too early that you're default dead, whereas it's very dangerous to start worrying too late.
</p><br>
<p>   
    The reason is a phenomenon I wrote about earlier: the fatal pinch. The fatal pinch is default dead + slow growth + not
    enough time to fix it. And the way founders end up in it is by not realizing that's where they're headed.
</p><br>
<p>    
    There is another reason founders don't ask themselves whether they're default alive or default dead: they assume it will
    be easy to raise more money. But that assumption is often false, and worse still, the more you depend on it, the falser
    it becomes.
</p><br>
<p>   
    Maybe it will help to separate facts from hopes. Instead of thinking of the future with vague optimism, explicitly
    separate the components. Say "We're default dead, but we're counting on investors to save us." Maybe as you say that, it
    will set off the same alarms in your head that it does in mine. And if you set off the alarms sufficiently early, you
    may be able to avoid the fatal pinch.
</p><br>
<p>    
    It would be safe to be default dead if you could count on investors saving you. As a rule their interest is a function
    of growth. If you have steep revenue growth, say over 5x a year, you can start to count on investors being interested
    even if you're not profitable. [1] But investors are so fickle that you can never do more than start to count on them.
    Sometimes something about your business will spook investors even if your growth is great. So no matter how good your
    growth is, you can never safely treat fundraising as more than a plan A. You should always have a plan B as well: you
    should know (as in write down) precisely what you'll need to do to survive if you can't raise more money, and precisely
    when you'll have to switch to plan B if plan A isn't working.
</p><br>
<p>   
    In any case, growing fast versus operating cheaply is far from the sharp dichotomy many founders assume it to be. In
    practice there is surprisingly little connection between how much a startup spends and how fast it grows. When a startup
    grows fast, it's usually because the product hits a nerve, in the sense of hitting some big need straight on. When a
    startup spends a lot, it's usually because the product is expensive to develop or sell, or simply because they're
    wasteful.
</p><br>
<p>    
    If you're paying attention, you'll be asking at this point not just how to avoid the fatal pinch, but how to avoid being
    default dead. That one is easy: don't hire too fast. Hiring too fast is by far the biggest killer of startups that raise
    money. [2]
</p><br>
<p>   
    Founders tell themselves they need to hire in order to grow. But most err on the side of overestimating this need rather
    than underestimating it. Why? Partly because there's so much work to do. Naive founders think that if they can just hire
    enough people, it will all get done. Partly because successful startups have lots of employees, so it seems like that's
    what one does in order to be successful. In fact the large staffs of successful startups are probably more the effect of
    growth than the cause. And partly because when founders have slow growth they don't want to face what is usually the
    real reason: the product is not appealing enough.
</p><br>
<p>   
    Plus founders who've just raised money are often encouraged to overhire by the VCs who funded them. Kill-or-cure
    strategies are optimal for VCs because they're protected by the portfolio effect. VCs want to blow you up, in one sense
    of the phrase or the other. But as a founder your incentives are different. You want above all to survive. [3]
</p><br>
<p>    
    Here's a common way startups die. They make something moderately appealing and have decent initial growth. They raise
    their first round fairly easily, because the founders seem smart and the idea sounds plausible. But because the product
    is only moderately appealing, growth is ok but not great. The founders convince themselves that hiring a bunch of people
    is the way to boost growth. Their investors agree. But (because the product is only moderately appealing) the growth
    never comes. Now they're rapidly running out of runway. They hope further investment will save them. But because they
    have high expenses and slow growth, they're now unappealing to investors. They're unable to raise more, and the company
    dies.
</p><br>
<p>    
    What the company should have done is address the fundamental problem: that the product is only moderately appealing.
    Hiring people is rarely the way to fix that. More often than not it makes it harder. At this early stage, the product
    needs to evolve more than to be "built out," and that's usually easier with fewer people. [4]
</p><br>
<p>   
    Asking whether you're default alive or default dead may save you from this. Maybe the alarm bells it sets off will
    counteract the forces that push you to overhire. Instead you'll be compelled to seek growth in other ways. For example,
    by doing things that don't scale, or by redesigning the product in the way only founders can. And for many if not most
    startups, these paths to growth will be the ones that actually work.
</p><br>
<p>   
    Airbnb waited 4 months after raising money at the end of Y Combinator before they hired their first employee. In the
    meantime the founders were terribly overworked. But they were overworked evolving Airbnb into the astonishingly
    successful organism it is now.
</p><br>
<p>   
    Notes
</p><br>
<p>   
    [1] Steep usage growth will also interest investors. Revenue will ultimately be a constant multiple of usage, so x%
    usage growth predicts x% revenue growth. But in practice investors discount merely predicted revenue, so if you're
    measuring usage you need a higher growth rate to impress investors.
</p><br>
<p>   
    [2] Startups that don't raise money are saved from hiring too fast because they can't afford to. But that doesn't mean
    you should avoid raising money in order to avoid this problem, any more than that total abstinence is the only way to
    avoid becoming an alcoholic.
</p><br>
<p>   
    [3] I would not be surprised if VCs' tendency to push founders to overhire is not even in their own interest. They don't
    know how many of the companies that get killed by overspending might have done well if they'd survived. My guess is a
    significant number.
</p><br>
<p>    
    [4] After reading a draft, Sam Altman wrote:
</p><br>
<p>    
    "I think you should make the hiring point more strongly. I think it's roughly correct to say that YC's most successful
    companies have never been the fastest to hire, and one of the marks of a great founder is being able to resist this
    urge."
</p><br>
<p>    
    Paul Buchheit adds:
</p><br>
<p>    
    "A related problem that I see a lot is premature scaling—founders take a small business that isn't really working (bad
    unit economics, typically) and then scale it up because they want impressive growth numbers. This is similar to
    over-hiring in that it makes the business much harder to fix once it's big, plus they are bleeding cash really fast."
</p><br>
<p>    
    Thanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston, and Geoff Ralston for reading drafts of this.
</p><br>
<br><br>
<span id="title">Why It's Safe For Founders To Be Nice</span>
                <br>
                <span id="secondText">August 2015</span>
                <br><br><br><br>   
<p>
    I recently got an email from a founder that helped me understand something important: why it's safe for startup
    founders to be nice people.
</p><br>
<p> 
    I grew up with a cartoon idea of a very successful businessman (in the cartoon it was always a man): a rapacious,
    cigar-smoking, table-thumping guy in his fifties who wins by exercising power, and isn't too fussy about how. As
    I've written before, one of the things that has surprised me most about startups is how few of the most successful
    founders are like that. Maybe successful people in other industries are; I don't know; but not startup founders. [1]
</p><br>
<p> 
    I knew this empirically, but I never saw the math of why till I got this founder's email. In it he said he worried
    that he was fundamentally soft-hearted and tended to give away too much for free. He thought perhaps he needed "a
    little dose of sociopath-ness."
</p><br>
<p> 
    I told him not to worry about it, because so long as he built something good enough to spread by word of mouth, he'd
    have a superlinear growth curve. If he was bad at extracting money from people, at worst this curve would be some
    constant multiple less than 1 of what it might have been. But a constant multiple of any curve is exactly the same
    shape. The numbers on the Y axis are smaller, but the curve is just as steep, and when anything grows at the rate of
    a successful startup, the Y axis will take care of itself.
</p><br>
<p> 
    Some examples will make this clear. Suppose your company is making $1000 a month now, and you've made something so
    great that it's growing at 5% a week. Two years from now, you'll be making about $160k a month.
</p><br>
<p> 
    Now suppose you're so un-rapacious that you only extract half as much from your users as you could. That means two
    years later you'll be making $80k a month instead of $160k. How far behind are you? How long will it take to catch
    up with where you'd have been if you were extracting every penny? A mere 15 weeks. After two years, the un-rapacious
    founder is only 3.5 months behind the rapacious one. [2]
</p><br>
<p> 
    If you're going to optimize a number, the one to choose is your growth rate. Suppose as before that you only extract
    half as much from users as you could, but that you're able to grow 6% a week instead of 5%. Now how are you doing
    compared to the rapacious founder after two years? You're already ahead—$214k a month versus $160k—and pulling away
    fast. In another year you'll be making $4.4 million a month to the rapacious founder's $2 million.
</p><br>
<p> 
    Obviously one case where it would help to be rapacious is when growth depends on that. What makes startups different
    is that usually it doesn't. Startups usually win by making something so great that people recommend it to their
    friends. And being rapacious not only doesn't help you do that, but probably hurts. [3]
</p><br>
<p> 
    The reason startup founders can safely be nice is that making great things is compounded, and rapacity isn't.
</p><br>
<p> 
    So if you're a founder, here's a deal you can make with yourself that will both make you happy and make your company
    successful. Tell yourself you can be as nice as you want, so long as you work hard on your growth rate to
    compensate. Most successful startups make that tradeoff unconsciously. Maybe if you do it consciously you'll do it
    even better.
</p><br>
<p> 
Notes
</p><br>
<p> 
    [1] Many think successful startup founders are driven by money. In fact the secret weapon of the most successful
    founders is that they aren't. If they were, they'd have taken one of the acquisition offers that every fast-growing
    startup gets on the way up. What drives the most successful founders is the same thing that drives most people who
    make things: the company is their project.
</p><br>
<p> 
    [2] In fact since 2 ≈ 1.05 ^ 15, the un-rapacious founder is always 15 weeks behind the rapacious one.
</p><br>
<p> 
    [3] The other reason it might help to be good at squeezing money out of customers is that startups usually lose
    money at first, and making more per customer makes it easier to get to profitability before your initial funding
    runs out. But while it is very common for startups to die from running through their initial funding and then being
    unable to raise more, the underlying cause is usually slow growth or excessive spending rather than insufficient
    effort to extract money from existing customers.
</p><br>
<p> Thanks to Sam Altman, Harj Taggar, Jessica Livingston, and Geoff Ralston for reading drafts of this, and to Randall
    Bennett for being such a nice guy.
</p><br>
<br><br>
<span id="title">Change Your Name</span>
                <br>
                <span id="secondText">August 2015</span>
                <br><br><br><br>   
<p>
    If you have a US startup called X and you don't have x.com, you should probably change your name.
</p><br>
<p>  
    The reason is not just that people can't find you. For companies with mobile apps, especially, having the right domain
    name is not as critical as it used to be for getting users. The problem with not having the .com of your name is that it
    signals weakness. Unless you're so big that your reputation precedes you, a marginal domain suggests you're a marginal
    company. Whereas (as Stripe shows) having x.com signals strength even if it has no relation to what you do.
</p><br>
<p>   
    Even good founders can be in denial about this. Their denial derives from two very powerful forces: identity, and lack
    of imagination.
</p><br>
<p>   
    X is what we are, founders think. There's no other name as good. Both of which are false.
</p><br>
<p>  
    You can fix the first by stepping back from the problem. Imagine you'd called your company something else. If you had,
    surely you'd be just as attached to that name as you are to your current one. The idea of switching to your current name
    would seem repellent. [1]
</p><br>
<p>  
    There's nothing intrinsically great about your current name. Nearly all your attachment to it comes from it being
    attached to you. [2]
</p><br>
<p>   
    The way to neutralize the second source of denial, your inability to think of other potential names, is to acknowledge
    that you're bad at naming. Naming is a completely separate skill from those you need to be a good founder. You can be a
    great startup founder but hopeless at thinking of names for your company.
</p><br>
<p>   
    Once you acknowledge that, you stop believing there is nothing else you could be called. There are lots of other
    potential names that are as good or better; you just can't think of them.
</p><br>
<p>   
    How do you find them? One answer is the default way to solve problems you're bad at: find someone else who can think of
    names. But with company names there is another possible approach. It turns out almost any word or word pair that is not
    an obviously bad name is a sufficiently good one, and the number of such domains is so large that you can find plenty
    that are cheap or even untaken. So make a list and try to buy some. That's what Stripe did. (Their search also turned up
    parse.com, which their friends at Parse took.)
</p><br>
<p>   
    The reason I know that naming companies is a distinct skill orthogonal to the others you need in a startup is that I
    happen to have it. Back when I was running YC and did more office hours with startups, I would often help them find new
    names. 80% of the time we could find at least one good name in a 20 minute office hour slot.
</p><br>
<p>   
    Now when I do office hours I have to focus on more important questions, like what the company is doing. I tell them when
    they need to change their name. But I know the power of the forces that have them in their grip, so I know most won't
    listen. [3]
</p><br>
<p>  
    There are of course examples of startups that have succeeded without having the .com of their name. There are startups
    that have succeeded despite any number of different mistakes. But this mistake is less excusable than most. It's
    something that can be fixed in a couple days if you have sufficient discipline to acknowledge the problem.
</p><br>
<p>    
    100% of the top 20 YC companies by valuation have the .com of their name. 94% of the top 50 do. But only 66% of
    companies in the current batch have the .com of their name. Which suggests there are lessons ahead for most of the rest,
    one way or another.
</p><br>
<p>  
    Notes
</p><br>
<p>  
    [1] Incidentally, this thought experiment works for nationality and religion too.
</p><br>
<p>  
    [2] The liking you have for a name that has become part of your identity manifests itself not directly, which would be
    easy to discount, but as a collection of specious beliefs about its intrinsic qualities. (This too is true of
    nationality and religion as well.)
</p><br>
<p>  
    [3] Sometimes founders know it's a problem that they don't have the .com of their name, but delusion strikes a step
    later in the belief that they'll be able to buy it despite having no evidence it's for sale. Don't believe a domain is
    for sale unless the owner has already told you an asking price.
</p><br>
<p>  
    Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading drafts of this.
</p><br>
<br><br>
<span id="title">What Microsoft Is This The Altair Basic Of?</span>
                <br>
                <span id="secondText">February 2015</span>
                <br><br><br><br>   
<p>
    One of the most valuable exercises you can try if you want to understand startups is to look at the most successful
    companies and explain why they were not as lame as they seemed when they first launched. Because they practically all
    seemed lame at first. Not just small, lame. Not just the first step up a big mountain. More like the first step into a
    swamp.
</p><br>
<p>  
    A Basic interpreter for the Altair? How could that ever grow into a giant company? People sleeping on airbeds in
    strangers' apartments? A web site for college students to stalk one another? A wimpy little single-board computer for
    hobbyists that used a TV as a monitor? A new search engine, when there were already about 10, and they were all trying
    to de-emphasize search? These ideas didn't just seem small. They seemed wrong. They were the kind of ideas you could not
    merely ignore, but ridicule.
</p><br>
<p>  
    Often the founders themselves didn't know why their ideas were promising. They were attracted to these ideas by
    instinct, because they were living in the future and they sensed that something was missing. But they could not have put
    into words exactly how their ugly ducklings were going to grow into big, beautiful swans.
</p><br>
<p>  
    Most people's first impulse when they hear about a lame-sounding new startup idea is to make fun of it. Even a lot of
    people who should know better.
</p><br>
<p>  
    When I encounter a startup with a lame-sounding idea, I ask "What Microsoft is this the Altair Basic of?" Now it's a
    puzzle, and the burden is on me to solve it. Sometimes I can't think of an answer, especially when the idea is a made-up
    one. But it's remarkable how often there does turn out to be an answer. Often it's one the founders themselves hadn't
    seen yet.
</p><br>
<p>   
    Intriguingly, there are sometimes multiple answers. I talked to a startup a few days ago that could grow into 3 distinct
    Microsofts. They'd probably vary in size by orders of magnitude. But you can never predict how big a Microsoft is going
    to be, so in cases like that I encourage founders to follow whichever path is most immediately exciting to them. Their
    instincts got them this far. Why stop now?
</p><br>
<br><br>
<span id="title">The Ronco Principle</span>
                <br>
                <span id="secondText">January 2015</span>
                <br><br><br><br>   
<p>
    No one, VC or angel, has invested in more of the top startups than Ron Conway. He knows what happened in every deal in
    the Valley, half the time because he arranged it.
</p><br>
<p>  
    And yet he's a super nice guy. In fact, nice is not the word. Ronco is good. I know of zero instances in which he has
    behaved badly. It's hard even to imagine.
</p><br>
<p>  
    When I first came to Silicon Valley I thought "How lucky that someone so powerful is so benevolent." But gradually I
    realized it wasn't luck. It was by being benevolent that Ronco became so powerful. All the deals he gets to invest in
    come to him through referrals. Google did. Facebook did. Twitter was a referral from Evan Williams himself. And the
    reason so many people refer deals to him is that he's proven himself to be a good guy.
</p><br>
<p>   
    Good does not mean being a pushover. I would not want to face an angry Ronco. But if Ron's angry at you, it's because
    you did something wrong. Ron is so old school he's Old Testament. He will smite you in his just wrath, but there's no
    malice in it.
</p><br>
<p>  
    In almost every domain there are advantages to seeming good. It makes people trust you. But actually being good is an
    expensive way to seem good. To an amoral person it might seem to be overkill.
</p><br>
<p>  
    In some fields it might be, but apparently not in the startup world. Though plenty of investors are jerks, there is a
    clear trend among them: the most successful investors are also the most upstanding. [1]
</p><br>
<p>   
    It was not always this way. I would not feel confident saying that about investors twenty years ago.
</p><br>
<p>   
    What changed? The startup world became more transparent and more unpredictable. Both make it harder to seem good without
    actually being good.
</p><br>
<p>   
    It's obvious why transparency has that effect. When an investor maltreats a founder now, it gets out. Maybe not all the
    way to the press, but other founders hear about it, and that investor starts to lose deals. [2]
</p><br>
<p>   
    The effect of unpredictability is more subtle. It increases the work of being inconsistent. If you're going to be
    two-faced, you have to know who you should be nice to and who you can get away with being nasty to. In the startup
    world, things change so rapidly that you can't tell. The random college kid you talk to today might in a couple years be
    the CEO of the hottest startup in the Valley. If you can't tell who to be nice to, you have to be nice to everyone. And
    probably the only people who can manage that are the people who are genuinely good.
</p><br>
<p>   
    In a sufficiently connected and unpredictable world, you can't seem good without being good.
</p><br>
<p>   
    As often happens, Ron discovered how to be the investor of the future by accident. He didn't foresee the future of
    startup investing, realize it would pay to be upstanding, and force himself to behave that way. It would feel unnatural
    to him to behave any other way. He was already living in the future.
</p><br>
<p>   
    Fortunately that future is not limited to the startup world. The startup world is more transparent and unpredictable
    than most, but almost everywhere the trend is in that direction.
</p><br>
<p>   Notes
</p><br>
<p>   
    [1] I'm not saying that if you sort investors by benevolence you've also sorted them by returns, but rather that if you
    do a scatterplot with benevolence on the x axis and returns on the y, you'd see a clear upward trend.
</p><br>
<p> 
    [2] Y Combinator in particular, because it aggregates data from so many startups, has a pretty comprehensive view of
    investor behavior.
</p><br>
<p>  
    Thanks to Sam Altman and Jessica Livingston for reading drafts of this.
</p><br>
<br><br>
<span id="title">What Doesn't Seem Like Work</span>
                <br>
                <span id="secondText">January 2015</span>
                <br><br><br><br>   
<p>
    My father is a mathematician. For most of my childhood he worked for Westinghouse, modelling nuclear reactors.
</p><br>
<p>  
    He was one of those lucky people who know early on what they want to do. When you talk to him about his childhood,
    there's a clear watershed at about age 12, when he "got interested in maths."
</p><br>
<p>
    He grew up in the small Welsh seacoast town of Pwllheli. As we retraced his walk to school on Google Street View, he
    said that it had been nice growing up in the country.
</p><br>
<p>  
    "Didn't it get boring when you got to be about 15?" I asked.
</p><br>
<p>   
    "No," he said, "by then I was interested in maths."
</p><br>
<p>  
    In another conversation he told me that what he really liked was solving problems. To me the exercises at the end of
    each chapter in a math textbook represent work, or at best a way to reinforce what you learned in that chapter. To him
    the problems were the reward. The text of each chapter was just some advice about solving them. He said that as soon as
    he got a new textbook he'd immediately work out all the problems — to the slight annoyance of his teacher, since the
    class was supposed to work through the book gradually.
</p><br>
<p>   
    Few people know so early or so certainly what they want to work on. But talking to my father reminded me of a heuristic
    the rest of us can use. If something that seems like work to other people doesn't seem like work to you, that's
    something you're well suited for. For example, a lot of programmers I know, including me, actually like debugging. It's
    not something people tend to volunteer; one likes it the way one likes popping zits. But you may have to like debugging
    to like programming, considering the degree to which programming consists of it.
</p><br>
<p>  
    The stranger your tastes seem to other people, the stronger evidence they probably are of what you should do. When I was
    in college I used to write papers for my friends. It was quite interesting to write a paper for a class I wasn't taking.
    Plus they were always so relieved.
</p><br>
<p>  
    It seemed curious that the same task could be painful to one person and pleasant to another, but I didn't realize at the
    time what this imbalance implied, because I wasn't looking for it. I didn't realize how hard it can be to decide what
    you should work on, and that you sometimes have to figure it out from subtle clues, like a detective solving a case in a
    mystery novel. So I bet it would help a lot of people to ask themselves about this explicitly. What seems like work to
    other people that doesn't seem like work to you?
</p><br>
<p>  
    Thanks to Sam Altman, Trevor Blackwell, Jessica Livingston, Robert Morris, and my father for reading drafts of this.
</p><br>
<br><br>
<span id="title">Don't Talk To Corp Dev</span>
                <br>
                <span id="secondText">January 2015</span>
                <br><br><br><br>   
<p>
    Corporate Development, aka corp dev, is the group within companies that buys other companies. If you're talking to
    someone from corp dev, that's why, whether you realize it yet or not.
</p><br>
<p>    
    It's usually a mistake to talk to corp dev unless (a) you want to sell your company right now and (b) you're
    sufficiently likely to get an offer at an acceptable price. In practice that means startups should only talk to corp dev
    when they're either doing really well or really badly. If you're doing really badly, meaning the company is about to
    die, you may as well talk to them, because you have nothing to lose. And if you're doing really well, you can safely
    talk to them, because you both know the price will have to be high, and if they show the slightest sign of wasting your
    time, you'll be confident enough to tell them to get lost.
</p><br>
<p>    
    The danger is to companies in the middle. Particularly to young companies that are growing fast, but haven't been doing
    it for long enough to have grown big yet. It's usually a mistake for a promising company less than a year old even to
    talk to corp dev.
</p><br>
<p>    
    But it's a mistake founders constantly make. When someone from corp dev wants to meet, the founders tell themselves they
    should at least find out what they want. Besides, they don't want to offend Big Company by refusing to meet.
</p><br>
<p>    
    Well, I'll tell you what they want. They want to talk about buying you. That's what the title "corp dev" means. So
    before agreeing to meet with someone from corp dev, ask yourselves, "Do we want to sell the company right now?" And if
    the answer is no, tell them "Sorry, but we're focusing on growing the company." They won't be offended. And certainly
    the founders of Big Company won't be offended. If anything they'll think more highly of you. You'll remind them of
    themselves. They didn't sell either; that's why they're in a position now to buy other companies. [1]
</p><br>
<p>    
    Most founders who get contacted by corp dev already know what it means. And yet even when they know what corp dev does
    and know they don't want to sell, they take the meeting. Why do they do it? The same mix of denial and wishful thinking
    that underlies most mistakes founders make. It's flattering to talk to someone who wants to buy you. And who knows,
    maybe their offer will be surprisingly high. You should at least see what it is, right?
</p><br>
<p>    
    No. If they were going to send you an offer immediately by email, sure, you might as well open it. But that is not how
    conversations with corp dev work. If you get an offer at all, it will be at the end of a long and unbelievably
    distracting process. And if the offer is surprising, it will be surprisingly low.
</p><br>
<p>    
    Distractions are the thing you can least afford in a startup. And conversations with corp dev are the worst sort of
    distraction, because as well as consuming your attention they undermine your morale. One of the tricks to surviving a
    grueling process is not to stop and think how tired you are. Instead you get into a sort of flow. [2] Imagine what it
    would do to you if at mile 20 of a marathon, someone ran up beside you and said "You must feel really tired. Would you
    like to stop and take a rest?" Conversations with corp dev are like that but worse, because the suggestion of stopping
    gets combined in your mind with the imaginary high price you think they'll offer.
</p><br>
<p>    
    And then you're really in trouble. If they can, corp dev people like to turn the tables on you. They like to get you to
    the point where you're trying to convince them to buy instead of them trying to convince you to sell. And surprisingly
    often they succeed.
</p><br>
<p>    
    This is a very slippery slope, greased with some of the most powerful forces that can work on founders' minds, and
    attended by an experienced professional whose full time job is to push you down it.
</p><br>
<p>     
    Their tactics in pushing you down that slope are usually fairly brutal. Corp dev people's whole job is to buy companies,
    and they don't even get to choose which. The only way their performance is measured is by how cheaply they can buy you,
    and the more ambitious ones will stop at nothing to achieve that. For example, they'll almost always start with a
    lowball offer, just to see if you'll take it. Even if you don't, a low initial offer will demoralize you and make you
    easier to manipulate.
</p><br>
<p>     
    And that is the most innocent of their tactics. Just wait till you've agreed on a price and think you have a done deal,
    and then they come back and say their boss has vetoed the deal and won't do it for more than half the agreed upon price.
    Happens all the time. If you think investors can behave badly, it's nothing compared to what corp dev people can do.
    Even corp dev people at companies that are otherwise benevolent.
</p><br>
<p>    
    I remember once complaining to a friend at Google about some nasty trick their corp dev people had pulled on a YC
    startup.
</p><br>
<p>    
    "What happened to Don't be Evil?" I asked.
</p><br>
<p>    
    "I don't think corp dev got the memo," he replied.
</p><br>
<p>    
    The tactics you encounter in M&A conversations can be like nothing you've experienced in the otherwise comparatively
    upstanding world of Silicon Valley. It's as if a chunk of genetic material from the old-fashioned robber baron business
    world got incorporated into the startup world. [3]
</p><br>
<p>     
    The simplest way to protect yourself is to use the trick that John D. Rockefeller, whose grandfather was an alcoholic,
    used to protect himself from becoming one. He once told a Sunday school class
    Boys, do you know why I never became a drunkard? Because I never took the first drink.
    Do you want to sell your company right now? Not eventually, right now. If not, just don't take the first meeting. They
    won't be offended. And you in turn will be guaranteed to be spared one of the worst experiences that can happen to a
    startup.
</p><br>
<p>     
    If you do want to sell, there's another set of techniques for doing that. But the biggest mistake founders make in
    dealing with corp dev is not doing a bad job of talking to them when they're ready to, but talking to them before they
    are. So if you remember only the title of this essay, you already know most of what you need to know about M&A in the
    first year.
</p><br>
<p>     
    Notes
</p><br>
<p>  
    [1] I'm not saying you should never sell. I'm saying you should be clear in your own mind about whether you want to sell
    or not, and not be led by manipulation or wishful thinking into trying to sell earlier than you otherwise would have.
</p><br>
<p>  
    [2] In a startup, as in most competitive sports, the task at hand almost does this for you; you're too busy to feel
    tired. But when you lose that protection, e.g. at the final whistle, the fatigue hits you like a wave. To talk to corp
    dev is to let yourself feel it mid-game.
</p><br>
<p>   
    [3] To be fair, the apparent misdeeds of corp dev people are magnified by the fact that they function as the face of a
    large organization that often doesn't know its own mind. Acquirers can be surprisingly indecisive about acquisitions,
    and their flakiness is indistinguishable from dishonesty by the time it filters down to you.
</p><br>
<p>   
    Thanks to Marc Andreessen, Jessica Livingston, Geoff Ralston, and Qasar Younis for reading drafts of this.
</p><br>
<br><br>
<span id="title">Let The Other 95% Of Great Programmers In</span>
                <br>
                <span id="secondText">December 2014</span>
                <br><br><br><br>   
<p>
    American technology companies want the government to make immigration easier because they say they can't find enough
    programmers in the US. Anti-immigration people say that instead of letting foreigners take these jobs, we should train
    more Americans to be programmers. Who's right?
</p><br>
<p>    
    The technology companies are right. What the anti-immigration people don't understand is that there is a huge variation
    in ability between competent programmers and exceptional ones, and while you can train people to be competent, you can't
    train them to be exceptional. Exceptional programmers have an aptitude for and interest in programming that is not
    merely the product of training. [1]
</p><br>
<p>   
    The US has less than 5% of the world's population. Which means if the qualities that make someone a great programmer are
    evenly distributed, 95% of great programmers are born outside the US.
</p><br>
<p>    
    The anti-immigration people have to invent some explanation to account for all the effort technology companies have
    expended trying to make immigration easier. So they claim it's because they want to drive down salaries. But if you talk
    to startups, you find practically every one over a certain size has gone through legal contortions to get programmers
    into the US, where they then paid them the same as they'd have paid an American. Why would they go to extra trouble to
    get programmers for the same price? The only explanation is that they're telling the truth: there are just not enough
    great programmers to go around. [2]
</p><br>
<p>    
    I asked the CEO of a startup with about 70 programmers how many more he'd hire if he could get all the great programmers
    he wanted. He said "We'd hire 30 tomorrow morning." And this is one of the hot startups that always win recruiting
    battles. It's the same all over Silicon Valley. Startups are that constrained for talent.
</p><br>
<p>    
    It would be great if more Americans were trained as programmers, but no amount of training can flip a ratio as
    overwhelming as 95 to 5. Especially since programmers are being trained in other countries too. Barring some cataclysm,
    it will always be true that most great programmers are born outside the US. It will always be true that most people who
    are great at anything are born outside the US. [3]
</p><br>
<p>   
    Exceptional performance implies immigration. A country with only a few percent of the world's population will be
    exceptional in some field only if there are a lot of immigrants working in it.
</p><br>
<p>    
    But this whole discussion has taken something for granted: that if we let more great programmers into the US, they'll
    want to come. That's true now, and we don't realize how lucky we are that it is. If we want to keep this option open,
    the best way to do it is to take advantage of it: the more of the world's great programmers are here, the more the rest
    will want to come here.
</p><br>
<p>   
    And if we don't, the US could be seriously fucked. I realize that's strong language, but the people dithering about this
    don't seem to realize the power of the forces at work here. Technology gives the best programmers huge leverage. The
    world market in programmers seems to be becoming dramatically more liquid. And since good people like good colleagues,
    that means the best programmers could collect in just a few hubs. Maybe mostly in one hub.
</p><br>
<p>    
    What if most of the great programmers collected in one hub, and it wasn't here? That scenario may seem unlikely now, but
    it won't be if things change as much in the next 50 years as they did in the last 50.
</p><br>
<p>    
    We have the potential to ensure that the US remains a technology superpower just by letting in a few thousand great
    programmers a year. What a colossal mistake it would be to let that opportunity slip. It could easily be the defining
    mistake this generation of American politicians later become famous for. And unlike other potential mistakes on that
    scale, it costs nothing to fix.
</p><br>
<p>    
    So please, get on with it.
</p><br>
<p>   
    Notes
</p><br>
<p> 
    [1] How much better is a great programmer than an ordinary one? So much better that you can't even measure the
    difference directly. A great programmer doesn't merely do the same work faster. A great programmer will invent things an
    ordinary programmer would never even think of. This doesn't mean a great programmer is infinitely more valuable, because
    any invention has a finite market value. But it's easy to imagine cases where a great programmer might invent things
    worth 100x or even 1000x an average programmer's salary.
</p><br>
<p> 
    [2] There are a handful of consulting firms that rent out big pools of foreign programmers they bring in on H1-B visas.
    By all means crack down on these. It should be easy to write legislation that distinguishes them, because they are so
    different from technology companies. But it is dishonest of the anti-immigration people to claim that companies like
    Google and Facebook are driven by the same motives. An influx of inexpensive but mediocre programmers is the last thing
    they'd want; it would destroy them.
</p><br>
<p> 
    [3] Though this essay talks about programmers, the group of people we need to import is broader, ranging from designers
    to programmers to electrical engineers. The best one could do as a general term might be "digital talent." It seemed
    better to make the argument a little too narrow than to confuse everyone with a neologism.
</p><br>
<p>    
    Thanks to Sam Altman, John Collison, Patrick Collison, Jessica Livingston, Geoff Ralston, Fred Wilson, and Qasar Younis
    for reading drafts of this.
</p><br>
<br><br>
<span id="title">How To Be An Expert In A Changing World</span>
                <br>
                <span id="secondText">December 2014</span>
                <br><br><br><br>   
<p>
If the world were static, we could have monotonically increasing confidence in our beliefs. The more (and more varied)
experience a belief survived, the less likely it would be false. Most people implicitly believe something like this
about their opinions. And they're justified in doing so with opinions about things that don't change much, like human
nature. But you can't trust your opinions in the same way about things that change, which could include practically
everything else.
</p><br>
<p> 
When experts are wrong, it's often because they're experts on an earlier version of the world.
</p><br>
<p> 
Is it possible to avoid that? Can you protect yourself against obsolete beliefs? To some extent, yes. I spent almost a
decade investing in early stage startups, and curiously enough protecting yourself against obsolete beliefs is exactly
what you have to do to succeed as a startup investor. Most really good startup ideas look like bad ideas at first, and
many of those look bad specifically because some change in the world just switched them from bad to good. I spent a lot
of time learning to recognize such ideas, and the techniques I used may be applicable to ideas in general.
</p><br>
<p> 
The first step is to have an explicit belief in change. People who fall victim to a monotonically increasing confidence
in their opinions are implicitly concluding the world is static. If you consciously remind yourself it isn't, you start
to look for change.
</p><br>
<p> 
Where should one look for it? Beyond the moderately useful generalization that human nature doesn't change much, the
unfortunate fact is that change is hard to predict. This is largely a tautology but worth remembering all the same:
change that matters usually comes from an unforeseen quarter.
</p><br>
<p> 
So I don't even try to predict it. When I get asked in interviews to predict the future, I always have to struggle to
come up with something plausible-sounding on the fly, like a student who hasn't prepared for an exam. [1] But it's not
out of laziness that I haven't prepared. It seems to me that beliefs about the future are so rarely correct that they
usually aren't worth the extra rigidity they impose, and that the best strategy is simply to be aggressively
open-minded. Instead of trying to point yourself in the right direction, admit you have no idea what the right direction
is, and try instead to be super sensitive to the winds of change.
</p><br>
<p> 
It's ok to have working hypotheses, even though they may constrain you a bit, because they also motivate you. It's
exciting to chase things and exciting to try to guess answers. But you have to be disciplined about not letting your
hypotheses harden into anything more. [2]
</p><br>
<p> 
I believe this passive m.o. works not just for evaluating new ideas but also for having them. The way to come up with
new ideas is not to try explicitly to, but to try to solve problems and simply not discount weird hunches you have in
the process.
</p><br>
<p> 
The winds of change originate in the unconscious minds of domain experts. If you're sufficiently expert in a field, any
weird idea or apparently irrelevant question that occurs to you is ipso facto worth exploring. [3] Within Y Combinator,
when an idea is described as crazy, it's a compliment—in fact, on average probably a higher compliment than when an idea
is described as good.
</p><br>
<p> 
Startup investors have extraordinary incentives for correcting obsolete beliefs. If they can realize before other
investors that some apparently unpromising startup isn't, they can make a huge amount of money. But the incentives are
more than just financial. Investors' opinions are explicitly tested: startups come to them and they have to say yes or
no, and then, fairly quickly, they learn whether they guessed right. The investors who say no to a Google (and there
were several) will remember it for the rest of their lives.
</p><br>
<p> 
Anyone who must in some sense bet on ideas rather than merely commenting on them has similar incentives. Which means
anyone who wants such incentives can have them, by turning their comments into bets: if you write about a topic in some
fairly durable and public form, you'll find you worry much more about getting things right than most people would in a
casual conversation. [4]
</p><br>
<p> 
Another trick I've found to protect myself against obsolete beliefs is to focus initially on people rather than ideas.
Though the nature of future discoveries is hard to predict, I've found I can predict quite well what sort of people will
make them. Good new ideas come from earnest, energetic, independent-minded people.
</p><br>
<p> 
Betting on people over ideas saved me countless times as an investor. We thought Airbnb was a bad idea, for example. But
we could tell the founders were earnest, energetic, and independent-minded. (Indeed, almost pathologically so.) So we
suspended disbelief and funded them.
</p><br>
<p> 
This too seems a technique that should be generally applicable. Surround yourself with the sort of people new ideas come
from. If you want to notice quickly when your beliefs become obsolete, you can't do better than to be friends with the
people whose discoveries will make them so.
</p><br>
<p> 
It's hard enough already not to become the prisoner of your own expertise, but it will only get harder, because change
is accelerating. That's not a recent trend; change has been accelerating since the paleolithic era. Ideas beget ideas. I
don't expect that to change. But I could be wrong.
</p><br>
<p> 
Notes
</p><br>
<p> 
[1] My usual trick is to talk about aspects of the present that most people haven't noticed yet.
</p><br>
<p> 
[2] Especially if they become well enough known that people start to identify them with you. You have to be extra
skeptical about things you want to believe, and once a hypothesis starts to be identified with you, it will almost
certainly start to be in that category.
</p><br>
<p> 
[3] In practice "sufficiently expert" doesn't require one to be recognized as an expert—which is a trailing indicator in
any case. In many fields a year of focused work plus caring a lot would be enough.
</p><br>
<p> 
[4] Though they are public and persist indefinitely, comments on e.g. forums and places like Twitter seem empirically to
work like casual conversation. The threshold may be whether what you write has a title.
</p><br>
<p> 
Thanks to Sam Altman, Patrick Collison, and Robert Morris for reading drafts of this.
</p><br>
<br><br>
<span id="title">How You Know</span>
                <br>
                <span id="secondText">December 2014</span>
                <br><br><br><br>   
<p>
    I've read Villehardouin's chronicle of the Fourth Crusade at least two times, maybe three. And yet if I had to write
    down everything I remember from it, I doubt it would amount to much more than a page. Multiply this times several
    hundred, and I get an uneasy feeling when I look at my bookshelves. What use is it to read all these books if I remember
    so little from them?
</p><br>
<p>   
    A few months ago, as I was reading Constance Reid's excellent biography of Hilbert, I figured out if not the answer to
    this question, at least something that made me feel better about it. She writes:
    Hilbert had no patience with mathematical lectures which filled the students with facts but did not teach them how to
    frame a problem and solve it. He often used to tell them that "a perfect formulation of a problem is already half its
    solution."
    That has always seemed to me an important point, and I was even more convinced of it after hearing it confirmed by
    Hilbert.
</p><br>
<p>    
    But how had I come to believe in this idea in the first place? A combination of my own experience and other things I'd
    read. None of which I could at that moment remember! And eventually I'd forget that Hilbert had confirmed it too. But my
    increased belief in the importance of this idea would remain something I'd learned from this book, even after I'd
    forgotten I'd learned it.
</p><br>
<p>   
    Reading and experience train your model of the world. And even if you forget the experience or what you read, its effect
    on your model of the world persists. Your mind is like a compiled program you've lost the source of. It works, but you
    don't know why.
</p><br>
<p>  
    The place to look for what I learned from Villehardouin's chronicle is not what I remember from it, but my mental models
    of the crusades, Venice, medieval culture, siege warfare, and so on. Which doesn't mean I couldn't have read more
    attentively, but at least the harvest of reading is not so miserably small as it might seem.
</p><br>
<p>   
    This is one of those things that seem obvious in retrospect. But it was a surprise to me and presumably would be to
    anyone else who felt uneasy about (apparently) forgetting so much they'd read.
</p><br>
<p>   
    Realizing it does more than make you feel a little better about forgetting, though. There are specific implications.
</p><br>
<p>   
    For example, reading and experience are usually "compiled" at the time they happen, using the state of your brain at
    that time. The same book would get compiled differently at different points in your life. Which means it is very much
    worth reading important books multiple times. I always used to feel some misgivings about rereading books. I
    unconsciously lumped reading together with work like carpentry, where having to do something again is a sign you did it
    wrong the first time. Whereas now the phrase "already read" seems almost ill-formed.
</p><br>
<p>  
    Intriguingly, this implication isn't limited to books. Technology will increasingly make it possible to relive our
    experiences. When people do that today it's usually to enjoy them again (e.g. when looking at pictures of a trip) or to
    find the origin of some bug in their compiled code (e.g. when Stephen Fry succeeded in remembering the childhood trauma
    that prevented him from singing). But as technologies for recording and playing back your life improve, it may become
    common for people to relive experiences without any goal in mind, simply to learn from them again as one might when
    rereading a book.
</p><br>
<p>  
    Eventually we may be able not just to play back experiences but also to index and even edit them. So although not
    knowing how you know things may seem part of being human, it may not be.
</p><br>
<p>   
    Thanks to Sam Altman, Jessica Livingston, and Robert Morris for reading drafts of this.
</p><br>
       
            </div>
        </div>
        <script>
            var counter = document.querySelector(".percent");

            TweenLite.set(counter, {
                xPercent: -5,
                yPercent: -5,
            });

            window.addEventListener("mousemove", moveCounter);

            function moveCounter(e) {
                TweenLite.to(counter, 0.5, {
                  x: e.clientX,
                  y: e.clientY, 
                });
            }

            function progress() {
                var s = $(window).scrollTop();
                var d = $(document).height();
                var h = $(window).height();
                var progress = (s/(d)) * 100;

                var $bgColor = progress > 99 ? "#fff" : "#fff";
                var $textColor = progress > 99 ? "fff" : "#333";

                $("h1")
                    .text(Math.round(progress) + "%")
                    .css({ color: $textColor}); 

                $(".fill")
                    .height(progress + "%")
                    .css({ backgroundColor: $bgColor});
            }
            progress();
            $(document).on("scroll", progress);
        </script>
    </body>




    <div class="welcome-section content-hidden">
        <div class="content-wrap">
            <ul class="fly-in-text">
                <li>p</li>
                <li>a</li>
                <li>u</li>
                <li>l</li>
                <li>g</li>
                <li>r</li>
                <li>a</li>
                <li>h</li>
                <li>a</li>
                <li>m</li>
            </ul>
            <a href="#" class="enter-button">ENTER</a>
        </div>
    </div>
    <script type="text/javascript">
    
        $(function() {
            var welcomeSection = $('.welcome-section'),
                enterButton = welcomeSection.find('.enter-button');
    
                setTimeout(function() {
                    welcomeSection.removeClass('content-hidden');
                },800);
                
                enterButton.on('click', function(e) {
                    e.preventDefault();
                    welcomeSection.addClass('content-hidden').fadeOut();
                });
        });
        </script>
</body>
</html>